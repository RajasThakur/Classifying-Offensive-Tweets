{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIM2yMbgktwv"
   },
   "source": [
    "## Inspirations: https://github.com/bentrevett/pytorch-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "ggUwYQmgf9Cb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext.data as ttd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmoB1tvcgIN8",
    "outputId": "a6fded7a-f789-4cbe-ddbe-0715a69df0f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ft2QkqmQgN7q",
    "outputId": "cd635bdf-cf20-4fb3-d2db-1dc69de7537b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'drive/My Drive/AppliedNLP/HW3'\n",
      "/content/drive/My Drive/AppliedNLP/HW3\n"
     ]
    }
   ],
   "source": [
    "cd 'drive/My Drive/AppliedNLP/HW3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHfs_fdrPcL3"
   },
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "ur3eIicOPg4g"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8XSG1-uQKMv",
    "outputId": "fd632219-4b89-41d2-c8fa-cf4484e9ae92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9346 entries, 0 to 9345\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      9346 non-null   int64 \n",
      " 1   text    9346 non-null   object\n",
      " 2   target  9346 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 219.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tb1wE2qRj2L1",
    "outputId": "9e9cdc4e-f8b3-4498-ff97-ff76820c9083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3894 entries, 0 to 3893\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      3894 non-null   int64 \n",
      " 1   text    3894 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 61.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "3DmtfGsBiAKc"
   },
   "outputs": [],
   "source": [
    "train.drop(['id'],axis=1,inplace= True)\n",
    "test.drop(['id'],axis=1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "M1JMs0rEkUqY"
   },
   "outputs": [],
   "source": [
    "train.columns = ['data', 'labels']\n",
    "test.columns = ['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "VANu-zf2lQO9"
   },
   "outputs": [],
   "source": [
    "train.to_csv('train2.csv', index=False)\n",
    "test.to_csv('test2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "VJgoD2Hsgk7k"
   },
   "outputs": [],
   "source": [
    "TEXT = ttd.Field(\n",
    "    sequential=True,\n",
    "    batch_first=True,\n",
    "    lower=True,\n",
    "    tokenize='spacy',\n",
    "    pad_first=True)\n",
    "\n",
    "LABEL = ttd.LabelField(dtype = torch.float, batch_first=True)\n",
    "\n",
    "#Train dataset\n",
    "Train_dataset = ttd.TabularDataset(\n",
    "    path='train2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT),('label', LABEL)]\n",
    ")\n",
    "\n",
    "#Test dataset\n",
    "Test_dataset = ttd.TabularDataset(\n",
    "    path='test2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "6hS3eltrzeke"
   },
   "outputs": [],
   "source": [
    "ex = Train_dataset.examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDkn4P9a4H0P",
    "outputId": "77ae7d8d-7f31-4c97-84f1-32a87ee40278"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.data.example.Example"
      ]
     },
     "execution_count": 196,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l728QWX3zkxM",
    "outputId": "b0c73678-d78d-4ca1-fd17-23200343bbf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@user',\n",
       " 'she',\n",
       " 'should',\n",
       " 'ask',\n",
       " 'a',\n",
       " 'few',\n",
       " 'native',\n",
       " 'americans',\n",
       " 'what',\n",
       " 'their',\n",
       " 'take',\n",
       " 'on',\n",
       " 'this',\n",
       " 'is',\n",
       " '.']"
      ]
     },
     "execution_count": 197,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TyO1xB9dzrdY",
    "outputId": "ae0f0100-14aa-495e-a989-a62a83856e58"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 198,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvmxfX5OGLJB"
   },
   "source": [
    "### Splitting into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "IH18at01g3oA"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "SEED=1234\n",
    "training_dataset, testing_dataset = Train_dataset.split(split_ratio=0.7,random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "hfh9owBpGm0A"
   },
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "training_dataset, valid_dataset = training_dataset.split(random_state = random.seed(SEED)) # default is 0.7\n",
    "#training_dataset, valid_dataset = Train_dataset.split(random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OD3nJogaGupy",
    "outputId": "7a1c09e5-9cbb-41d8-fd2b-7aab033ab0f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4579\n",
      "Number of validation examples: 1963\n",
      "Number of testing examples: 2804\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(training_dataset)}')\n",
    "print(f'Number of validation examples: {len(valid_dataset)}')\n",
    "print(f'Number of testing examples: {len(testing_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNt0cDTgHKWB"
   },
   "source": [
    "### Using Pretrained Embeddings from Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "6ieo_C0AlvE6"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(training_dataset, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "dTkkNIV4iYrN"
   },
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(training_dataset)\n",
    "LABEL.build_vocab(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "Lew5Hi_klyi5"
   },
   "outputs": [],
   "source": [
    "vocab_text = TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "bqCyTEoXl7JZ"
   },
   "outputs": [],
   "source": [
    "#vocab_text.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "ts_z-QhKJI3q"
   },
   "outputs": [],
   "source": [
    "#vocab_text.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tM30vqWqinab",
    "outputId": "29280cfa-f21d-409f-c9a2-d3e73716d1b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11155"
      ]
     },
     "execution_count": 207,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "YBeGypYQBSgf"
   },
   "outputs": [],
   "source": [
    "vocab_label = LABEL.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YE2Ud9bvBZLm",
    "outputId": "15a0270d-c36d-4f09-bedc-6a69e5d43210"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function torchtext.vocab._default_unk_index>, {'0': 0, '1': 1})"
      ]
     },
     "execution_count": 209,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_label.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ni84WH6UBdbV",
    "outputId": "842803de-75e4-4450-ec97-5fb6188e5fb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 210,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_label.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9fgCUSA7ju4",
    "outputId": "5b7ee373-c9b1-4892-c8a8-aedcfbde3d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "eMQPnFD0JXZW"
   },
   "outputs": [],
   "source": [
    "train_iter, valid_iter = ttd.BucketIterator.splits((training_dataset,valid_dataset), \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "id": "2_ssoUVcD2CJ"
   },
   "outputs": [],
   "source": [
    "test_iter = ttd.BucketIterator(testing_dataset, \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "QIgUTuHtkK2T"
   },
   "outputs": [],
   "source": [
    "testing_iter = ttd.BucketIterator(Test_dataset, \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sMK69OvJist",
    "outputId": "7b807ad5-294b-4e66-d02e-bb698f4410fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[    2,    44,  8230, 10418,     3,    24],\n",
      "        [    2,    18,     5,  1007,  1007,  1007],\n",
      "        [    2,    37,    32,    88,   320,     3],\n",
      "        [    2,  1380,    13,     4,  7451,    11],\n",
      "        [    2,    50,    19,    96,    35,    11],\n",
      "        [    2,   550,    21,    10,   215,  9565],\n",
      "        [    2,     2,    20,     5,    37,   594],\n",
      "        [    2,     7,    15,   796,     3,   371],\n",
      "        [    2,     2,    18,     5,  6411,    11],\n",
      "        [    2,     2,     2,    18,     5,  1367],\n",
      "        [    2,     2,  9117,   631,   220,    16],\n",
      "        [    2,  1624,   128,   173,  3002,     3],\n",
      "        [    2,     2,    18,     5,   593,  1089],\n",
      "        [    2,     2,   718,   113,  9245,   213],\n",
      "        [    2,     2, 11026,    15,     4,   243],\n",
      "        [    2,     2,  8030,   124,   435,    11],\n",
      "        [    2,     4,   481,   119,    33,    39],\n",
      "        [    2,  1557,     5,    75,   770,    11],\n",
      "        [    1,     2,   223,    86,    16,   179],\n",
      "        [    1,     2,     6,  1314,   579,    16],\n",
      "        [    1,     2,   408,    97,    32,    11],\n",
      "        [    1,     2,     7,    15,    71,   885],\n",
      "        [    1,     2,   109,  1207,    95,    11],\n",
      "        [    1,     2,    20,     5,    37,   306],\n",
      "        [    1,     2,   889,    99,  9916,     2],\n",
      "        [    1,     2,    40,    10,   720,     3],\n",
      "        [    1,     2,   306,   267,     3,     3],\n",
      "        [    1,     2,    80,   353,    14,    57],\n",
      "        [    1,     2,   196,   948,     7,   988],\n",
      "        [    1,     2,    18,     5,  8695,     2],\n",
      "        [    1,     2,  2726,   215,  9923,  1686],\n",
      "        [    1,     2,     2,    40,     4,   200],\n",
      "        [    1,     2,   198,    18,     5,     3],\n",
      "        [    1,     2,    17,    29,   156,    32],\n",
      "        [    1,     2,     2,    18,     5,  1985],\n",
      "        [    1,     2,   751,     5,    10,   720],\n",
      "        [    1,     2,     2,    14,  4117,     2],\n",
      "        [    1,     2,   196,     7,    15,  1367],\n",
      "        [    1,     2,     7,    15,    10,   821],\n",
      "        [    1,     2,     2,     7,    15,  1651],\n",
      "        [    1,     2,    20,     5,  6521,    11],\n",
      "        [    1,     2,  3532,    72,  5810,    11],\n",
      "        [    1,     2,   519,    56,    17,  9020],\n",
      "        [    1,     2,     2,   109,  1664,     3],\n",
      "        [    1,     2,    43,   256,  5587,   727],\n",
      "        [    1,     2,  7546,   239,   303,  5110],\n",
      "        [    1,     2,   179,    18,     5,   316],\n",
      "        [    1,     2,    10,   312,    12,  7570],\n",
      "        [    1,     2,  5853,   209,  1725,     3],\n",
      "        [    1,     2,    18,     5,   115,  6950],\n",
      "        [    1,     2,     2,    44,  1861,     3],\n",
      "        [    1,     2,   200,   193,    12,    24],\n",
      "        [    1,   291,    47,    44,     5,  2113],\n",
      "        [    1,     2,    20,     5,    43,  9900],\n",
      "        [    1,     2,  8214,    11,   363,   353],\n",
      "        [    1,     2,     2,    20,     5,   197],\n",
      "        [    1,     2,   129,  1739,    63,   220],\n",
      "        [    1,     2,  2640,     5,  3235,    11],\n",
      "        [    1,     2,    17,    34,   568,    11],\n",
      "        [    1,     2,    20,     5,   256,  1594],\n",
      "        [    1,     2,   624,    11,    11,    11],\n",
      "        [    1,     2,     2,    32,   129,  1544],\n",
      "        [    1,     2,  3537,    56,   568,     3],\n",
      "        [    1,     2,     2,  4160,  2450,  7085]], device='cuda:0') torch.Size([64, 6])\n",
      "targets: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 0., 0., 0.], device='cuda:0') shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "  print(\"inputs:\", batch.data, batch.data.shape)\n",
    "  print(\"targets:\",batch.label, \"shape:\", batch.label.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bEUUOMBJkdL",
    "outputId": "8ff61e19-ce66-4da2-d5bf-5e941093a923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[   2,  198,   18,    5,    3],\n",
      "        [   2,   18,    5,   37,  594],\n",
      "        [   2,    2,  109, 1448,    0],\n",
      "        [   2,   20,    5,   10,  356],\n",
      "        [   2, 1608, 2864,    0, 2510],\n",
      "        [   2,   44, 3506, 3542,   94],\n",
      "        [   2,    2,  953,   16,  586],\n",
      "        [   2,   18,    5,    0,   11],\n",
      "        [   2,  157,    7,  111,    2],\n",
      "        [   2,    0,   13,  593,   11],\n",
      "        [   2,   20,    5,    0,  368],\n",
      "        [   2,    0,   11,   11,   11],\n",
      "        [   2,    2,  198,   45,   93],\n",
      "        [   2,   48,   18,    5,    3],\n",
      "        [   1,    2,  448,   21,  982],\n",
      "        [   1,    2,    7,   15,  382],\n",
      "        [   1,    2,  671, 9691,    3],\n",
      "        [   1,    2,  357,   49,    0],\n",
      "        [   1,    2,  348, 1867, 1531],\n",
      "        [   1,    2,    0,   32,    3],\n",
      "        [   1,    2, 1939,  123,  232],\n",
      "        [   1,    2,   10, 1840, 3189],\n",
      "        [   1,    2,   48,   33,   39],\n",
      "        [   1,    2,   20,    5,  418],\n",
      "        [   1,    2,  429,    0,    3],\n",
      "        [   1,    2,  111,   31,    0],\n",
      "        [   1,    2,   88,   19, 1054],\n",
      "        [   1,    2, 3511,    4,  726],\n",
      "        [   1,    2,  203,  315,   32],\n",
      "        [   1,    2,    0,    0, 4571],\n",
      "        [   1,    2, 1937,   98,   44],\n",
      "        [   1,    2,   17,   34,    0],\n",
      "        [   1,    2,  196,  231,  102],\n",
      "        [   1,  499,  297,  591,   24],\n",
      "        [   1,    2,    2,   40,   16],\n",
      "        [   1,    2,  579,    5,  202],\n",
      "        [   1,    2,  306,  220,    0],\n",
      "        [   1,    2,    2,  179,  109],\n",
      "        [   1,    2,    0,   11,   11],\n",
      "        [   1,    2,   20,    5, 1417],\n",
      "        [   1,    2,   20,    5,  529],\n",
      "        [   1,    2,    0, 3666,    3],\n",
      "        [   1,    2,   18,    5, 2307],\n",
      "        [   1,    2,    2,    5, 7404],\n",
      "        [   1,    2,  198,   20,    5],\n",
      "        [   1,    2,   18,    5, 2496],\n",
      "        [   1,    2, 5601,    0,  371],\n",
      "        [   1,    2,  166,  186, 2051],\n",
      "        [   1,    2,    7,   15,  366],\n",
      "        [   1,    2,    2,    0,   24],\n",
      "        [   1,    1,    2,   32, 3872],\n",
      "        [   1,    1,    2,  254, 1017],\n",
      "        [   1,    1,    2,  705,   11],\n",
      "        [   1,    1,    2,  441,   16],\n",
      "        [   1,    1,    2, 2344, 7632],\n",
      "        [   1,    1,    2,  109,  158],\n",
      "        [   1,    1,    2,    0,   11],\n",
      "        [   1,    1,    1,    2,    0],\n",
      "        [   1,    1,    1,    2,    0],\n",
      "        [   1,    1,    1,    2, 1083],\n",
      "        [   1,    1,    1,    2,  176],\n",
      "        [   1,    1,    1,    2, 7603],\n",
      "        [   1,    1,    1,    2, 1046],\n",
      "        [   1,    1,    1,    2,    0]], device='cuda:0') torch.Size([64, 5])\n",
      "targets: tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0') shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in valid_iter:\n",
    "  print(\"inputs:\", batch.data, batch.data.shape)\n",
    "  print(\"targets:\",batch.label, \"shape:\", batch.label.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQ4VudN8Jn4p",
    "outputId": "6f2fdf8e-48da-4fbe-f6a2-3437d321a000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([   2,    2,    2,    2,    2,   14,   93,   26, 1216,  232,   11,  390,\n",
      "          71,  288,  195,  172,   11, 1610,    5,   71, 1537, 2316,   48,   14,\n",
      "         205,  733,  109,   68,   19,   11,   44,   15,    4,  555,   11,   29,\n",
      "          88,  376,   95,  788,    4,    0,   29,   82,  124,   35, 2481,  879,\n",
      "         185,  828,    9,  284,    0,   11, 2648,  284,    0,    5,   26,  174,\n",
      "           0,    3,  168,   72,   11], device='cuda:0') torch.Size([65])\n",
      "targets: tensor([1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 1., 0.], device='cuda:0') shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_iter:\n",
    "  print(\"inputs:\", batch.data[0], batch.data[0].shape)\n",
    "  print(\"targets:\",batch.label, \"shape:\", batch.label.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3owjIKD59osE"
   },
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "olHFPGVE9qO9"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, n_vocab, embed_dim, n_hidden, n_rnnlayers, n_outputs):\n",
    "    super(RNN, self).__init__()\n",
    "    self.V = n_vocab\n",
    "    self.D = embed_dim\n",
    "    self.M = n_hidden\n",
    "    self.K = n_outputs\n",
    "    self.L = n_rnnlayers\n",
    "\n",
    "    self.embed = nn.Embedding(self.V, self.D)\n",
    "    self.rnn = nn.LSTM(\n",
    "        input_size=self.D,\n",
    "        hidden_size=self.M,\n",
    "        num_layers=self.L,\n",
    "        batch_first=True)\n",
    "    self.fc = nn.Linear(self.M, self.K)\n",
    "  \n",
    "  def forward(self, X):\n",
    "    # initial hidden states\n",
    "    h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "    c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "\n",
    "    # embedding layer\n",
    "    # turns word indexes into word vectors\n",
    "    out = self.embed(X)\n",
    "\n",
    "    # get RNN unit output\n",
    "    out, _ = self.rnn(out, (h0, c0))\n",
    "\n",
    "    # max pool\n",
    "    out, _ = torch.max(out, 1)\n",
    "\n",
    "    # we only want h(T) at the final time step\n",
    "    out = self.fc(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDzixZyS9yDe",
    "outputId": "545bf314-1b33-49bf-82ef-cd74a44248a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embed): Embedding(11155, 100)\n",
       "  (rnn): LSTM(100, 100, batch_first=True)\n",
       "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 219,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(len(vocab_text), 100, 100, 1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gQvXbBX906c",
    "outputId": "8b0f1d4e-0579-4718-d701-d30bfb30bf22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6286    Valid Loss: 0.5879, Duration: 0:00:00.415046\n",
      "Epoch 2/10, Train Loss: 0.4566    Valid Loss: 0.5630, Duration: 0:00:00.380345\n",
      "Epoch 3/10, Train Loss: 0.2359    Valid Loss: 0.6512, Duration: 0:00:00.383875\n",
      "Epoch 4/10, Train Loss: 0.0866    Valid Loss: 0.8689, Duration: 0:00:00.368160\n",
      "Epoch 5/10, Train Loss: 0.0313    Valid Loss: 1.2934, Duration: 0:00:00.376413\n",
      "Epoch 6/10, Train Loss: 0.0134    Valid Loss: 1.3450, Duration: 0:00:00.367673\n",
      "Epoch 7/10, Train Loss: 0.0132    Valid Loss: 1.3078, Duration: 0:00:00.358503\n",
      "Epoch 8/10, Train Loss: 0.0055    Valid Loss: 1.4205, Duration: 0:00:00.365775\n",
      "Epoch 9/10, Train Loss: 0.0037    Valid Loss: 1.5629, Duration: 0:00:00.375591\n",
      "Epoch 10/10, Train Loss: 0.0027    Valid Loss: 1.5847, Duration: 0:00:00.363269\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs=10\n",
    "# STEP 5: INSTANTIATE LOSS CLASS\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# STEP 7: TRAIN THE MODEL\n",
    "\n",
    "train_losses= np.zeros(epochs)\n",
    "valid_losses= np.zeros(epochs)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  t0= datetime.now()\n",
    "  train_loss=[]\n",
    "  \n",
    "  model.train()\n",
    "  for batch in train_iter:\n",
    "   \n",
    "\n",
    "    # forward pass\n",
    "    output= model(batch.data)\n",
    "    batch.label = batch.label.unsqueeze(1)\n",
    "    batch.label = batch.label.float()\n",
    "    loss=criterion(output,batch.label)\n",
    "\n",
    "    # set gradients to zero \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "  \n",
    "  train_loss=np.mean(train_loss)\n",
    "      \n",
    "  valid_loss=[]\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for batch in valid_iter:\n",
    " \n",
    "      # forward pass\n",
    "      output= model(batch.data)\n",
    "      batch.label = batch.label.unsqueeze(1)\n",
    "      batch.label = batch.label.float()\n",
    "      loss=criterion(output,batch.label)\n",
    "      valid_loss.append(loss.item())\n",
    "\n",
    "    valid_loss=np.mean(valid_loss)\n",
    "  \n",
    "  # save Losses\n",
    "  train_losses[epoch]= train_loss\n",
    "  valid_losses[epoch]= valid_loss\n",
    "  dt= datetime.now()-t0\n",
    "  print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}    Valid Loss: {valid_loss:.4f}, Duration: {dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "yEmjYgO3-B3N"
   },
   "outputs": [],
   "source": [
    "# Accuracy- write a function to get accuracy\n",
    "# use this function to get accuracy and print accuracy\n",
    "def get_accuracy(data_iter, model):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    correct =0 \n",
    "    total =0\n",
    "    \n",
    "    for batch in data_iter:\n",
    "\n",
    "      output=model(batch.data)\n",
    "      _,indices = torch.max(output,dim=1)\n",
    "      correct+= (batch.label==indices).sum().item()\n",
    "      total += batch.label.shape[0]\n",
    "    \n",
    "    acc= correct/total\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6P31E7v-CVJ",
    "outputId": "e783c61a-6bba-423c-e835-d4c4e0353657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6626,\t Valid acc: 0.6719,\t Test acc: 0.6658\n"
     ]
    }
   ],
   "source": [
    "train_acc = get_accuracy(train_iter, model)\n",
    "valid_acc = get_accuracy(valid_iter, model)\n",
    "test_acc = get_accuracy(test_iter ,model)\n",
    "print(f'Train acc: {train_acc:.4f},\\t Valid acc: {valid_acc:.4f},\\t Test acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "_2MmgUFK-CYm"
   },
   "outputs": [],
   "source": [
    "# Write a function to get predictions\n",
    "\n",
    "def get_predictions(test_iter, model):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    predictions= np.array([])\n",
    "    y_test= np.array([])\n",
    "\n",
    "    for batch in test_iter:\n",
    "      \n",
    "      output=model(batch.data)\n",
    "      _,indices = torch.max(output,dim=1)\n",
    "      predictions=np.concatenate((predictions,indices.cpu().numpy())) \n",
    "      y_test = np.concatenate((y_test,batch.label.cpu().numpy())) \n",
    "      \n",
    "  return y_test, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdR6oIyQ-S2v"
   },
   "source": [
    "#### Exporting output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "ZtXvrN_c-S21"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_1(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "5adhMLwF-S3B"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['data']:\n",
    "  test_predictions.append(predict_sentiment_1(model,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "kBcORlZ--S3I"
   },
   "outputs": [],
   "source": [
    "# Rounding off predictions to 0 and 1\n",
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "3uouMwu_-S3P"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_LSTM_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9RYsVlwgXHH"
   },
   "source": [
    "## LSTM Model - Bidirectional with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "3-F0hGivJpQx"
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, n_vocab, embed_dim, n_hidden, n_rnnlayers, n_outputs, bidirectional, dropout_rate):\n",
    "    super(RNN, self).__init__()\n",
    "    self.V = n_vocab\n",
    "    self.D = embed_dim\n",
    "    self.M = n_hidden\n",
    "    self.K = n_outputs\n",
    "    self.L = n_rnnlayers\n",
    "    self.num_diections= bidirectional\n",
    "    self.dropout_rate=dropout_rate\n",
    "    \n",
    "    # embedding layer\n",
    "    self.embed = nn.Embedding(self.V, self.D)\n",
    "    \n",
    "    # rnn layers\n",
    "    self.rnn = nn.LSTM(\n",
    "        input_size=self.D,\n",
    "        hidden_size=self.M,\n",
    "        num_layers=self.L,\n",
    "        bidirectional=self.num_diections,\n",
    "        dropout= self.dropout_rate,\n",
    "        batch_first=True)\n",
    "    \n",
    "    # dense layer\n",
    "    self.fc = nn.Linear(self.M *2 , self.K)\n",
    "\n",
    "    # dropout layer\n",
    "    self.dropout= nn.Dropout(self.dropout_rate)\n",
    "  \n",
    "  def forward(self, X):\n",
    "    # initial hidden states\n",
    "    h0 = torch.zeros(self.L*2, X.size(0), self.M).to(device)\n",
    "    c0 = torch.zeros(self.L*2, X.size(0), self.M).to(device)\n",
    "\n",
    "    # embedding layer\n",
    "    # turns word indexes into word vectors\n",
    "    # X (batch_size, sentence length)\n",
    "    embedding = self.embed(X)   # (batch_size, sentence_length, emd_dim)\n",
    "    embedding= self.dropout(embedding) # (batch_size, sentence_length, emd_dim)\n",
    "\n",
    "    # get RNN unit output\n",
    "    output, (hidden,cell) = self.rnn(embedding, (h0, c0))\n",
    "\n",
    "\n",
    "    #output = [batch size, sent len, hid dim * num directions]\n",
    "    #hidden = [num layers * num directions, batch size, hid dim]\n",
    "    #cell = [num layers * num directions, batch size, hid dim]\n",
    "\n",
    "    # max pool\n",
    "    output, _ = torch.max(output, 1)\n",
    "    output= self.dropout(output)\n",
    "    # we only want h(T) at the final time step\n",
    "    output = self.fc(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "id": "ceR0j6Schyyb"
   },
   "outputs": [],
   "source": [
    "n_vocab = len(TEXT.vocab)\n",
    "embed_dim = 100\n",
    "n_hidden = 256 \n",
    "n_rnnlayers = 2\n",
    "n_outputs = 1 \n",
    "bidirectional = True \n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48EZZ6ijh86G",
    "outputId": "cc9ea522-b118-460b-f824-50e392650d2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embed): Embedding(11155, 100)\n",
       "  (rnn): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 234,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LSTM = RNN(n_vocab, embed_dim, n_hidden, n_rnnlayers, n_outputs, bidirectional, dropout_rate)\n",
    "model_LSTM.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BbBB9RSiCYl",
    "outputId": "10b9b628-ec7f-4574-a199-b38340005aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embed): Embedding(11155, 100)\n",
      "  (rnn): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxRBFx7SiEar",
    "outputId": "868f7975-5d9b-4f6b-98b5-5735d6482a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11155, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnDGs-aUiOyB"
   },
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lHdQVKYiP_Y",
    "outputId": "0652e449-65c5-4832-944a-ea53d1b377bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6389    Valid Loss: 0.6138, Duration: 0:00:01.219322\n",
      "Epoch 2/10, Train Loss: 0.6263    Valid Loss: 0.6269, Duration: 0:00:01.124877\n",
      "Epoch 3/10, Train Loss: 0.6102    Valid Loss: 0.5918, Duration: 0:00:01.123280\n",
      "Epoch 4/10, Train Loss: 0.5995    Valid Loss: 0.5905, Duration: 0:00:01.131024\n",
      "Epoch 5/10, Train Loss: 0.5791    Valid Loss: 0.5839, Duration: 0:00:01.134628\n",
      "Epoch 6/10, Train Loss: 0.5667    Valid Loss: 0.5809, Duration: 0:00:01.134667\n",
      "Epoch 7/10, Train Loss: 0.5561    Valid Loss: 0.5774, Duration: 0:00:01.143747\n",
      "Epoch 8/10, Train Loss: 0.5407    Valid Loss: 0.5996, Duration: 0:00:01.169770\n",
      "Epoch 9/10, Train Loss: 0.5200    Valid Loss: 0.5753, Duration: 0:00:01.143727\n",
      "Epoch 10/10, Train Loss: 0.5091    Valid Loss: 0.5786, Duration: 0:00:01.171825\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs=10\n",
    "# STEP 5: INSTANTIATE LOSS CLASS\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "\n",
    "optimizer = torch.optim.Adam(model_LSTM.parameters(), lr=learning_rate)\n",
    "\n",
    "# Freeze embedding Layer\n",
    "\n",
    "#freeze embeddings\n",
    "model_LSTM.embed.weight.requires_grad  = False\n",
    "\n",
    "# STEP 7: TRAIN THE MODEL\n",
    "\n",
    "train_losses= np.zeros(epochs)\n",
    "valid_losses= np.zeros(epochs)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  t0= datetime.now()\n",
    "  train_loss=[]\n",
    "  \n",
    "  model_LSTM.train()\n",
    "  for batch in train_iter:\n",
    "   \n",
    "    # forward pass\n",
    "    output= model_LSTM(batch.data)\n",
    "    batch.label = batch.label.unsqueeze(1)\n",
    "    batch.label = batch.label.float()\n",
    "    loss=criterion(output,batch.label)\n",
    "\n",
    "    # set gradients to zero \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "  \n",
    "  train_loss=np.mean(train_loss)\n",
    "      \n",
    "  valid_loss=[]\n",
    "  model_LSTM.eval()\n",
    "  with torch.no_grad():\n",
    "    for batch in valid_iter:\n",
    " \n",
    "      # forward pass\n",
    "      output= model_LSTM(batch.data)\n",
    "      batch.label = batch.label.unsqueeze(1)\n",
    "      #batch.label = batch.label.float()\n",
    "      loss=criterion(output,batch.label)\n",
    "      \n",
    "      valid_loss.append(loss.item())\n",
    "\n",
    "    valid_loss=np.mean(valid_loss)\n",
    "  \n",
    "  # save Losses\n",
    "  train_losses[epoch]= train_loss\n",
    "  valid_losses[epoch]= valid_loss\n",
    "  dt= datetime.now()-t0\n",
    "  print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}    Valid Loss: {valid_loss:.4f}, Duration: {dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "QCogfbFIlreG"
   },
   "outputs": [],
   "source": [
    "# Accuracy- write a function to get accuracy\n",
    "# use this function to get accuracy and print accuracy\n",
    "def get_accuracy(data_iter, model):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    correct =0 \n",
    "    total =0\n",
    "    \n",
    "    for batch in data_iter:\n",
    "\n",
    "      output=model(batch.data)\n",
    "      _,indices = torch.max(output,dim=1)\n",
    "      correct+= (batch.label==indices).sum().item()\n",
    "      total += batch.label.shape[0]\n",
    "    \n",
    "    acc= correct/total\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYomactfpFDV",
    "outputId": "0f42ada8-db48-4990-d030-b2a4aa38c443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6626,\t Valid acc: 0.6719,\t Test acc: 0.6658\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy on Train, Validation and Test Datasets\n",
    "train_acc = get_accuracy(train_iter, model_LSTM)\n",
    "valid_acc = get_accuracy(valid_iter, model_LSTM)\n",
    "test_acc = get_accuracy(test_iter ,model_LSTM)\n",
    "print(f'Train acc: {train_acc:.4f},\\t Valid acc: {valid_acc:.4f},\\t Test acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GB6XRIQGDYOp"
   },
   "source": [
    "#### Exporting output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "w5Z-APNn_5-T"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_1(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "Mw5gEUgx_5-k"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['data']:\n",
    "  test_predictions.append(predict_sentiment_1(model_LSTM,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "Q6ICE1Po_5-s"
   },
   "outputs": [],
   "source": [
    "# Rounding off predictions to 0 and 1\n",
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "7w6z2dHD_5-z"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_LSTM_BD.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V19gucqu4Xcc"
   },
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "T4wYdf_G4bNV"
   },
   "outputs": [],
   "source": [
    "# Creating a Vanilla RNN Function\n",
    "import torch.nn as nn\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.embedding(text)\n",
    "        #print(embedded.shape)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        #assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhl4TAdQD3qk",
    "outputId": "11a4eb96-b1f7-47b5-8d8a-8fe9b3d2200e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11155"
      ]
     },
     "execution_count": 252,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48W8azEOYY0u",
    "outputId": "583f31aa-1211-4aed-e378-47ceaae37718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6075, -0.8885, -0.3198,  ...,  0.9384,  0.3964, -0.0569],\n",
      "        [ 0.4732,  0.1814,  0.2208,  ..., -2.4314, -0.5819,  1.7075],\n",
      "        [-2.2917,  1.7018,  1.2485,  ...,  0.9472,  0.7102, -0.9532],\n",
      "        ...,\n",
      "        [ 1.3456,  1.4064,  1.9149,  ...,  1.3751, -0.0479, -0.8710],\n",
      "        [ 0.3412,  0.6755,  0.1827,  ..., -0.8494,  0.8603, -0.8099],\n",
      "        [-0.7106, -0.6365, -0.1531,  ..., -1.1898,  0.5334,  0.1502]])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOQhx8Y0XqJO",
    "outputId": "bfacbc24-144b-421b-8382-5e7fdd55f208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11155, 100])\n"
     ]
    }
   ],
   "source": [
    "embeddings = TEXT.vocab.vectors\n",
    "#VanillaRNN.embedding.weight.data.copy_(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "gIbSoy6q4i9J"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "learning_rate = 0.005\n",
    "\n",
    "model_VRNN = VanillaRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSCI4LHP4i_9",
    "outputId": "274982f6-debf-4fd4-ceef-3ab0575a8e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,207,405 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model_VRNN):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "fLA0Rf9_4jDT"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Defining Optimizer\n",
    "optimizer = optim.Adam(model_VRNN.parameters(), lr=1e-3)\n",
    "\n",
    "# Defining Loss Function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Pushing Model to GPU\n",
    "model_VRNN = model_VRNN.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "XllEDF_04t-r"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "kcZSrM194uB1"
   },
   "outputs": [],
   "source": [
    "# Defining training loop\n",
    "def train_func(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.data).squeeze(1)\n",
    "        #label = label.float()\n",
    "        #print(len(predictions))\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "Gsl6EC4l4uFj"
   },
   "outputs": [],
   "source": [
    "# Defining evaluation loop\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.data).squeeze(1)\n",
    "            #label = label.float()\n",
    "            #print(type(batch.label))\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "WbnXfWFX5PMA"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ya0NV9q_Cupa"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPqX_Qtu5PPU",
    "outputId": "45f4653b-408b-44c7-d314-051884f61092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.657 | Train Acc: 62.94%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 61.45%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.619 | Train Acc: 67.39%\n",
      "\t Val. Loss: 0.651 |  Val. Acc: 66.87%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.580 | Train Acc: 70.22%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 64.10%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.569 | Train Acc: 71.64%\n",
      "\t Val. Loss: 0.670 |  Val. Acc: 62.79%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.523 | Train Acc: 74.27%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 64.02%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train Acc: 78.59%\n",
      "\t Val. Loss: 0.710 |  Val. Acc: 65.36%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train Acc: 82.16%\n",
      "\t Val. Loss: 0.776 |  Val. Acc: 63.65%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train Acc: 86.38%\n",
      "\t Val. Loss: 0.893 |  Val. Acc: 58.88%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train Acc: 89.84%\n",
      "\t Val. Loss: 0.987 |  Val. Acc: 60.39%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train Acc: 92.26%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 60.70%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_func(model_VRNN, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model_VRNN, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_VRNN.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLVRiNNc5PUl",
    "outputId": "c57d77ea-6c32-4eaa-d984-e81bdafa48f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.657 | Test Acc: 66.67%\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy on Test Dataset\n",
    "model_VRNN.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model_VRNN, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azM1NSOMDCAD"
   },
   "source": [
    "#### Exporting output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "hSaRrUQH-FUw"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_2(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    #length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    #length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "hioU69Bb-FVM"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['text']:\n",
    "  test_predictions.append(predict_sentiment_2(model_VRNN,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "w63zvlVW-FVT"
   },
   "outputs": [],
   "source": [
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "6cCK3ajK-FVa"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_VRNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akZnmAdunp_5"
   },
   "source": [
    "## Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7y7emZHokTM"
   },
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "LRVL4-N1omxn"
   },
   "outputs": [],
   "source": [
    "TEXT = ttd.Field(\n",
    "    sequential=True,\n",
    "    #batch_first=True,\n",
    "    lower=True,\n",
    "    tokenize='spacy',\n",
    "    #pad_first=True,\n",
    "    include_lengths = True)\n",
    "\n",
    "LABEL = ttd.LabelField(dtype = torch.float)#, batch_first=True)\n",
    "\n",
    "#Train dataset\n",
    "Train_dataset = ttd.TabularDataset(\n",
    "    path='train2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT),('label', LABEL)]\n",
    ")\n",
    "\n",
    "#Test dataset\n",
    "Test_dataset = ttd.TabularDataset(\n",
    "    path='test2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "mZ7lmWAWFFwF"
   },
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "training_dataset, testing_dataset = Train_dataset.split(split_ratio=0.7,random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "1_i2h7cTFFwI"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "SEED=1234\n",
    "training_dataset, valid_dataset = training_dataset.split(random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZL0eO0lo0zl",
    "outputId": "a6875a3c-5df3-45cd-8e56-78dfbad17868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4579\n",
      "Number of validation examples: 1963\n",
      "Number of testing examples: 2804\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(training_dataset)}')\n",
    "print(f'Number of validation examples: {len(valid_dataset)}')\n",
    "print(f'Number of testing examples: {len(testing_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "um6sPLEto0zz"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(training_dataset, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "id": "jF7FXjcEo0z8"
   },
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(training_dataset)\n",
    "LABEL.build_vocab(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "xoA-M0eHo-vR"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter = ttd.BucketIterator.splits((training_dataset,valid_dataset), \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "QuAKBDuN5Uu2"
   },
   "outputs": [],
   "source": [
    "test_iter = ttd.BucketIterator(testing_dataset, \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGosF9YlDsUt"
   },
   "source": [
    "#### Defining the Bidirectional function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "utc2K2dJpIez"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BiDRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "id": "KIZBIpdgpTLf"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model_BDRNN = BiDRNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fl-FUrTKpVsS",
    "outputId": "339fd4c6-7dcd-4b1e-c605-715bd09e05ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,426,157 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model_BDRNN):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdMIn-qzpfhJ",
    "outputId": "a58db3a6-dad9-4359-9bba-e4312d15ed1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11155, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRqCbFx6piTt",
    "outputId": "4ed51677-a469-4ffd-90a5-a49fcaa4ebc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2644,  0.1614,  1.9024,  ..., -0.0895,  0.0145,  0.3330],\n",
       "        [ 0.8016,  0.1560,  0.7757,  ..., -1.2540, -1.0613, -0.6615],\n",
       "        [-0.1094,  0.9084,  2.0204,  ...,  0.2334, -0.8108,  0.4120],\n",
       "        ...,\n",
       "        [ 1.0947, -0.9882,  0.3398,  ...,  2.3886, -0.1666,  0.2511],\n",
       "        [-0.2642,  0.1371, -1.1057,  ..., -0.1192,  0.1426, -1.4332],\n",
       "        [-0.3453,  1.8291,  0.3241,  ..., -0.0213,  0.8426, -1.3327]])"
      ]
     },
     "execution_count": 283,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_BDRNN.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2fiVOdTpk8I",
    "outputId": "4d132aa6-e15c-4538-c776-c5f714ee38a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1094,  0.9084,  2.0204,  ...,  0.2334, -0.8108,  0.4120],\n",
      "        ...,\n",
      "        [ 1.0947, -0.9882,  0.3398,  ...,  2.3886, -0.1666,  0.2511],\n",
      "        [-0.2642,  0.1371, -1.1057,  ..., -0.1192,  0.1426, -1.4332],\n",
      "        [-0.3453,  1.8291,  0.3241,  ..., -0.0213,  0.8426, -1.3327]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model_BDRNN.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model_BDRNN.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model_BDRNN.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "id": "7hYCqE-OpnoX"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model_BDRNN.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model_BDRNN = model_BDRNN.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "id": "gAN4dhAapsLu"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "id": "okbk7ahDpubN"
   },
   "outputs": [],
   "source": [
    "def train_func2(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.data\n",
    "        #print(\"Length of texts: \",len(text))\n",
    "        #print('Lenghts of text sequences: ',len(text_lengths))\n",
    "        #print(\"Length of labels: \",len(batch.label))\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "id": "BCR3hBG-pzlh"
   },
   "outputs": [],
   "source": [
    "def evaluate2(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.data\n",
    "            #print(\"Length of texts: \",len(text))\n",
    "            #print('Lenghts of text sequences: ',len(text_lengths))\n",
    "            #print(\"Length of labels: \",len(batch.label))\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "id": "-s7Z3tRdFfAF"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLsl4OtAD1ot"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bbUBI22p47T",
    "outputId": "617aedad-dce9-4af3-c5c3-fb0fa8980088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.634 | Train Acc: 67.07%\n",
      "\t Val. Loss: 0.626 |  Val. Acc: 67.35%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.587 | Train Acc: 70.42%\n",
      "\t Val. Loss: 0.561 |  Val. Acc: 69.29%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.563 | Train Acc: 71.76%\n",
      "\t Val. Loss: 0.564 |  Val. Acc: 71.40%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.517 | Train Acc: 76.04%\n",
      "\t Val. Loss: 0.524 |  Val. Acc: 74.73%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.478 | Train Acc: 78.66%\n",
      "\t Val. Loss: 0.516 |  Val. Acc: 75.39%\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.456 | Train Acc: 79.15%\n",
      "\t Val. Loss: 0.530 |  Val. Acc: 75.81%\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.427 | Train Acc: 80.82%\n",
      "\t Val. Loss: 0.516 |  Val. Acc: 75.36%\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.385 | Train Acc: 83.47%\n",
      "\t Val. Loss: 0.515 |  Val. Acc: 76.69%\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.356 | Train Acc: 85.07%\n",
      "\t Val. Loss: 0.518 |  Val. Acc: 76.37%\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.328 | Train Acc: 85.84%\n",
      "\t Val. Loss: 0.586 |  Val. Acc: 77.55%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_func2(model_BDRNN, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate2(model_BDRNN, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_BDRNN.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOGCXtR2p7sX",
    "outputId": "02cbeeb3-8778-47f0-ac1e-cfa90efbc809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.506 | Test Acc: 77.46%\n"
     ]
    }
   ],
   "source": [
    "model_BDRNN.load_state_dict(torch.load('tut2-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate2(model_BDRNN, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "id": "P6Ar3DQYYc_A"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_1(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "id": "tk51nVyB9D0c"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['text']:\n",
    "  test_predictions.append(predict_sentiment_1(model_BDRNN,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "id": "huQ4id9K9Zby"
   },
   "outputs": [],
   "source": [
    "# Rounding off to 0 and 1\n",
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "id": "1LMiX_IF9dwh"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_BiDRNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY0PIMb-CQap"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaN-Qh7DQjZd"
   },
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "id": "AMOhkUVhHR1s"
   },
   "outputs": [],
   "source": [
    "TEXT = ttd.Field(\n",
    "    sequential=True,\n",
    "    batch_first=True,\n",
    "    lower=True,\n",
    "    tokenize='spacy',\n",
    "    pad_first=True)\n",
    "    #include_lengths = True)\n",
    "\n",
    "LABEL = ttd.LabelField(dtype = torch.float)#, batch_first=True)\n",
    "\n",
    "#Train dataset\n",
    "Train_dataset = ttd.TabularDataset(\n",
    "    path='train2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT),('label', LABEL)]\n",
    ")\n",
    "\n",
    "#Test dataset\n",
    "Test_dataset = ttd.TabularDataset(\n",
    "    path='test2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "7ZzhFW0RHR1y"
   },
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "training_dataset, testing_dataset = Train_dataset.split(split_ratio=0.7,random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "id": "vf2sxbAVHR1z"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "SEED=1234\n",
    "training_dataset, valid_dataset = training_dataset.split(random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycbz7dk_HR11",
    "outputId": "d9de5aa8-5d94-4168-db76-ff4182f4f3b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4579\n",
      "Number of validation examples: 1963\n",
      "Number of testing examples: 2804\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(training_dataset)}')\n",
    "print(f'Number of validation examples: {len(valid_dataset)}')\n",
    "print(f'Number of testing examples: {len(testing_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "id": "bdXWmuIjHR13"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(training_dataset, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "id": "93zNCmvxHR14"
   },
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(training_dataset)\n",
    "LABEL.build_vocab(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "id": "VRv1ZFwjHR16"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter = ttd.BucketIterator.splits((training_dataset,valid_dataset), \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "id": "FovY50xiHR17"
   },
   "outputs": [],
   "source": [
    "test_iter = ttd.BucketIterator(testing_dataset, \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ctM-nAIHkKN"
   },
   "source": [
    "#### Creating a CNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "id": "uyovJzOUHm1h"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CNN_model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "id": "bnwwDzNuHsrt"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,2,2]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model_CNN = CNN_model(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k6IBEK4PH8FI",
    "outputId": "af6fee94-29a5-4b6e-edb8-f4ae3e06a3b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,176,101 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model_CNN):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "retwIdX6IBuC",
    "outputId": "00ecaf22-16bc-475d-a301-f5cf43f1841f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1318, -0.3035,  1.0498,  ..., -0.1555, -0.5055,  0.1247],\n",
       "        [-0.4578,  0.8600,  0.8540,  ...,  3.0938,  0.9062,  0.7151],\n",
       "        [-2.7232,  0.1180, -0.5694,  ..., -0.5942,  1.4725,  0.3334],\n",
       "        ...,\n",
       "        [ 0.6199, -0.3344,  0.4038,  ..., -0.7188, -1.0255,  2.5693],\n",
       "        [-0.0091,  1.5333,  0.7273,  ...,  2.3677,  0.1274, -0.9346],\n",
       "        [ 1.0803,  0.3010, -0.9273,  ..., -0.2327,  0.4826,  0.1647]])"
      ]
     },
     "execution_count": 331,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model_CNN.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "id": "H8q04NXnIHWc"
   },
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model_CNN.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model_CNN.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "id": "61Q2FWQ3IM6W"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model_CNN.parameters(),lr=0.001)\n",
    "#optimizer = optim.SGD(model_CNN.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model_CNN = model_CNN.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "id": "_veyXj5yIOUB"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "id": "ucbq5oGvIQP2"
   },
   "outputs": [],
   "source": [
    "def train_func3(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.data).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "id": "405rWbaoISdS"
   },
   "outputs": [],
   "source": [
    "def evaluate3(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.data).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "id": "iGWhZkAdIWZb"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhXx0GL_ILJf"
   },
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_EPdpdjaIavu",
    "outputId": "9377007e-dab4-4b55-857c-afdb7df0b086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.623 | Train Acc: 66.86%\n",
      "\t Val. Loss: 0.569 |  Val. Acc: 70.29%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.519 | Train Acc: 75.64%\n",
      "\t Val. Loss: 0.535 |  Val. Acc: 74.80%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.459 | Train Acc: 79.41%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 77.05%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train Acc: 82.29%\n",
      "\t Val. Loss: 0.490 |  Val. Acc: 77.12%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train Acc: 84.46%\n",
      "\t Val. Loss: 0.515 |  Val. Acc: 77.30%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train Acc: 87.02%\n",
      "\t Val. Loss: 0.509 |  Val. Acc: 76.64%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train Acc: 89.29%\n",
      "\t Val. Loss: 0.527 |  Val. Acc: 76.59%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train Acc: 91.69%\n",
      "\t Val. Loss: 0.550 |  Val. Acc: 76.64%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train Acc: 93.61%\n",
      "\t Val. Loss: 0.567 |  Val. Acc: 76.19%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train Acc: 95.08%\n",
      "\t Val. Loss: 0.607 |  Val. Acc: 76.06%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_func3(model_CNN, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate3(model_CNN, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_CNN.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24C4A30xEiOi",
    "outputId": "3f14bd80-0ff5-4d4a-eea3-633eb5e42918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.475 | Test Acc: 78.42%\n"
     ]
    }
   ],
   "source": [
    "model_CNN.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate3(model_CNN, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM92geJrRAjk"
   },
   "source": [
    "#### Making final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "id": "TatGSVr8RAjk"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "model_CNN.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence.lower())]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "id": "hKLGBMiPRAjn"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['text']:\n",
    "  test_predictions.append(predict_sentiment(model_CNN,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "id": "Tv0HExQIRAjr"
   },
   "outputs": [],
   "source": [
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "id": "XsCGfagARAjt"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_CNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rW9cKb57GPYd"
   },
   "source": [
    "#### Freezing weight embeddings and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3Jyo-lmGOpp",
    "outputId": "778bc6e1-012b-4f29-93cf-dde2ddfdb73e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train Acc: 84.13%\n",
      "\t Val. Loss: 0.506 |  Val. Acc: 76.64%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train Acc: 86.00%\n",
      "\t Val. Loss: 0.510 |  Val. Acc: 76.77%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train Acc: 86.09%\n",
      "\t Val. Loss: 0.520 |  Val. Acc: 76.80%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train Acc: 86.74%\n",
      "\t Val. Loss: 0.519 |  Val. Acc: 76.62%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train Acc: 88.26%\n",
      "\t Val. Loss: 0.524 |  Val. Acc: 76.42%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train Acc: 89.09%\n",
      "\t Val. Loss: 0.536 |  Val. Acc: 75.66%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train Acc: 89.06%\n",
      "\t Val. Loss: 0.539 |  Val. Acc: 75.91%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train Acc: 90.39%\n",
      "\t Val. Loss: 0.552 |  Val. Acc: 76.67%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train Acc: 90.64%\n",
      "\t Val. Loss: 0.559 |  Val. Acc: 76.57%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train Acc: 91.24%\n",
      "\t Val. Loss: 0.591 |  Val. Acc: 75.91%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "FREEZING = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "#freeze embeddings\n",
    "model_CNN.embedding.weight.requires_grad = unfrozen = False\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_func3(model_CNN, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate3(model_CNN, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_CNN.state_dict(), 'tut5-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    if (epoch + 1) >= FREEZING:\n",
    "        #unfreeze embeddings\n",
    "        model_CNN.embedding.weight.requires_grad = unfrozen = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BaSbzPzIeG8",
    "outputId": "b560f8ad-d471-44e2-9b59-f4957676e42e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.488 | Test Acc: 78.12%\n"
     ]
    }
   ],
   "source": [
    "model_CNN.load_state_dict(torch.load('tut5-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate3(model_CNN, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0eC5WHYEl4P"
   },
   "source": [
    "#### Making final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "id": "OyS2AhfpU0oA"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "model_CNN.load_state_dict(torch.load('tut5-model.pt'))\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence.lower())]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "id": "7oLnf15vtkau"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['text']:\n",
    "  test_predictions.append(predict_sentiment(model_CNN,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "id": "NCWXRXAKuGlA"
   },
   "outputs": [],
   "source": [
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "id": "FRduhnP5uKv1"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_CNN_FW.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrKPf9z7EpHJ"
   },
   "source": [
    "## We observe that the initial CNN model gave us the best results on test dataset with 78.42% accuracy and hence we will choose that for making final predictions, which will be implemented in the final file."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
