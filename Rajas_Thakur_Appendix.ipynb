{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kIM2yMbgktwv"
   },
   "source": [
    "## Inspirations: https://github.com/bentrevett/pytorch-sentiment-analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "id": "ggUwYQmgf9Cb"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext.data as ttd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cmoB1tvcgIN8",
    "outputId": "a6fded7a-f789-4cbe-ddbe-0715a69df0f0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ft2QkqmQgN7q",
    "outputId": "cd635bdf-cf20-4fb3-d2db-1dc69de7537b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: 'drive/My Drive/AppliedNLP/HW3'\n",
      "/content/drive/My Drive/AppliedNLP/HW3\n"
     ]
    }
   ],
   "source": [
    "cd 'drive/My Drive/AppliedNLP/HW3'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GHfs_fdrPcL3"
   },
   "source": [
    "### Data Preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "id": "ur3eIicOPg4g"
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m8XSG1-uQKMv",
    "outputId": "fd632219-4b89-41d2-c8fa-cf4484e9ae92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 9346 entries, 0 to 9345\n",
      "Data columns (total 3 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      9346 non-null   int64 \n",
      " 1   text    9346 non-null   object\n",
      " 2   target  9346 non-null   int64 \n",
      "dtypes: int64(2), object(1)\n",
      "memory usage: 219.2+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tb1wE2qRj2L1",
    "outputId": "9e9cdc4e-f8b3-4498-ff97-ff76820c9083"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3894 entries, 0 to 3893\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   id      3894 non-null   int64 \n",
      " 1   text    3894 non-null   object\n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 61.0+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "id": "3DmtfGsBiAKc"
   },
   "outputs": [],
   "source": [
    "train.drop(['id'],axis=1,inplace= True)\n",
    "test.drop(['id'],axis=1,inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "id": "M1JMs0rEkUqY"
   },
   "outputs": [],
   "source": [
    "train.columns = ['data', 'labels']\n",
    "test.columns = ['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "id": "VANu-zf2lQO9"
   },
   "outputs": [],
   "source": [
    "train.to_csv('train2.csv', index=False)\n",
    "test.to_csv('test2.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "id": "VJgoD2Hsgk7k"
   },
   "outputs": [],
   "source": [
    "TEXT = ttd.Field(\n",
    "    sequential=True,\n",
    "    batch_first=True,\n",
    "    lower=True,\n",
    "    tokenize='spacy',\n",
    "    pad_first=True)\n",
    "\n",
    "LABEL = ttd.LabelField(dtype = torch.float, batch_first=True)\n",
    "\n",
    "#Train dataset\n",
    "Train_dataset = ttd.TabularDataset(\n",
    "    path='train2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT),('label', LABEL)]\n",
    ")\n",
    "\n",
    "#Test dataset\n",
    "Test_dataset = ttd.TabularDataset(\n",
    "    path='test2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "id": "6hS3eltrzeke"
   },
   "outputs": [],
   "source": [
    "ex = Train_dataset.examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FDkn4P9a4H0P",
    "outputId": "77ae7d8d-7f31-4c97-84f1-32a87ee40278"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torchtext.data.example.Example"
      ]
     },
     "execution_count": 196,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l728QWX3zkxM",
    "outputId": "b0c73678-d78d-4ca1-fd17-23200343bbf4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@user',\n",
       " 'she',\n",
       " 'should',\n",
       " 'ask',\n",
       " 'a',\n",
       " 'few',\n",
       " 'native',\n",
       " 'americans',\n",
       " 'what',\n",
       " 'their',\n",
       " 'take',\n",
       " 'on',\n",
       " 'this',\n",
       " 'is',\n",
       " '.']"
      ]
     },
     "execution_count": 197,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "TyO1xB9dzrdY",
    "outputId": "ae0f0100-14aa-495e-a989-a62a83856e58"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 198,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex.label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kvmxfX5OGLJB"
   },
   "source": [
    "### Splitting into Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "id": "IH18at01g3oA"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "SEED=1234\n",
    "training_dataset, testing_dataset = Train_dataset.split(split_ratio=0.7,random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "id": "hfh9owBpGm0A"
   },
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "training_dataset, valid_dataset = training_dataset.split(random_state = random.seed(SEED)) # default is 0.7\n",
    "#training_dataset, valid_dataset = Train_dataset.split(random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OD3nJogaGupy",
    "outputId": "7a1c09e5-9cbb-41d8-fd2b-7aab033ab0f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4579\n",
      "Number of validation examples: 1963\n",
      "Number of testing examples: 2804\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(training_dataset)}')\n",
    "print(f'Number of validation examples: {len(valid_dataset)}')\n",
    "print(f'Number of testing examples: {len(testing_dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNt0cDTgHKWB"
   },
   "source": [
    "### Using Pretrained Embeddings from Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "id": "6ieo_C0AlvE6"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(training_dataset, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "id": "dTkkNIV4iYrN"
   },
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(training_dataset)\n",
    "LABEL.build_vocab(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "Lew5Hi_klyi5"
   },
   "outputs": [],
   "source": [
    "vocab_text = TEXT.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "id": "bqCyTEoXl7JZ"
   },
   "outputs": [],
   "source": [
    "#vocab_text.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "id": "ts_z-QhKJI3q"
   },
   "outputs": [],
   "source": [
    "#vocab_text.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tM30vqWqinab",
    "outputId": "29280cfa-f21d-409f-c9a2-d3e73716d1b8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11155"
      ]
     },
     "execution_count": 207,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vocab_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "id": "YBeGypYQBSgf"
   },
   "outputs": [],
   "source": [
    "vocab_label = LABEL.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YE2Ud9bvBZLm",
    "outputId": "15a0270d-c36d-4f09-bedc-6a69e5d43210"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<function torchtext.vocab._default_unk_index>, {'0': 0, '1': 1})"
      ]
     },
     "execution_count": 209,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_label.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ni84WH6UBdbV",
    "outputId": "842803de-75e4-4450-ec97-5fb6188e5fb9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0', '1']"
      ]
     },
     "execution_count": 210,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_label.itos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q9fgCUSA7ju4",
    "outputId": "5b7ee373-c9b1-4892-c8a8-aedcfbde3d89"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "id": "eMQPnFD0JXZW"
   },
   "outputs": [],
   "source": [
    "train_iter, valid_iter = ttd.BucketIterator.splits((training_dataset,valid_dataset), \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {
    "id": "2_ssoUVcD2CJ"
   },
   "outputs": [],
   "source": [
    "test_iter = ttd.BucketIterator(testing_dataset, \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "id": "QIgUTuHtkK2T"
   },
   "outputs": [],
   "source": [
    "testing_iter = ttd.BucketIterator(Test_dataset, \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4sMK69OvJist",
    "outputId": "7b807ad5-294b-4e66-d02e-bb698f4410fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[    2,    44,  8230, 10418,     3,    24],\n",
      "        [    2,    18,     5,  1007,  1007,  1007],\n",
      "        [    2,    37,    32,    88,   320,     3],\n",
      "        [    2,  1380,    13,     4,  7451,    11],\n",
      "        [    2,    50,    19,    96,    35,    11],\n",
      "        [    2,   550,    21,    10,   215,  9565],\n",
      "        [    2,     2,    20,     5,    37,   594],\n",
      "        [    2,     7,    15,   796,     3,   371],\n",
      "        [    2,     2,    18,     5,  6411,    11],\n",
      "        [    2,     2,     2,    18,     5,  1367],\n",
      "        [    2,     2,  9117,   631,   220,    16],\n",
      "        [    2,  1624,   128,   173,  3002,     3],\n",
      "        [    2,     2,    18,     5,   593,  1089],\n",
      "        [    2,     2,   718,   113,  9245,   213],\n",
      "        [    2,     2, 11026,    15,     4,   243],\n",
      "        [    2,     2,  8030,   124,   435,    11],\n",
      "        [    2,     4,   481,   119,    33,    39],\n",
      "        [    2,  1557,     5,    75,   770,    11],\n",
      "        [    1,     2,   223,    86,    16,   179],\n",
      "        [    1,     2,     6,  1314,   579,    16],\n",
      "        [    1,     2,   408,    97,    32,    11],\n",
      "        [    1,     2,     7,    15,    71,   885],\n",
      "        [    1,     2,   109,  1207,    95,    11],\n",
      "        [    1,     2,    20,     5,    37,   306],\n",
      "        [    1,     2,   889,    99,  9916,     2],\n",
      "        [    1,     2,    40,    10,   720,     3],\n",
      "        [    1,     2,   306,   267,     3,     3],\n",
      "        [    1,     2,    80,   353,    14,    57],\n",
      "        [    1,     2,   196,   948,     7,   988],\n",
      "        [    1,     2,    18,     5,  8695,     2],\n",
      "        [    1,     2,  2726,   215,  9923,  1686],\n",
      "        [    1,     2,     2,    40,     4,   200],\n",
      "        [    1,     2,   198,    18,     5,     3],\n",
      "        [    1,     2,    17,    29,   156,    32],\n",
      "        [    1,     2,     2,    18,     5,  1985],\n",
      "        [    1,     2,   751,     5,    10,   720],\n",
      "        [    1,     2,     2,    14,  4117,     2],\n",
      "        [    1,     2,   196,     7,    15,  1367],\n",
      "        [    1,     2,     7,    15,    10,   821],\n",
      "        [    1,     2,     2,     7,    15,  1651],\n",
      "        [    1,     2,    20,     5,  6521,    11],\n",
      "        [    1,     2,  3532,    72,  5810,    11],\n",
      "        [    1,     2,   519,    56,    17,  9020],\n",
      "        [    1,     2,     2,   109,  1664,     3],\n",
      "        [    1,     2,    43,   256,  5587,   727],\n",
      "        [    1,     2,  7546,   239,   303,  5110],\n",
      "        [    1,     2,   179,    18,     5,   316],\n",
      "        [    1,     2,    10,   312,    12,  7570],\n",
      "        [    1,     2,  5853,   209,  1725,     3],\n",
      "        [    1,     2,    18,     5,   115,  6950],\n",
      "        [    1,     2,     2,    44,  1861,     3],\n",
      "        [    1,     2,   200,   193,    12,    24],\n",
      "        [    1,   291,    47,    44,     5,  2113],\n",
      "        [    1,     2,    20,     5,    43,  9900],\n",
      "        [    1,     2,  8214,    11,   363,   353],\n",
      "        [    1,     2,     2,    20,     5,   197],\n",
      "        [    1,     2,   129,  1739,    63,   220],\n",
      "        [    1,     2,  2640,     5,  3235,    11],\n",
      "        [    1,     2,    17,    34,   568,    11],\n",
      "        [    1,     2,    20,     5,   256,  1594],\n",
      "        [    1,     2,   624,    11,    11,    11],\n",
      "        [    1,     2,     2,    32,   129,  1544],\n",
      "        [    1,     2,  3537,    56,   568,     3],\n",
      "        [    1,     2,     2,  4160,  2450,  7085]], device='cuda:0') torch.Size([64, 6])\n",
      "targets: tensor([0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 1., 1., 0., 0., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1., 0., 0., 1., 1.,\n",
      "        0., 0., 1., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 1., 1., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 1., 0., 0., 0.], device='cuda:0') shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_iter:\n",
    "  print(\"inputs:\", batch.data, batch.data.shape)\n",
    "  print(\"targets:\",batch.label, \"shape:\", batch.label.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7bEUUOMBJkdL",
    "outputId": "8ff61e19-ce66-4da2-d5bf-5e941093a923"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([[   2,  198,   18,    5,    3],\n",
      "        [   2,   18,    5,   37,  594],\n",
      "        [   2,    2,  109, 1448,    0],\n",
      "        [   2,   20,    5,   10,  356],\n",
      "        [   2, 1608, 2864,    0, 2510],\n",
      "        [   2,   44, 3506, 3542,   94],\n",
      "        [   2,    2,  953,   16,  586],\n",
      "        [   2,   18,    5,    0,   11],\n",
      "        [   2,  157,    7,  111,    2],\n",
      "        [   2,    0,   13,  593,   11],\n",
      "        [   2,   20,    5,    0,  368],\n",
      "        [   2,    0,   11,   11,   11],\n",
      "        [   2,    2,  198,   45,   93],\n",
      "        [   2,   48,   18,    5,    3],\n",
      "        [   1,    2,  448,   21,  982],\n",
      "        [   1,    2,    7,   15,  382],\n",
      "        [   1,    2,  671, 9691,    3],\n",
      "        [   1,    2,  357,   49,    0],\n",
      "        [   1,    2,  348, 1867, 1531],\n",
      "        [   1,    2,    0,   32,    3],\n",
      "        [   1,    2, 1939,  123,  232],\n",
      "        [   1,    2,   10, 1840, 3189],\n",
      "        [   1,    2,   48,   33,   39],\n",
      "        [   1,    2,   20,    5,  418],\n",
      "        [   1,    2,  429,    0,    3],\n",
      "        [   1,    2,  111,   31,    0],\n",
      "        [   1,    2,   88,   19, 1054],\n",
      "        [   1,    2, 3511,    4,  726],\n",
      "        [   1,    2,  203,  315,   32],\n",
      "        [   1,    2,    0,    0, 4571],\n",
      "        [   1,    2, 1937,   98,   44],\n",
      "        [   1,    2,   17,   34,    0],\n",
      "        [   1,    2,  196,  231,  102],\n",
      "        [   1,  499,  297,  591,   24],\n",
      "        [   1,    2,    2,   40,   16],\n",
      "        [   1,    2,  579,    5,  202],\n",
      "        [   1,    2,  306,  220,    0],\n",
      "        [   1,    2,    2,  179,  109],\n",
      "        [   1,    2,    0,   11,   11],\n",
      "        [   1,    2,   20,    5, 1417],\n",
      "        [   1,    2,   20,    5,  529],\n",
      "        [   1,    2,    0, 3666,    3],\n",
      "        [   1,    2,   18,    5, 2307],\n",
      "        [   1,    2,    2,    5, 7404],\n",
      "        [   1,    2,  198,   20,    5],\n",
      "        [   1,    2,   18,    5, 2496],\n",
      "        [   1,    2, 5601,    0,  371],\n",
      "        [   1,    2,  166,  186, 2051],\n",
      "        [   1,    2,    7,   15,  366],\n",
      "        [   1,    2,    2,    0,   24],\n",
      "        [   1,    1,    2,   32, 3872],\n",
      "        [   1,    1,    2,  254, 1017],\n",
      "        [   1,    1,    2,  705,   11],\n",
      "        [   1,    1,    2,  441,   16],\n",
      "        [   1,    1,    2, 2344, 7632],\n",
      "        [   1,    1,    2,  109,  158],\n",
      "        [   1,    1,    2,    0,   11],\n",
      "        [   1,    1,    1,    2,    0],\n",
      "        [   1,    1,    1,    2,    0],\n",
      "        [   1,    1,    1,    2, 1083],\n",
      "        [   1,    1,    1,    2,  176],\n",
      "        [   1,    1,    1,    2, 7603],\n",
      "        [   1,    1,    1,    2, 1046],\n",
      "        [   1,    1,    1,    2,    0]], device='cuda:0') torch.Size([64, 5])\n",
      "targets: tensor([0., 0., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        1., 0., 1., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 0., 0., 1., 0., 0., 0., 1., 0., 0., 0.,\n",
      "        0., 0., 1., 0., 0., 1., 0., 0., 1., 0.], device='cuda:0') shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in valid_iter:\n",
    "  print(\"inputs:\", batch.data, batch.data.shape)\n",
    "  print(\"targets:\",batch.label, \"shape:\", batch.label.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qQ4VudN8Jn4p",
    "outputId": "6f2fdf8e-48da-4fbe-f6a2-3437d321a000"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs: tensor([   2,    2,    2,    2,    2,   14,   93,   26, 1216,  232,   11,  390,\n",
      "          71,  288,  195,  172,   11, 1610,    5,   71, 1537, 2316,   48,   14,\n",
      "         205,  733,  109,   68,   19,   11,   44,   15,    4,  555,   11,   29,\n",
      "          88,  376,   95,  788,    4,    0,   29,   82,  124,   35, 2481,  879,\n",
      "         185,  828,    9,  284,    0,   11, 2648,  284,    0,    5,   26,  174,\n",
      "           0,    3,  168,   72,   11], device='cuda:0') torch.Size([65])\n",
      "targets: tensor([1., 1., 1., 0., 1., 0., 1., 0., 1., 0., 1., 1., 1., 0., 0., 1., 0., 1.,\n",
      "        0., 1., 0., 0., 1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "        1., 0., 0., 0., 0., 1., 0., 1., 1., 0., 0., 1., 0., 1., 0., 0., 0., 0.,\n",
      "        1., 1., 1., 1., 1., 1., 0., 0., 1., 0.], device='cuda:0') shape: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "for batch in test_iter:\n",
    "  print(\"inputs:\", batch.data[0], batch.data[0].shape)\n",
    "  print(\"targets:\",batch.label, \"shape:\", batch.label.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3owjIKD59osE"
   },
   "source": [
    "## LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {
    "id": "olHFPGVE9qO9"
   },
   "outputs": [],
   "source": [
    "# Define the model\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, n_vocab, embed_dim, n_hidden, n_rnnlayers, n_outputs):\n",
    "    super(RNN, self).__init__()\n",
    "    self.V = n_vocab\n",
    "    self.D = embed_dim\n",
    "    self.M = n_hidden\n",
    "    self.K = n_outputs\n",
    "    self.L = n_rnnlayers\n",
    "\n",
    "    self.embed = nn.Embedding(self.V, self.D)\n",
    "    self.rnn = nn.LSTM(\n",
    "        input_size=self.D,\n",
    "        hidden_size=self.M,\n",
    "        num_layers=self.L,\n",
    "        batch_first=True)\n",
    "    self.fc = nn.Linear(self.M, self.K)\n",
    "  \n",
    "  def forward(self, X):\n",
    "    # initial hidden states\n",
    "    h0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "    c0 = torch.zeros(self.L, X.size(0), self.M).to(device)\n",
    "\n",
    "    # embedding layer\n",
    "    # turns word indexes into word vectors\n",
    "    out = self.embed(X)\n",
    "\n",
    "    # get RNN unit output\n",
    "    out, _ = self.rnn(out, (h0, c0))\n",
    "\n",
    "    # max pool\n",
    "    out, _ = torch.max(out, 1)\n",
    "\n",
    "    # we only want h(T) at the final time step\n",
    "    out = self.fc(out)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dDzixZyS9yDe",
    "outputId": "545bf314-1b33-49bf-82ef-cd74a44248a2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embed): Embedding(11155, 100)\n",
       "  (rnn): LSTM(100, 100, batch_first=True)\n",
       "  (fc): Linear(in_features=100, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 219,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = RNN(len(vocab_text), 100, 100, 1, 1)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4gQvXbBX906c",
    "outputId": "8b0f1d4e-0579-4718-d701-d30bfb30bf22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6286    Valid Loss: 0.5879, Duration: 0:00:00.415046\n",
      "Epoch 2/10, Train Loss: 0.4566    Valid Loss: 0.5630, Duration: 0:00:00.380345\n",
      "Epoch 3/10, Train Loss: 0.2359    Valid Loss: 0.6512, Duration: 0:00:00.383875\n",
      "Epoch 4/10, Train Loss: 0.0866    Valid Loss: 0.8689, Duration: 0:00:00.368160\n",
      "Epoch 5/10, Train Loss: 0.0313    Valid Loss: 1.2934, Duration: 0:00:00.376413\n",
      "Epoch 6/10, Train Loss: 0.0134    Valid Loss: 1.3450, Duration: 0:00:00.367673\n",
      "Epoch 7/10, Train Loss: 0.0132    Valid Loss: 1.3078, Duration: 0:00:00.358503\n",
      "Epoch 8/10, Train Loss: 0.0055    Valid Loss: 1.4205, Duration: 0:00:00.365775\n",
      "Epoch 9/10, Train Loss: 0.0037    Valid Loss: 1.5629, Duration: 0:00:00.375591\n",
      "Epoch 10/10, Train Loss: 0.0027    Valid Loss: 1.5847, Duration: 0:00:00.363269\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.01\n",
    "epochs=10\n",
    "# STEP 5: INSTANTIATE LOSS CLASS\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# STEP 7: TRAIN THE MODEL\n",
    "\n",
    "train_losses= np.zeros(epochs)\n",
    "valid_losses= np.zeros(epochs)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  t0= datetime.now()\n",
    "  train_loss=[]\n",
    "  \n",
    "  model.train()\n",
    "  for batch in train_iter:\n",
    "   \n",
    "\n",
    "    # forward pass\n",
    "    output= model(batch.data)\n",
    "    batch.label = batch.label.unsqueeze(1)\n",
    "    batch.label = batch.label.float()\n",
    "    loss=criterion(output,batch.label)\n",
    "\n",
    "    # set gradients to zero \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "  \n",
    "  train_loss=np.mean(train_loss)\n",
    "      \n",
    "  valid_loss=[]\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    for batch in valid_iter:\n",
    " \n",
    "      # forward pass\n",
    "      output= model(batch.data)\n",
    "      batch.label = batch.label.unsqueeze(1)\n",
    "      batch.label = batch.label.float()\n",
    "      loss=criterion(output,batch.label)\n",
    "      valid_loss.append(loss.item())\n",
    "\n",
    "    valid_loss=np.mean(valid_loss)\n",
    "  \n",
    "  # save Losses\n",
    "  train_losses[epoch]= train_loss\n",
    "  valid_losses[epoch]= valid_loss\n",
    "  dt= datetime.now()-t0\n",
    "  print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}    Valid Loss: {valid_loss:.4f}, Duration: {dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "id": "yEmjYgO3-B3N"
   },
   "outputs": [],
   "source": [
    "# Accuracy- write a function to get accuracy\n",
    "# use this function to get accuracy and print accuracy\n",
    "def get_accuracy(data_iter, model):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    correct =0 \n",
    "    total =0\n",
    "    \n",
    "    for batch in data_iter:\n",
    "\n",
    "      output=model(batch.data)\n",
    "      _,indices = torch.max(output,dim=1)\n",
    "      correct+= (batch.label==indices).sum().item()\n",
    "      total += batch.label.shape[0]\n",
    "    \n",
    "    acc= correct/total\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F6P31E7v-CVJ",
    "outputId": "e783c61a-6bba-423c-e835-d4c4e0353657"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6626,\t Valid acc: 0.6719,\t Test acc: 0.6658\n"
     ]
    }
   ],
   "source": [
    "train_acc = get_accuracy(train_iter, model)\n",
    "valid_acc = get_accuracy(valid_iter, model)\n",
    "test_acc = get_accuracy(test_iter ,model)\n",
    "print(f'Train acc: {train_acc:.4f},\\t Valid acc: {valid_acc:.4f},\\t Test acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {
    "id": "_2MmgUFK-CYm"
   },
   "outputs": [],
   "source": [
    "# Write a function to get predictions\n",
    "\n",
    "def get_predictions(test_iter, model):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    predictions= np.array([])\n",
    "    y_test= np.array([])\n",
    "\n",
    "    for batch in test_iter:\n",
    "      \n",
    "      output=model(batch.data)\n",
    "      _,indices = torch.max(output,dim=1)\n",
    "      predictions=np.concatenate((predictions,indices.cpu().numpy())) \n",
    "      y_test = np.concatenate((y_test,batch.label.cpu().numpy())) \n",
    "      \n",
    "  return y_test, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdR6oIyQ-S2v"
   },
   "source": [
    "#### Exporting output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "id": "ZtXvrN_c-S21"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_1(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "id": "5adhMLwF-S3B"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['data']:\n",
    "  test_predictions.append(predict_sentiment_1(model,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {
    "id": "kBcORlZ--S3I"
   },
   "outputs": [],
   "source": [
    "# Rounding off predictions to 0 and 1\n",
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "id": "3uouMwu_-S3P"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_LSTM_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q9RYsVlwgXHH"
   },
   "source": [
    "## LSTM Model - Bidirectional with dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {
    "id": "3-F0hGivJpQx"
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model\n",
    "class RNN(nn.Module):\n",
    "  def __init__(self, n_vocab, embed_dim, n_hidden, n_rnnlayers, n_outputs, bidirectional, dropout_rate):\n",
    "    super(RNN, self).__init__()\n",
    "    self.V = n_vocab\n",
    "    self.D = embed_dim\n",
    "    self.M = n_hidden\n",
    "    self.K = n_outputs\n",
    "    self.L = n_rnnlayers\n",
    "    self.num_diections= bidirectional\n",
    "    self.dropout_rate=dropout_rate\n",
    "    \n",
    "    # embedding layer\n",
    "    self.embed = nn.Embedding(self.V, self.D)\n",
    "    \n",
    "    # rnn layers\n",
    "    self.rnn = nn.LSTM(\n",
    "        input_size=self.D,\n",
    "        hidden_size=self.M,\n",
    "        num_layers=self.L,\n",
    "        bidirectional=self.num_diections,\n",
    "        dropout= self.dropout_rate,\n",
    "        batch_first=True)\n",
    "    \n",
    "    # dense layer\n",
    "    self.fc = nn.Linear(self.M *2 , self.K)\n",
    "\n",
    "    # dropout layer\n",
    "    self.dropout= nn.Dropout(self.dropout_rate)\n",
    "  \n",
    "  def forward(self, X):\n",
    "    # initial hidden states\n",
    "    h0 = torch.zeros(self.L*2, X.size(0), self.M).to(device)\n",
    "    c0 = torch.zeros(self.L*2, X.size(0), self.M).to(device)\n",
    "\n",
    "    # embedding layer\n",
    "    # turns word indexes into word vectors\n",
    "    # X (batch_size, sentence length)\n",
    "    embedding = self.embed(X)   # (batch_size, sentence_length, emd_dim)\n",
    "    embedding= self.dropout(embedding) # (batch_size, sentence_length, emd_dim)\n",
    "\n",
    "    # get RNN unit output\n",
    "    output, (hidden,cell) = self.rnn(embedding, (h0, c0))\n",
    "\n",
    "\n",
    "    #output = [batch size, sent len, hid dim * num directions]\n",
    "    #hidden = [num layers * num directions, batch size, hid dim]\n",
    "    #cell = [num layers * num directions, batch size, hid dim]\n",
    "\n",
    "    # max pool\n",
    "    output, _ = torch.max(output, 1)\n",
    "    output= self.dropout(output)\n",
    "    # we only want h(T) at the final time step\n",
    "    output = self.fc(output)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "id": "ceR0j6Schyyb"
   },
   "outputs": [],
   "source": [
    "n_vocab = len(TEXT.vocab)\n",
    "embed_dim = 100\n",
    "n_hidden = 256 \n",
    "n_rnnlayers = 2\n",
    "n_outputs = 1 \n",
    "bidirectional = True \n",
    "dropout_rate = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48EZZ6ijh86G",
    "outputId": "cc9ea522-b118-460b-f824-50e392650d2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (embed): Embedding(11155, 100)\n",
       "  (rnn): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
       "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
       "  (dropout): Dropout(p=0.5, inplace=False)\n",
       ")"
      ]
     },
     "execution_count": 234,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_LSTM = RNN(n_vocab, embed_dim, n_hidden, n_rnnlayers, n_outputs, bidirectional, dropout_rate)\n",
    "model_LSTM.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1BbBB9RSiCYl",
    "outputId": "10b9b628-ec7f-4574-a199-b38340005aa9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (embed): Embedding(11155, 100)\n",
      "  (rnn): LSTM(100, 256, num_layers=2, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (fc): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (dropout): Dropout(p=0.5, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FxRBFx7SiEar",
    "outputId": "868f7975-5d9b-4f6b-98b5-5735d6482a72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11155, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gnDGs-aUiOyB"
   },
   "source": [
    "#### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-lHdQVKYiP_Y",
    "outputId": "0652e449-65c5-4832-944a-ea53d1b377bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Train Loss: 0.6389    Valid Loss: 0.6138, Duration: 0:00:01.219322\n",
      "Epoch 2/10, Train Loss: 0.6263    Valid Loss: 0.6269, Duration: 0:00:01.124877\n",
      "Epoch 3/10, Train Loss: 0.6102    Valid Loss: 0.5918, Duration: 0:00:01.123280\n",
      "Epoch 4/10, Train Loss: 0.5995    Valid Loss: 0.5905, Duration: 0:00:01.131024\n",
      "Epoch 5/10, Train Loss: 0.5791    Valid Loss: 0.5839, Duration: 0:00:01.134628\n",
      "Epoch 6/10, Train Loss: 0.5667    Valid Loss: 0.5809, Duration: 0:00:01.134667\n",
      "Epoch 7/10, Train Loss: 0.5561    Valid Loss: 0.5774, Duration: 0:00:01.143747\n",
      "Epoch 8/10, Train Loss: 0.5407    Valid Loss: 0.5996, Duration: 0:00:01.169770\n",
      "Epoch 9/10, Train Loss: 0.5200    Valid Loss: 0.5753, Duration: 0:00:01.143727\n",
      "Epoch 10/10, Train Loss: 0.5091    Valid Loss: 0.5786, Duration: 0:00:01.171825\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "epochs=10\n",
    "# STEP 5: INSTANTIATE LOSS CLASS\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# STEP 6: INSTANTIATE OPTIMIZER CLASS\n",
    "\n",
    "optimizer = torch.optim.Adam(model_LSTM.parameters(), lr=learning_rate)\n",
    "\n",
    "# Freeze embedding Layer\n",
    "\n",
    "#freeze embeddings\n",
    "model_LSTM.embed.weight.requires_grad  = False\n",
    "\n",
    "# STEP 7: TRAIN THE MODEL\n",
    "\n",
    "train_losses= np.zeros(epochs)\n",
    "valid_losses= np.zeros(epochs)\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "  \n",
    "  t0= datetime.now()\n",
    "  train_loss=[]\n",
    "  \n",
    "  model_LSTM.train()\n",
    "  for batch in train_iter:\n",
    "   \n",
    "    # forward pass\n",
    "    output= model_LSTM(batch.data)\n",
    "    batch.label = batch.label.unsqueeze(1)\n",
    "    batch.label = batch.label.float()\n",
    "    loss=criterion(output,batch.label)\n",
    "\n",
    "    # set gradients to zero \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss.append(loss.item())\n",
    "  \n",
    "  train_loss=np.mean(train_loss)\n",
    "      \n",
    "  valid_loss=[]\n",
    "  model_LSTM.eval()\n",
    "  with torch.no_grad():\n",
    "    for batch in valid_iter:\n",
    " \n",
    "      # forward pass\n",
    "      output= model_LSTM(batch.data)\n",
    "      batch.label = batch.label.unsqueeze(1)\n",
    "      #batch.label = batch.label.float()\n",
    "      loss=criterion(output,batch.label)\n",
    "      \n",
    "      valid_loss.append(loss.item())\n",
    "\n",
    "    valid_loss=np.mean(valid_loss)\n",
    "  \n",
    "  # save Losses\n",
    "  train_losses[epoch]= train_loss\n",
    "  valid_losses[epoch]= valid_loss\n",
    "  dt= datetime.now()-t0\n",
    "  print(f'Epoch {epoch+1}/{epochs}, Train Loss: {train_loss:.4f}    Valid Loss: {valid_loss:.4f}, Duration: {dt}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "id": "QCogfbFIlreG"
   },
   "outputs": [],
   "source": [
    "# Accuracy- write a function to get accuracy\n",
    "# use this function to get accuracy and print accuracy\n",
    "def get_accuracy(data_iter, model):\n",
    "  model.eval()\n",
    "  with torch.no_grad():\n",
    "    correct =0 \n",
    "    total =0\n",
    "    \n",
    "    for batch in data_iter:\n",
    "\n",
    "      output=model(batch.data)\n",
    "      _,indices = torch.max(output,dim=1)\n",
    "      correct+= (batch.label==indices).sum().item()\n",
    "      total += batch.label.shape[0]\n",
    "    \n",
    "    acc= correct/total\n",
    "\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rYomactfpFDV",
    "outputId": "0f42ada8-db48-4990-d030-b2a4aa38c443"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.6626,\t Valid acc: 0.6719,\t Test acc: 0.6658\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy on Train, Validation and Test Datasets\n",
    "train_acc = get_accuracy(train_iter, model_LSTM)\n",
    "valid_acc = get_accuracy(valid_iter, model_LSTM)\n",
    "test_acc = get_accuracy(test_iter ,model_LSTM)\n",
    "print(f'Train acc: {train_acc:.4f},\\t Valid acc: {valid_acc:.4f},\\t Test acc: {test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GB6XRIQGDYOp"
   },
   "source": [
    "#### Exporting output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {
    "id": "w5Z-APNn_5-T"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_1(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "id": "Mw5gEUgx_5-k"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['data']:\n",
    "  test_predictions.append(predict_sentiment_1(model_LSTM,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "id": "Q6ICE1Po_5-s"
   },
   "outputs": [],
   "source": [
    "# Rounding off predictions to 0 and 1\n",
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "id": "7w6z2dHD_5-z"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_LSTM_BD.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V19gucqu4Xcc"
   },
   "source": [
    "## Vanilla RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "T4wYdf_G4bNV"
   },
   "outputs": [],
   "source": [
    "# Creating a Vanilla RNN Function\n",
    "import torch.nn as nn\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim,batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text):\n",
    "\n",
    "        #text = [sent len, batch size]\n",
    "        embedded = self.embedding(text)\n",
    "        #print(embedded.shape)\n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        output, hidden = self.rnn(embedded)\n",
    "        #output = [sent len, batch size, hid dim]\n",
    "        #hidden = [1, batch size, hid dim]\n",
    "        \n",
    "        #assert torch.equal(output[-1,:,:], hidden.squeeze(0))\n",
    "        \n",
    "        return self.fc(hidden.squeeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhl4TAdQD3qk",
    "outputId": "11a4eb96-b1f7-47b5-8d8a-8fe9b3d2200e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11155"
      ]
     },
     "execution_count": 252,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(TEXT.vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "48W8azEOYY0u",
    "outputId": "583f31aa-1211-4aed-e378-47ceaae37718"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6075, -0.8885, -0.3198,  ...,  0.9384,  0.3964, -0.0569],\n",
      "        [ 0.4732,  0.1814,  0.2208,  ..., -2.4314, -0.5819,  1.7075],\n",
      "        [-2.2917,  1.7018,  1.2485,  ...,  0.9472,  0.7102, -0.9532],\n",
      "        ...,\n",
      "        [ 1.3456,  1.4064,  1.9149,  ...,  1.3751, -0.0479, -0.8710],\n",
      "        [ 0.3412,  0.6755,  0.1827,  ..., -0.8494,  0.8603, -0.8099],\n",
      "        [-0.7106, -0.6365, -0.1531,  ..., -1.1898,  0.5334,  0.1502]])\n"
     ]
    }
   ],
   "source": [
    "print(TEXT.vocab.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pOQhx8Y0XqJO",
    "outputId": "bfacbc24-144b-421b-8382-5e7fdd55f208"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11155, 100])\n"
     ]
    }
   ],
   "source": [
    "embeddings = TEXT.vocab.vectors\n",
    "#VanillaRNN.embedding.weight.data.copy_(embeddings)\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "id": "gIbSoy6q4i9J"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "learning_rate = 0.005\n",
    "\n",
    "model_VRNN = VanillaRNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PSCI4LHP4i_9",
    "outputId": "274982f6-debf-4fd4-ceef-3ab0575a8e1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,207,405 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model_VRNN):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {
    "id": "fLA0Rf9_4jDT"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Defining Optimizer\n",
    "optimizer = optim.Adam(model_VRNN.parameters(), lr=1e-3)\n",
    "\n",
    "# Defining Loss Function\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "# Pushing Model to GPU\n",
    "model_VRNN = model_VRNN.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "XllEDF_04t-r"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "kcZSrM194uB1"
   },
   "outputs": [],
   "source": [
    "# Defining training loop\n",
    "def train_func(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(batch.data).squeeze(1)\n",
    "        #label = label.float()\n",
    "        #print(len(predictions))\n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "Gsl6EC4l4uFj"
   },
   "outputs": [],
   "source": [
    "# Defining evaluation loop\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.data).squeeze(1)\n",
    "            #label = label.float()\n",
    "            #print(type(batch.label))\n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "WbnXfWFX5PMA"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ya0NV9q_Cupa"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPqX_Qtu5PPU",
    "outputId": "45f4653b-408b-44c7-d314-051884f61092"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.657 | Train Acc: 62.94%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 61.45%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.619 | Train Acc: 67.39%\n",
      "\t Val. Loss: 0.651 |  Val. Acc: 66.87%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.580 | Train Acc: 70.22%\n",
      "\t Val. Loss: 0.660 |  Val. Acc: 64.10%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.569 | Train Acc: 71.64%\n",
      "\t Val. Loss: 0.670 |  Val. Acc: 62.79%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.523 | Train Acc: 74.27%\n",
      "\t Val. Loss: 0.694 |  Val. Acc: 64.02%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.457 | Train Acc: 78.59%\n",
      "\t Val. Loss: 0.710 |  Val. Acc: 65.36%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.395 | Train Acc: 82.16%\n",
      "\t Val. Loss: 0.776 |  Val. Acc: 63.65%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.327 | Train Acc: 86.38%\n",
      "\t Val. Loss: 0.893 |  Val. Acc: 58.88%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.261 | Train Acc: 89.84%\n",
      "\t Val. Loss: 0.987 |  Val. Acc: 60.39%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.208 | Train Acc: 92.26%\n",
      "\t Val. Loss: 1.100 |  Val. Acc: 60.70%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_func(model_VRNN, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model_VRNN, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_VRNN.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OLVRiNNc5PUl",
    "outputId": "c57d77ea-6c32-4eaa-d984-e81bdafa48f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.657 | Test Acc: 66.67%\n"
     ]
    }
   ],
   "source": [
    "# Checking accuracy on Test Dataset\n",
    "model_VRNN.load_state_dict(torch.load('tut1-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate(model_VRNN, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "azM1NSOMDCAD"
   },
   "source": [
    "#### Exporting output to CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {
    "id": "hSaRrUQH-FUw"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_2(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    #length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    #length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "id": "hioU69Bb-FVM"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['text']:\n",
    "  test_predictions.append(predict_sentiment_2(model_VRNN,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "id": "w63zvlVW-FVT"
   },
   "outputs": [],
   "source": [
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {
    "id": "6cCK3ajK-FVa"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_VRNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akZnmAdunp_5"
   },
   "source": [
    "## Bidirectional RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T7y7emZHokTM"
   },
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {
    "id": "LRVL4-N1omxn"
   },
   "outputs": [],
   "source": [
    "TEXT = ttd.Field(\n",
    "    sequential=True,\n",
    "    #batch_first=True,\n",
    "    lower=True,\n",
    "    tokenize='spacy',\n",
    "    #pad_first=True,\n",
    "    include_lengths = True)\n",
    "\n",
    "LABEL = ttd.LabelField(dtype = torch.float)#, batch_first=True)\n",
    "\n",
    "#Train dataset\n",
    "Train_dataset = ttd.TabularDataset(\n",
    "    path='train2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT),('label', LABEL)]\n",
    ")\n",
    "\n",
    "#Test dataset\n",
    "Test_dataset = ttd.TabularDataset(\n",
    "    path='test2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "id": "mZ7lmWAWFFwF"
   },
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "training_dataset, testing_dataset = Train_dataset.split(split_ratio=0.7,random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "id": "1_i2h7cTFFwI"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "SEED=1234\n",
    "training_dataset, valid_dataset = training_dataset.split(random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9ZL0eO0lo0zl",
    "outputId": "a6875a3c-5df3-45cd-8e56-78dfbad17868"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4579\n",
      "Number of validation examples: 1963\n",
      "Number of testing examples: 2804\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(training_dataset)}')\n",
    "print(f'Number of validation examples: {len(valid_dataset)}')\n",
    "print(f'Number of testing examples: {len(testing_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "id": "um6sPLEto0zz"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(training_dataset, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "id": "jF7FXjcEo0z8"
   },
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(training_dataset)\n",
    "LABEL.build_vocab(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "id": "xoA-M0eHo-vR"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter = ttd.BucketIterator.splits((training_dataset,valid_dataset), \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "QuAKBDuN5Uu2"
   },
   "outputs": [],
   "source": [
    "test_iter = ttd.BucketIterator(testing_dataset, \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGosF9YlDsUt"
   },
   "source": [
    "#### Defining the Bidirectional function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "utc2K2dJpIez"
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class BiDRNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, \n",
    "                 bidirectional, dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.rnn = nn.LSTM(embedding_dim, \n",
    "                           hidden_dim, \n",
    "                           num_layers=n_layers, \n",
    "                           bidirectional=bidirectional, \n",
    "                           dropout=dropout)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [sent len, batch size]\n",
    "        \n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "        \n",
    "        #embedded = [sent len, batch size, emb dim]\n",
    "        \n",
    "        #pack sequence\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.rnn(packed_embedded)\n",
    "        \n",
    "        #unpack sequence\n",
    "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)\n",
    "\n",
    "        #output = [sent len, batch size, hid dim * num directions]\n",
    "        #output over padding tokens are zero tensors\n",
    "        \n",
    "        #hidden = [num layers * num directions, batch size, hid dim]\n",
    "        #cell = [num layers * num directions, batch size, hid dim]\n",
    "        \n",
    "        #concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
    "        #and apply dropout\n",
    "        \n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "                \n",
    "        #hidden = [batch size, hid dim * num directions]\n",
    "            \n",
    "        return self.fc(hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "id": "KIZBIpdgpTLf"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "HIDDEN_DIM = 256\n",
    "OUTPUT_DIM = 1\n",
    "N_LAYERS = 2\n",
    "BIDIRECTIONAL = True\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model_BDRNN = BiDRNN(INPUT_DIM, \n",
    "            EMBEDDING_DIM, \n",
    "            HIDDEN_DIM, \n",
    "            OUTPUT_DIM, \n",
    "            N_LAYERS, \n",
    "            BIDIRECTIONAL, \n",
    "            DROPOUT, \n",
    "            PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fl-FUrTKpVsS",
    "outputId": "339fd4c6-7dcd-4b1e-c605-715bd09e05ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 3,426,157 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model_BDRNN):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kdMIn-qzpfhJ",
    "outputId": "a58db3a6-dad9-4359-9bba-e4312d15ed1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([11155, 100])\n"
     ]
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "print(pretrained_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aRqCbFx6piTt",
    "outputId": "4ed51677-a469-4ffd-90a5-a49fcaa4ebc6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.2644,  0.1614,  1.9024,  ..., -0.0895,  0.0145,  0.3330],\n",
       "        [ 0.8016,  0.1560,  0.7757,  ..., -1.2540, -1.0613, -0.6615],\n",
       "        [-0.1094,  0.9084,  2.0204,  ...,  0.2334, -0.8108,  0.4120],\n",
       "        ...,\n",
       "        [ 1.0947, -0.9882,  0.3398,  ...,  2.3886, -0.1666,  0.2511],\n",
       "        [-0.2642,  0.1371, -1.1057,  ..., -0.1192,  0.1426, -1.4332],\n",
       "        [-0.3453,  1.8291,  0.3241,  ..., -0.0213,  0.8426, -1.3327]])"
      ]
     },
     "execution_count": 283,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_BDRNN.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r2fiVOdTpk8I",
    "outputId": "4d132aa6-e15c-4538-c776-c5f714ee38a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1094,  0.9084,  2.0204,  ...,  0.2334, -0.8108,  0.4120],\n",
      "        ...,\n",
      "        [ 1.0947, -0.9882,  0.3398,  ...,  2.3886, -0.1666,  0.2511],\n",
      "        [-0.2642,  0.1371, -1.1057,  ..., -0.1192,  0.1426, -1.4332],\n",
      "        [-0.3453,  1.8291,  0.3241,  ..., -0.0213,  0.8426, -1.3327]])\n"
     ]
    }
   ],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model_BDRNN.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model_BDRNN.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "\n",
    "print(model_BDRNN.embedding.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "id": "7hYCqE-OpnoX"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model_BDRNN.parameters())\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model_BDRNN = model_BDRNN.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "id": "gAN4dhAapsLu"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "id": "okbk7ahDpubN"
   },
   "outputs": [],
   "source": [
    "def train_func2(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text, text_lengths = batch.data\n",
    "        #print(\"Length of texts: \",len(text))\n",
    "        #print('Lenghts of text sequences: ',len(text_lengths))\n",
    "        #print(\"Length of labels: \",len(batch.label))\n",
    "        predictions = model(text, text_lengths).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "id": "BCR3hBG-pzlh"
   },
   "outputs": [],
   "source": [
    "def evaluate2(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            text, text_lengths = batch.data\n",
    "            #print(\"Length of texts: \",len(text))\n",
    "            #print('Lenghts of text sequences: ',len(text_lengths))\n",
    "            #print(\"Length of labels: \",len(batch.label))\n",
    "            predictions = model(text, text_lengths).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "id": "-s7Z3tRdFfAF"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vLsl4OtAD1ot"
   },
   "source": [
    "#### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4bbUBI22p47T",
    "outputId": "617aedad-dce9-4af3-c5c3-fb0fa8980088"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.634 | Train Acc: 67.07%\n",
      "\t Val. Loss: 0.626 |  Val. Acc: 67.35%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.587 | Train Acc: 70.42%\n",
      "\t Val. Loss: 0.561 |  Val. Acc: 69.29%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.563 | Train Acc: 71.76%\n",
      "\t Val. Loss: 0.564 |  Val. Acc: 71.40%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.517 | Train Acc: 76.04%\n",
      "\t Val. Loss: 0.524 |  Val. Acc: 74.73%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.478 | Train Acc: 78.66%\n",
      "\t Val. Loss: 0.516 |  Val. Acc: 75.39%\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.456 | Train Acc: 79.15%\n",
      "\t Val. Loss: 0.530 |  Val. Acc: 75.81%\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.427 | Train Acc: 80.82%\n",
      "\t Val. Loss: 0.516 |  Val. Acc: 75.36%\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.385 | Train Acc: 83.47%\n",
      "\t Val. Loss: 0.515 |  Val. Acc: 76.69%\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.356 | Train Acc: 85.07%\n",
      "\t Val. Loss: 0.518 |  Val. Acc: 76.37%\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 0.328 | Train Acc: 85.84%\n",
      "\t Val. Loss: 0.586 |  Val. Acc: 77.55%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_func2(model_BDRNN, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate2(model_BDRNN, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_BDRNN.state_dict(), 'tut2-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vOGCXtR2p7sX",
    "outputId": "02cbeeb3-8778-47f0-ac1e-cfa90efbc809"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.506 | Test Acc: 77.46%\n"
     ]
    }
   ],
   "source": [
    "model_BDRNN.load_state_dict(torch.load('tut2-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate2(model_BDRNN, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {
    "id": "P6Ar3DQYYc_A"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "\n",
    "def predict_sentiment_1(model, sentence):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    length = [len(indexed)]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(1)\n",
    "    length_tensor = torch.LongTensor(length)\n",
    "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {
    "id": "tk51nVyB9D0c"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['text']:\n",
    "  test_predictions.append(predict_sentiment_1(model_BDRNN,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "id": "huQ4id9K9Zby"
   },
   "outputs": [],
   "source": [
    "# Rounding off to 0 and 1\n",
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "id": "1LMiX_IF9dwh"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_BiDRNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mY0PIMb-CQap"
   },
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GaN-Qh7DQjZd"
   },
   "source": [
    "#### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {
    "id": "AMOhkUVhHR1s"
   },
   "outputs": [],
   "source": [
    "TEXT = ttd.Field(\n",
    "    sequential=True,\n",
    "    batch_first=True,\n",
    "    lower=True,\n",
    "    tokenize='spacy',\n",
    "    pad_first=True)\n",
    "    #include_lengths = True)\n",
    "\n",
    "LABEL = ttd.LabelField(dtype = torch.float)#, batch_first=True)\n",
    "\n",
    "#Train dataset\n",
    "Train_dataset = ttd.TabularDataset(\n",
    "    path='train2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT),('label', LABEL)]\n",
    ")\n",
    "\n",
    "#Test dataset\n",
    "Test_dataset = ttd.TabularDataset(\n",
    "    path='test2.csv',\n",
    "    format='csv',\n",
    "    skip_header=True,\n",
    "    fields=[('data', TEXT)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {
    "id": "7ZzhFW0RHR1y"
   },
   "outputs": [],
   "source": [
    "SEED=1234\n",
    "training_dataset, testing_dataset = Train_dataset.split(split_ratio=0.7,random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {
    "id": "vf2sxbAVHR1z"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "SEED=1234\n",
    "training_dataset, valid_dataset = training_dataset.split(random_state = random.seed(SEED)) # default is 0.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ycbz7dk_HR11",
    "outputId": "d9de5aa8-5d94-4168-db76-ff4182f4f3b5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 4579\n",
      "Number of validation examples: 1963\n",
      "Number of testing examples: 2804\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of training examples: {len(training_dataset)}')\n",
    "print(f'Number of validation examples: {len(valid_dataset)}')\n",
    "print(f'Number of testing examples: {len(testing_dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "id": "bdXWmuIjHR13"
   },
   "outputs": [],
   "source": [
    "MAX_VOCAB_SIZE = 25000\n",
    "\n",
    "TEXT.build_vocab(training_dataset, \n",
    "                 max_size = MAX_VOCAB_SIZE, \n",
    "                 vectors = \"glove.6B.100d\", \n",
    "                 unk_init = torch.Tensor.normal_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "id": "93zNCmvxHR14"
   },
   "outputs": [],
   "source": [
    "#TEXT.build_vocab(training_dataset)\n",
    "LABEL.build_vocab(training_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "id": "VRv1ZFwjHR16"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iter, valid_iter = ttd.BucketIterator.splits((training_dataset,valid_dataset), \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "id": "FovY50xiHR17"
   },
   "outputs": [],
   "source": [
    "test_iter = ttd.BucketIterator(testing_dataset, \n",
    "                              sort_key=lambda x: len(x.data),\n",
    "                              #sort_key=None,\n",
    "                              sort_within_batch = True,\n",
    "                              batch_size=64, \n",
    "                              device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ctM-nAIHkKN"
   },
   "source": [
    "#### Creating a CNN function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "id": "uyovJzOUHm1h"
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CNN_model(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, \n",
    "                 dropout, pad_idx):\n",
    "        \n",
    "        super().__init__()\n",
    "                \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
    "        \n",
    "        self.convs = nn.ModuleList([\n",
    "                                    nn.Conv2d(in_channels = 1, \n",
    "                                              out_channels = n_filters, \n",
    "                                              kernel_size = (fs, embedding_dim)) \n",
    "                                    for fs in filter_sizes\n",
    "                                    ])\n",
    "        \n",
    "        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, text):\n",
    "                \n",
    "        #text = [batch size, sent len]\n",
    "        \n",
    "        embedded = self.embedding(text)\n",
    "                \n",
    "        #embedded = [batch size, sent len, emb dim]\n",
    "        \n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        \n",
    "        #embedded = [batch size, 1, sent len, emb dim]\n",
    "        \n",
    "        conved = [F.relu(conv(embedded)).squeeze(3) for conv in self.convs]\n",
    "            \n",
    "        #conved_n = [batch size, n_filters, sent len - filter_sizes[n] + 1]\n",
    "                \n",
    "        pooled = [F.max_pool1d(conv, conv.shape[2]).squeeze(2) for conv in conved]\n",
    "        \n",
    "        #pooled_n = [batch size, n_filters]\n",
    "        \n",
    "        cat = self.dropout(torch.cat(pooled, dim = 1))\n",
    "\n",
    "        #cat = [batch size, n_filters * len(filter_sizes)]\n",
    "            \n",
    "        return self.fc(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "id": "bnwwDzNuHsrt"
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,2,2]\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "\n",
    "model_CNN = CNN_model(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "k6IBEK4PH8FI",
    "outputId": "af6fee94-29a5-4b6e-edb8-f4ae3e06a3b3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1,176,101 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f'The model has {count_parameters(model_CNN):,} trainable parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "retwIdX6IBuC",
    "outputId": "00ecaf22-16bc-475d-a301-f5cf43f1841f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.1318, -0.3035,  1.0498,  ..., -0.1555, -0.5055,  0.1247],\n",
       "        [-0.4578,  0.8600,  0.8540,  ...,  3.0938,  0.9062,  0.7151],\n",
       "        [-2.7232,  0.1180, -0.5694,  ..., -0.5942,  1.4725,  0.3334],\n",
       "        ...,\n",
       "        [ 0.6199, -0.3344,  0.4038,  ..., -0.7188, -1.0255,  2.5693],\n",
       "        [-0.0091,  1.5333,  0.7273,  ...,  2.3677,  0.1274, -0.9346],\n",
       "        [ 1.0803,  0.3010, -0.9273,  ..., -0.2327,  0.4826,  0.1647]])"
      ]
     },
     "execution_count": 331,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model_CNN.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {
    "id": "H8q04NXnIHWc"
   },
   "outputs": [],
   "source": [
    "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "\n",
    "model_CNN.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
    "model_CNN.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "id": "61Q2FWQ3IM6W"
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model_CNN.parameters(),lr=0.001)\n",
    "#optimizer = optim.SGD(model_CNN.parameters(), lr=1e-3)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model_CNN = model_CNN.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "id": "_veyXj5yIOUB"
   },
   "outputs": [],
   "source": [
    "def binary_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "\n",
    "    #round predictions to the closest integer\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "id": "ucbq5oGvIQP2"
   },
   "outputs": [],
   "source": [
    "def train_func3(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for batch in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        predictions = model(batch.data).squeeze(1)\n",
    "        \n",
    "        loss = criterion(predictions, batch.label)\n",
    "        \n",
    "        acc = binary_accuracy(predictions, batch.label)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "id": "405rWbaoISdS"
   },
   "outputs": [],
   "source": [
    "def evaluate3(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.data).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "            \n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "id": "iGWhZkAdIWZb"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UhXx0GL_ILJf"
   },
   "source": [
    "#### Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_EPdpdjaIavu",
    "outputId": "9377007e-dab4-4b55-857c-afdb7df0b086"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.623 | Train Acc: 66.86%\n",
      "\t Val. Loss: 0.569 |  Val. Acc: 70.29%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.519 | Train Acc: 75.64%\n",
      "\t Val. Loss: 0.535 |  Val. Acc: 74.80%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.459 | Train Acc: 79.41%\n",
      "\t Val. Loss: 0.495 |  Val. Acc: 77.05%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.405 | Train Acc: 82.29%\n",
      "\t Val. Loss: 0.490 |  Val. Acc: 77.12%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.356 | Train Acc: 84.46%\n",
      "\t Val. Loss: 0.515 |  Val. Acc: 77.30%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.313 | Train Acc: 87.02%\n",
      "\t Val. Loss: 0.509 |  Val. Acc: 76.64%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.258 | Train Acc: 89.29%\n",
      "\t Val. Loss: 0.527 |  Val. Acc: 76.59%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.221 | Train Acc: 91.69%\n",
      "\t Val. Loss: 0.550 |  Val. Acc: 76.64%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.180 | Train Acc: 93.61%\n",
      "\t Val. Loss: 0.567 |  Val. Acc: 76.19%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.142 | Train Acc: 95.08%\n",
      "\t Val. Loss: 0.607 |  Val. Acc: 76.06%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_func3(model_CNN, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate3(model_CNN, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_CNN.state_dict(), 'tut4-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "24C4A30xEiOi",
    "outputId": "3f14bd80-0ff5-4d4a-eea3-633eb5e42918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.475 | Test Acc: 78.42%\n"
     ]
    }
   ],
   "source": [
    "model_CNN.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate3(model_CNN, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM92geJrRAjk"
   },
   "source": [
    "#### Making final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "id": "TatGSVr8RAjk"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "model_CNN.load_state_dict(torch.load('tut4-model.pt'))\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence.lower())]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "id": "hKLGBMiPRAjn"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['text']:\n",
    "  test_predictions.append(predict_sentiment(model_CNN,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CTf4kOyrRAjp",
    "outputId": "a227c895-2d44-4616-89d1-bcdaebcf9796"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.465151846408844, 0.9686873555183411, 0.09507963806390762, 0.48898977041244507, 0.6569994688034058, 0.09669846296310425, 0.4130823016166687, 0.24770590662956238, 0.09016075730323792, 0.11028182506561279, 0.41365671157836914, 0.15479980409145355, 0.9080560207366943, 0.09659378975629807, 0.21090684831142426, 0.6286602020263672, 0.27938905358314514, 0.9763802289962769, 0.8842927813529968, 0.10443146526813507, 0.14120352268218994, 0.19857291877269745, 0.1382574588060379, 0.26858946681022644, 0.07057629525661469, 0.8856698870658875, 0.1269083172082901, 0.06816764920949936, 0.0811086893081665, 0.9583178162574768, 0.37876102328300476, 0.9146126508712769, 0.11199995130300522, 0.08443864434957504, 0.37352806329727173, 0.326129674911499, 0.5559449195861816, 0.21375715732574463, 0.0673597902059555, 0.5215111374855042, 0.2901114821434021, 0.1271006017923355, 0.11918247491121292, 0.6307768225669861, 0.24005091190338135, 0.44708436727523804, 0.3641147315502167, 0.6722557544708252, 0.054519180208444595, 0.14141078293323517, 0.07464737445116043, 0.37157687544822693, 0.5445374846458435, 0.7303841710090637, 0.8099648952484131, 0.3341834843158722, 0.41159793734550476, 0.09484700113534927, 0.9137669205665588, 0.15780940651893616, 0.38246744871139526, 0.6177181005477905, 0.409433513879776, 0.21695296466350555, 0.0718374028801918, 0.53975510597229, 0.5200502276420593, 0.20086358487606049, 0.1592010110616684, 0.10658901929855347, 0.9407317042350769, 0.05129183456301689, 0.2094797044992447, 0.29287078976631165, 0.0839414894580841, 0.6019871830940247, 0.3113957345485687, 0.22789382934570312, 0.066404327750206, 0.14490509033203125, 0.14890527725219727, 0.12004988640546799, 0.2074163854122162, 0.6704224944114685, 0.08293628692626953, 0.04588215798139572, 0.07274485379457474, 0.9741098284721375, 0.13769739866256714, 0.13137133419513702, 0.5023252367973328, 0.9644719958305359, 0.8726297616958618, 0.0867149606347084, 0.6075330376625061, 0.09206727147102356, 0.3499583303928375, 0.12908709049224854, 0.5197409391403198, 0.1322825402021408, 0.1859034299850464, 0.0466504842042923, 0.058184802532196045, 0.11940424889326096, 0.11777684837579727, 0.4392750859260559, 0.9540907144546509, 0.1643601953983307, 0.17742392420768738, 0.19592736661434174, 0.21833647787570953, 0.1510518491268158, 0.225466787815094, 0.753591001033783, 0.5318249464035034, 0.17331919074058533, 0.15718336403369904, 0.42631590366363525, 0.13366031646728516, 0.18756650388240814, 0.11020296066999435, 0.6707392930984497, 0.16361111402511597, 0.07469045370817184, 0.1335822194814682, 0.2860594391822815, 0.2654315233230591, 0.3161865770816803, 0.5788776278495789, 0.06636644154787064, 0.4095025062561035, 0.06293278932571411, 0.0623629130423069, 0.2545796036720276, 0.0833955630660057, 0.10994662344455719, 0.04353999346494675, 0.515167772769928, 0.07435665279626846, 0.2599906027317047, 0.11619938164949417, 0.8903486728668213, 0.18960444629192352, 0.5112340450286865, 0.44796937704086304, 0.049871720373630524, 0.09386898577213287, 0.12362772226333618, 0.059543855488300323, 0.04390420392155647, 0.18320876359939575, 0.13427013158798218, 0.07123228907585144, 0.9373250007629395, 0.16136287152767181, 0.09279833734035492, 0.08058057725429535, 0.099385567009449, 0.06062871217727661, 0.13113364577293396, 0.0964483693242073, 0.2221231758594513, 0.1473848819732666, 0.05756741762161255, 0.21765603125095367, 0.059398721903562546, 0.1679755598306656, 0.4224257469177246, 0.07560296356678009, 0.08031778037548065, 0.25101929903030396, 0.16719521582126617, 0.10925406962633133, 0.09842155873775482, 0.19386418163776398, 0.6875222325325012, 0.3114197850227356, 0.3099115788936615, 0.11087227612733841, 0.8760096430778503, 0.1006995141506195, 0.6995725035667419, 0.22031299769878387, 0.4763822853565216, 0.6381242275238037, 0.09918099641799927, 0.9598841667175293, 0.23340553045272827, 0.23658287525177002, 0.9535717368125916, 0.21496041119098663, 0.6811378598213196, 0.7999229431152344, 0.8461607098579407, 0.06705565005540848, 0.9763802289962769, 0.6517058610916138, 0.11635872721672058, 0.1436343491077423, 0.03914466127753258, 0.2763479948043823, 0.06464807689189911, 0.06570658832788467, 0.2549237310886383, 0.1444265991449356, 0.9185970425605774, 0.17828132212162018, 0.10255488753318787, 0.10040383040904999, 0.1963346153497696, 0.1932228058576584, 0.8719838261604309, 0.06975936889648438, 0.9069415330886841, 0.08621621876955032, 0.05027735233306885, 0.11727792024612427, 0.18775244057178497, 0.33084434270858765, 0.4398852288722992, 0.2807828187942505, 0.06929619610309601, 0.07032351940870285, 0.9777700304985046, 0.17100560665130615, 0.4530802071094513, 0.2425234317779541, 0.5373451113700867, 0.564391553401947, 0.09676256030797958, 0.5102658271789551, 0.11573094874620438, 0.07066456973552704, 0.13368703424930573, 0.12046734988689423, 0.08555534482002258, 0.5978843569755554, 0.296138197183609, 0.06648758053779602, 0.1094210222363472, 0.22149088978767395, 0.2816767394542694, 0.08653071522712708, 0.10224764794111252, 0.16023720800876617, 0.9873560667037964, 0.22656911611557007, 0.13455180823802948, 0.07376548647880554, 0.17041052877902985, 0.9587951302528381, 0.10784102231264114, 0.1168380156159401, 0.40669161081314087, 0.46684619784355164, 0.10249930620193481, 0.9801375269889832, 0.1672172099351883, 0.07304099202156067, 0.11380359530448914, 0.38973239064216614, 0.06393169611692429, 0.29850441217422485, 0.11580947041511536, 0.8778430819511414, 0.17187994718551636, 0.6140510439872742, 0.5355704426765442, 0.05333981290459633, 0.16312971711158752, 0.3000127971172333, 0.10167307406663895, 0.11635458469390869, 0.2222883701324463, 0.9758658409118652, 0.16819126904010773, 0.17955751717090607, 0.10960528254508972, 0.2849773168563843, 0.3289546072483063, 0.5529125332832336, 0.11790893971920013, 0.18832936882972717, 0.09221632033586502, 0.7897939085960388, 0.8501917123794556, 0.20259124040603638, 0.19406071305274963, 0.5902467966079712, 0.13821294903755188, 0.12852869927883148, 0.05839236080646515, 0.535849392414093, 0.1348760724067688, 0.0683981254696846, 0.1291576474905014, 0.16548942029476166, 0.12671154737472534, 0.07763911038637161, 0.17418313026428223, 0.23416027426719666, 0.14501544833183289, 0.44196265935897827, 0.0677526667714119, 0.21315202116966248, 0.05781051889061928, 0.9524897933006287, 0.6794980764389038, 0.07003055512905121, 0.5998051762580872, 0.07941137254238129, 0.2862209677696228, 0.0649731457233429, 0.08132007718086243, 0.342899888753891, 0.227373406291008, 0.4409349858760834, 0.30599120259284973, 0.8410109877586365, 0.09413595497608185, 0.22269582748413086, 0.28522762656211853, 0.13198722898960114, 0.20431171357631683, 0.2172328233718872, 0.9489052295684814, 0.6761930584907532, 0.5706685185432434, 0.5633150339126587, 0.395282506942749, 0.14514678716659546, 0.20302297174930573, 0.06899670511484146, 0.07097116857767105, 0.9480717182159424, 0.0981377512216568, 0.49371856451034546, 0.6081230640411377, 0.20492129027843475, 0.32132774591445923, 0.09651980549097061, 0.9802029132843018, 0.12753665447235107, 0.8369303941726685, 0.36505627632141113, 0.6487978100776672, 0.8509780168533325, 0.09957931935787201, 0.07146096974611282, 0.05443448945879936, 0.07193207740783691, 0.11383689194917679, 0.49728935956954956, 0.31240782141685486, 0.07833479344844818, 0.1380578875541687, 0.08347570151090622, 0.12675119936466217, 0.7343016266822815, 0.12555450201034546, 0.7326492071151733, 0.10407589375972748, 0.8628215193748474, 0.9756740927696228, 0.08723269402980804, 0.10282977670431137, 0.178366556763649, 0.8926330804824829, 0.1412072628736496, 0.4580422639846802, 0.49757176637649536, 0.0929468497633934, 0.15440239012241364, 0.14474549889564514, 0.05418771877884865, 0.6404361128807068, 0.16408896446228027, 0.20814673602581024, 0.9272632002830505, 0.5959439873695374, 0.09281966835260391, 0.9595033526420593, 0.31741833686828613, 0.43193215131759644, 0.34247663617134094, 0.5368000864982605, 0.08095896989107132, 0.544021487236023, 0.12983468174934387, 0.943472146987915, 0.19336757063865662, 0.07474032789468765, 0.28683486580848694, 0.3202037513256073, 0.13443541526794434, 0.09129555523395538, 0.4760676324367523, 0.20902696251869202, 0.9545629024505615, 0.8632171750068665, 0.0756005197763443, 0.14075236022472382, 0.08927643299102783, 0.4077872633934021, 0.06465417891740799, 0.1104637011885643, 0.29110434651374817, 0.17575162649154663, 0.15879762172698975, 0.1066216304898262, 0.12794120609760284, 0.05077016353607178, 0.13639207184314728, 0.10016167163848877, 0.21005266904830933, 0.1396397203207016, 0.06510300189256668, 0.9088517427444458, 0.3414345979690552, 0.5342299342155457, 0.6912713050842285, 0.8051062226295471, 0.5574999451637268, 0.11071067303419113, 0.8681196570396423, 0.2816449999809265, 0.07624497264623642, 0.35915204882621765, 0.24060167372226715, 0.05375456437468529, 0.07485797256231308, 0.43089354038238525, 0.5003161430358887, 0.7860184907913208, 0.9753501415252686, 0.36539730429649353, 0.5929532647132874, 0.46560728549957275, 0.18477344512939453, 0.09163609147071838, 0.2287614494562149, 0.205278679728508, 0.13865141570568085, 0.10105639696121216, 0.12346364557743073, 0.18334460258483887, 0.07002419233322144, 0.191367506980896, 0.9672191143035889, 0.05317985266447067, 0.06390319764614105, 0.11546235531568527, 0.3425224721431732, 0.9637311697006226, 0.05336720868945122, 0.07023178040981293, 0.17164351046085358, 0.4997234642505646, 0.0771159678697586, 0.9212048053741455, 0.09731444716453552, 0.08597385883331299, 0.13074234127998352, 0.0531831756234169, 0.3485831618309021, 0.6275581121444702, 0.7450423836708069, 0.5980393886566162, 0.15435603260993958, 0.15596221387386322, 0.07681427150964737, 0.0758819580078125, 0.21075883507728577, 0.31387341022491455, 0.14518040418624878, 0.0503704771399498, 0.725053608417511, 0.08309853821992874, 0.2049221694469452, 0.11785722523927689, 0.07744631916284561, 0.048177603632211685, 0.08015906810760498, 0.9103869199752808, 0.05888320133090019, 0.41104334592819214, 0.12893934547901154, 0.9816931486129761, 0.05132752284407616, 0.08067154139280319, 0.41025495529174805, 0.05958090350031853, 0.20152166485786438, 0.09513360261917114, 0.17027759552001953, 0.8688467144966125, 0.21321621537208557, 0.06622801721096039, 0.11334949731826782, 0.09121126681566238, 0.35542285442352295, 0.04594460874795914, 0.09711027890443802, 0.1814069002866745, 0.30834320187568665, 0.1254321187734604, 0.16438411176204681, 0.17746670544147491, 0.5704617500305176, 0.03779691457748413, 0.12801240384578705, 0.12395129352807999, 0.3772568702697754, 0.8251262307167053, 0.13117246329784393, 0.08784473687410355, 0.8204275965690613, 0.08584971725940704, 0.11271869391202927, 0.08380350470542908, 0.46004122495651245, 0.9673842787742615, 0.681182861328125, 0.0686408281326294, 0.2219124436378479, 0.6977450251579285, 0.17150981724262238, 0.15391679108142853, 0.12114398181438446, 0.20161208510398865, 0.13609011471271515, 0.10419587045907974, 0.17482450604438782, 0.09800390154123306, 0.20293648540973663, 0.12139208614826202, 0.46105077862739563, 0.0849919468164444, 0.1362643986940384, 0.09516442567110062, 0.14907097816467285, 0.45379069447517395, 0.9595395922660828, 0.04964043200016022, 0.5480931401252747, 0.35614198446273804, 0.19426380097866058, 0.08456343412399292, 0.3240826725959778, 0.06831870228052139, 0.12270032614469528, 0.39657139778137207, 0.14732955396175385, 0.09385931491851807, 0.1957232505083084, 0.22527210414409637, 0.22253641486167908, 0.11643390357494354, 0.1058332547545433, 0.16962270438671112, 0.05998283252120018, 0.12945173680782318, 0.9528478980064392, 0.09412820637226105, 0.23696273565292358, 0.61288982629776, 0.1912507861852646, 0.2714797854423523, 0.1625695377588272, 0.08702722191810608, 0.8329755067825317, 0.16018261015415192, 0.18417881429195404, 0.08271848410367966, 0.9370606541633606, 0.6625012755393982, 0.8514245748519897, 0.9580215215682983, 0.20380763709545135, 0.32805129885673523, 0.09060672670602798, 0.3126375079154968, 0.09922303259372711, 0.2667801082134247, 0.18367068469524384, 0.914154589176178, 0.4722239077091217, 0.08340658992528915, 0.7576204538345337, 0.1282934546470642, 0.06864254921674728, 0.06786101311445236, 0.15573251247406006, 0.9666300415992737, 0.8916287422180176, 0.3777976632118225, 0.0819806307554245, 0.12394832074642181, 0.13071297109127045, 0.13783125579357147, 0.13252195715904236, 0.27901583909988403, 0.5796941518783569, 0.9097367525100708, 0.12384499609470367, 0.1434604823589325, 0.3540772795677185, 0.31036609411239624, 0.23283162713050842, 0.30871012806892395, 0.2642732560634613, 0.7009195685386658, 0.38109374046325684, 0.8976664543151855, 0.9509957432746887, 0.26973000168800354, 0.15865245461463928, 0.12223917990922928, 0.09248892962932587, 0.10980808734893799, 0.08923513442277908, 0.15803799033164978, 0.06961476802825928, 0.10883384197950363, 0.35462358593940735, 0.3820185363292694, 0.2303464263677597, 0.21429544687271118, 0.06423571705818176, 0.794289767742157, 0.07758460193872452, 0.2428158074617386, 0.051807016134262085, 0.08780638873577118, 0.06350680440664291, 0.16179686784744263, 0.07929764688014984, 0.5417395234107971, 0.2891504466533661, 0.13987426459789276, 0.09850134700536728, 0.9673833847045898, 0.0455012284219265, 0.12769849598407745, 0.08545787632465363, 0.09396064281463623, 0.08925032615661621, 0.05044066533446312, 0.7227901816368103, 0.06801152229309082, 0.12441873550415039, 0.8056319952011108, 0.05859535187482834, 0.43964290618896484, 0.8185451626777649, 0.06909382343292236, 0.958853542804718, 0.06106390804052353, 0.06626726686954498, 0.5923722386360168, 0.3137452006340027, 0.35536912083625793, 0.1049756109714508, 0.09672997891902924, 0.8242788314819336, 0.06160647049546242, 0.41458261013031006, 0.3363957107067108, 0.4368276596069336, 0.08274918049573898, 0.5853289365768433, 0.06096995994448662, 0.20629280805587769, 0.5903133153915405, 0.06932293623685837, 0.05209073796868324, 0.23223356902599335, 0.9631319642066956, 0.38889938592910767, 0.07867524772882462, 0.07511502504348755, 0.19322262704372406, 0.15789295732975006, 0.6484941840171814, 0.19440507888793945, 0.7454355955123901, 0.16142934560775757, 0.4243759512901306, 0.9097315073013306, 0.07323431968688965, 0.28476688265800476, 0.8377624154090881, 0.800750195980072, 0.731558620929718, 0.27330899238586426, 0.7709370851516724, 0.09157204627990723, 0.21754594147205353, 0.2763862609863281, 0.9436857104301453, 0.7130953669548035, 0.17651580274105072, 0.1711081564426422, 0.09370768070220947, 0.10982098430395126, 0.2371012419462204, 0.24053090810775757, 0.312387615442276, 0.1299615055322647, 0.08506728708744049, 0.42022183537483215, 0.11424223333597183, 0.2912728786468506, 0.6928935050964355, 0.23825722932815552, 0.21324557065963745, 0.9576160311698914, 0.370647668838501, 0.07468181848526001, 0.38545671105384827, 0.28272730112075806, 0.061629217118024826, 0.2536749541759491, 0.08553206920623779, 0.6514407992362976, 0.20011714100837708, 0.1021963432431221, 0.8071586489677429, 0.10356120020151138, 0.4616584777832031, 0.6482559442520142, 0.40872082114219666, 0.39398983120918274, 0.6120127439498901, 0.06065281853079796, 0.1238865926861763, 0.17100578546524048, 0.7455307245254517, 0.08811648935079575, 0.030389180406928062, 0.46254250407218933, 0.7396177053451538, 0.14468955993652344, 0.09675677120685577, 0.09415129572153091, 0.10059031844139099, 0.295119971036911, 0.2455368936061859, 0.08129826188087463, 0.22822552919387817, 0.15409165620803833, 0.10920509696006775, 0.4240841269493103, 0.07180003821849823, 0.7460848093032837, 0.04639343544840813, 0.8048701882362366, 0.05331651493906975, 0.6606163382530212, 0.1607777327299118, 0.09250122308731079, 0.1970970630645752, 0.31851840019226074, 0.3824988603591919, 0.28032708168029785, 0.25865280628204346, 0.07662845402956009, 0.3462198078632355, 0.11837516725063324, 0.09911660104990005, 0.44777336716651917, 0.15957102179527283, 0.3865891098976135, 0.159241184592247, 0.6297303438186646, 0.18352997303009033, 0.07001184672117233, 0.13876156508922577, 0.16552242636680603, 0.5355095863342285, 0.1414792388677597, 0.7354580760002136, 0.23997525870800018, 0.08284202218055725, 0.5751397609710693, 0.9870747327804565, 0.9280332922935486, 0.0756492167711258, 0.12840381264686584, 0.9747914671897888, 0.10124046355485916, 0.28462931513786316, 0.7031601667404175, 0.19896076619625092, 0.1751997023820877, 0.40946507453918457, 0.3131471574306488, 0.10998708754777908, 0.14599865674972534, 0.4239926040172577, 0.2906217575073242, 0.40150001645088196, 0.1407610923051834, 0.981245756149292, 0.6630303859710693, 0.285351037979126, 0.1520969122648239, 0.9694281220436096, 0.6165887117385864, 0.18861359357833862, 0.2126615196466446, 0.20819970965385437, 0.08715268969535828, 0.13557732105255127, 0.15466663241386414, 0.9097490906715393, 0.5546213984489441, 0.3419247269630432, 0.14485514163970947, 0.09934494644403458, 0.8484034538269043, 0.2273390144109726, 0.7439461946487427, 0.9275690317153931, 0.1918397694826126, 0.20513752102851868, 0.2066660076379776, 0.1775265336036682, 0.1387411653995514, 0.0826292634010315, 0.31690096855163574, 0.10311360657215118, 0.13196925818920135, 0.5082191228866577, 0.2431892454624176, 0.168566033244133, 0.1384934037923813, 0.9087429642677307, 0.04181324690580368, 0.19319207966327667, 0.2521335780620575, 0.12047651410102844, 0.1793975681066513, 0.1468898206949234, 0.043572086840867996, 0.2111067771911621, 0.38633742928504944, 0.08324289321899414, 0.15134261548519135, 0.9715092182159424, 0.0551164485514164, 0.12142865359783173, 0.06070217862725258, 0.126999631524086, 0.5185137391090393, 0.8950574994087219, 0.15376898646354675, 0.4328443706035614, 0.21932023763656616, 0.13827115297317505, 0.11047457158565521, 0.2719603180885315, 0.1180068701505661, 0.2650796175003052, 0.0866871327161789, 0.18413497507572174, 0.09709832072257996, 0.9432335495948792, 0.1860736608505249, 0.10720467567443848, 0.2039976567029953, 0.7565516829490662, 0.3350444734096527, 0.16108167171478271, 0.04308881610631943, 0.2868686020374298, 0.22855503857135773, 0.7499282956123352, 0.07757579535245895, 0.9548590779304504, 0.13229869306087494, 0.2993740439414978, 0.17799408733844757, 0.4417707026004791, 0.9100422263145447, 0.1812378615140915, 0.06725335866212845, 0.5317929983139038, 0.05501013249158859, 0.20558786392211914, 0.4618496894836426, 0.06862746924161911, 0.1663200855255127, 0.3835570812225342, 0.1614471673965454, 0.3512898087501526, 0.11171112209558487, 0.9517146348953247, 0.06706662476062775, 0.9752470254898071, 0.5448139905929565, 0.1945980042219162, 0.4871024191379547, 0.1384437531232834, 0.3690973222255707, 0.045018360018730164, 0.07943980395793915, 0.3849739134311676, 0.14919757843017578, 0.10447829961776733, 0.11998756229877472, 0.4587388038635254, 0.9792693257331848, 0.5433439016342163, 0.48338404297828674, 0.9497563242912292, 0.7618580460548401, 0.9313931465148926, 0.14946191012859344, 0.1719646453857422, 0.19269780814647675, 0.6792717576026917, 0.09789569675922394, 0.7147807478904724, 0.16169175505638123, 0.10945341736078262, 0.07640910893678665, 0.059426844120025635, 0.7474366426467896, 0.09727420657873154, 0.27305418252944946, 0.6276003122329712, 0.9636871814727783, 0.9018255472183228, 0.08890416473150253, 0.06818094104528427, 0.06348726898431778, 0.42498576641082764, 0.5771331191062927, 0.29918354749679565, 0.07386413216590881, 0.2353552281856537, 0.28035449981689453, 0.06007790192961693, 0.1706482172012329, 0.04709259793162346, 0.4362652599811554, 0.1635950356721878, 0.8161033391952515, 0.10059234499931335, 0.21300320327281952, 0.13103005290031433, 0.10073471069335938, 0.30119165778160095, 0.2873423099517822, 0.3647860884666443, 0.08518937975168228, 0.8475527763366699, 0.2995750308036804, 0.8819090723991394, 0.5759308338165283, 0.3120937645435333, 0.31458282470703125, 0.09065795689821243, 0.14140675961971283, 0.4311433434486389, 0.6535637378692627, 0.6472998857498169, 0.9360484480857849, 0.9278942942619324, 0.046669479459524155, 0.2076120525598526, 0.11437427252531052, 0.1275589019060135, 0.5856128334999084, 0.7048118114471436, 0.32581695914268494, 0.2923569679260254, 0.1062876358628273, 0.9757389426231384, 0.4297409951686859, 0.6652834415435791, 0.41960856318473816, 0.8918489217758179, 0.15798035264015198, 0.4904461205005646, 0.09478713572025299, 0.9014449715614319, 0.09617175906896591, 0.12123535573482513, 0.4410189986228943, 0.05298628285527229, 0.1344526708126068, 0.20398485660552979, 0.15166263282299042, 0.4918571412563324, 0.14941556751728058, 0.2574218809604645, 0.059863507747650146, 0.07844039052724838, 0.3517245054244995, 0.10453809797763824, 0.1272813230752945, 0.21366038918495178, 0.26859912276268005, 0.9447858333587646, 0.20464080572128296, 0.8397603631019592, 0.5092067122459412, 0.230495423078537, 0.8786669373512268, 0.07499811053276062, 0.7900671362876892, 0.37396010756492615, 0.9384955167770386, 0.8671814799308777, 0.06817398965358734, 0.0498572513461113, 0.544109582901001, 0.03413612022995949, 0.06839048862457275, 0.3676644265651703, 0.102284736931324, 0.11409077048301697, 0.08046715706586838, 0.08567763864994049, 0.222929447889328, 0.3060193359851837, 0.1547096222639084, 0.08753684908151627, 0.6077786087989807, 0.07960084080696106, 0.08365839719772339, 0.38888347148895264, 0.8901016116142273, 0.11708906292915344, 0.35413941740989685, 0.59102463722229, 0.45869407057762146, 0.0501529835164547, 0.07118680328130722, 0.32727286219596863, 0.13430407643318176, 0.15959301590919495, 0.6745355725288391, 0.6019505262374878, 0.4133398234844208, 0.07388914376497269, 0.12832023203372955, 0.27469801902770996, 0.4134422540664673, 0.9631925821304321, 0.9912710189819336, 0.04632999002933502, 0.9590696096420288, 0.1634981632232666, 0.22047798335552216, 0.3399559259414673, 0.2835569381713867, 0.08968587219715118, 0.22944247722625732, 0.6692423820495605, 0.07299360632896423, 0.06981595605611801, 0.10301797837018967, 0.19136998057365417, 0.48068737983703613, 0.9320964217185974, 0.2499181032180786, 0.5724696516990662, 0.28272730112075806, 0.570214569568634, 0.7565326690673828, 0.9590528607368469, 0.33488014340400696, 0.9349551200866699, 0.04246925562620163, 0.10658411681652069, 0.28871017694473267, 0.1197618693113327, 0.3298637866973877, 0.38456782698631287, 0.8312979936599731, 0.16144885122776031, 0.07516653835773468, 0.08403797447681427, 0.1419515758752823, 0.9589257836341858, 0.14779140055179596, 0.5103591084480286, 0.8943331837654114, 0.07006795704364777, 0.7996253371238708, 0.2085421085357666, 0.967264711856842, 0.8909068703651428, 0.19482140243053436, 0.12902943789958954, 0.13330383598804474, 0.8827940821647644, 0.17342421412467957, 0.6712718605995178, 0.05784041807055473, 0.19577667117118835, 0.09148062020540237, 0.2702361047267914, 0.14508545398712158, 0.6247052550315857, 0.21419887244701385, 0.05649707093834877, 0.9216800332069397, 0.24585482478141785, 0.8259040117263794, 0.20165663957595825, 0.2934417724609375, 0.0791219100356102, 0.10535455495119095, 0.2440059930086136, 0.9175910353660583, 0.26859840750694275, 0.1471855342388153, 0.18955036997795105, 0.3465759754180908, 0.5073147416114807, 0.15528728067874908, 0.5860866904258728, 0.4323241114616394, 0.03541093319654465, 0.20369024574756622, 0.15562650561332703, 0.5193545818328857, 0.17343437671661377, 0.5881984233856201, 0.29329922795295715, 0.2546870708465576, 0.07008867710828781, 0.305912047624588, 0.20108874142169952, 0.2516237199306488, 0.1684308797121048, 0.6775889992713928, 0.38843780755996704, 0.12312928587198257, 0.0604422464966774, 0.2491547167301178, 0.3415206968784332, 0.7407775521278381, 0.04851032793521881, 0.0607740581035614, 0.10523805767297745, 0.12310215085744858, 0.20395629107952118, 0.24897195398807526, 0.041528698056936264, 0.07353488355875015, 0.24013997614383698, 0.5765020251274109, 0.10056780278682709, 0.14258727431297302, 0.10246908664703369, 0.10389324277639389, 0.20393039286136627, 0.6374884843826294, 0.4617525041103363, 0.13613830506801605, 0.9877212643623352, 0.13559968769550323, 0.10922610759735107, 0.5807626843452454, 0.04239160567522049, 0.15477052330970764, 0.37811747193336487, 0.2083214819431305, 0.5039203763008118, 0.15735481679439545, 0.9481903910636902, 0.43001875281333923, 0.34258490800857544, 0.8739287853240967, 0.09034109115600586, 0.31147080659866333, 0.11871950328350067, 0.12033843994140625, 0.06005963310599327, 0.7984368801116943, 0.6341708898544312, 0.2613172233104706, 0.36448153853416443, 0.45223844051361084, 0.07428881525993347, 0.12251574546098709, 0.686855673789978, 0.6001332402229309, 0.15293189883232117, 0.20580114424228668, 0.1947522610425949, 0.746924102306366, 0.0927339419722557, 0.3688758313655853, 0.11025932431221008, 0.11194328218698502, 0.2886711359024048, 0.36031925678253174, 0.0963907539844513, 0.5159060955047607, 0.889133870601654, 0.6186951398849487, 0.14731290936470032, 0.044521264731884, 0.31296271085739136, 0.44897371530532837, 0.1382816582918167, 0.5304468870162964, 0.05025894567370415, 0.13150742650032043, 0.43456003069877625, 0.14407992362976074, 0.3772960305213928, 0.1393602043390274, 0.48448002338409424, 0.12444476038217545, 0.3649921715259552, 0.09408070892095566, 0.12198064476251602, 0.07953023165464401, 0.5521693229675293, 0.09925099462270737, 0.6272881627082825, 0.9749000072479248, 0.12352544814348221, 0.13204219937324524, 0.13410960137844086, 0.30173781514167786, 0.0615532360970974, 0.260820209980011, 0.8137147426605225, 0.12878000736236572, 0.17997683584690094, 0.8449702858924866, 0.25817248225212097, 0.06441881507635117, 0.9496226906776428, 0.24494712054729462, 0.10571170598268509, 0.07891002297401428, 0.10417989641427994, 0.055307962000370026, 0.6914848685264587, 0.13415807485580444, 0.9159783720970154, 0.9434283375740051, 0.08699817210435867, 0.14635707437992096, 0.10835713148117065, 0.06369363516569138, 0.7964869737625122, 0.08868163824081421, 0.2711701989173889, 0.5937590003013611, 0.2075788974761963, 0.9115412831306458, 0.9395505785942078, 0.17374564707279205, 0.6334381103515625, 0.09873880445957184, 0.21894733607769012, 0.08908240497112274, 0.4204105734825134, 0.24497516453266144, 0.04074058309197426, 0.14721320569515228, 0.07819092273712158, 0.15667709708213806, 0.28272730112075806, 0.9702871441841125, 0.5117761492729187, 0.09375443309545517, 0.12766502797603607, 0.08321218937635422, 0.06086348742246628, 0.3205133080482483, 0.4807344079017639, 0.12550117075443268, 0.1106799766421318, 0.7156013250350952, 0.11003425717353821, 0.07334434986114502, 0.10188264399766922, 0.6956544518470764, 0.10248102247714996, 0.11174700409173965, 0.7460786700248718, 0.30128762125968933, 0.08592796325683594, 0.08064930140972137, 0.7685632705688477, 0.335703045129776, 0.6126160621643066, 0.08646392822265625, 0.029366007074713707, 0.15508988499641418, 0.1399902105331421, 0.4856085479259491, 0.11622771620750427, 0.13354821503162384, 0.5723196268081665, 0.1244250014424324, 0.07158170640468597, 0.1025933027267456, 0.4258028268814087, 0.8990619778633118, 0.5921525359153748, 0.07425203174352646, 0.15673425793647766, 0.79317706823349, 0.3915646970272064, 0.5335453748703003, 0.07789605110883713, 0.1285795122385025, 0.6618621945381165, 0.19519725441932678, 0.636864960193634, 0.0468236468732357, 0.09983382374048233, 0.15116208791732788, 0.23604930937290192, 0.5713365077972412, 0.2756294310092926, 0.06970778107643127, 0.9096932411193848, 0.19860237836837769, 0.33058449625968933, 0.14508545398712158, 0.837135374546051, 0.06505630910396576, 0.09325424581766129, 0.05497365817427635, 0.7577643990516663, 0.12433692067861557, 0.045130494982004166, 0.8981754183769226, 0.4641617238521576, 0.1343761533498764, 0.4145463705062866, 0.1893252432346344, 0.7824605107307434, 0.2526904344558716, 0.6057760119438171, 0.22421838343143463, 0.34056204557418823, 0.2218495011329651, 0.16370312869548798, 0.23657956719398499, 0.08351141959428787, 0.05473465099930763, 0.3518740236759186, 0.31049850583076477, 0.4337002635002136, 0.11558739095926285, 0.23399144411087036, 0.07568178325891495, 0.3997187614440918, 0.5063936710357666, 0.11132847517728806, 0.5194976329803467, 0.174028679728508, 0.06881478428840637, 0.19894225895404816, 0.13050329685211182, 0.6083633899688721, 0.14720596373081207, 0.1422731727361679, 0.15926004946231842, 0.8217500448226929, 0.6437617540359497, 0.030824564397335052, 0.9770957231521606, 0.9696388840675354, 0.028977753594517708, 0.11805839091539383, 0.7115851044654846, 0.1074991300702095, 0.6295109987258911, 0.659737765789032, 0.6448139548301697, 0.6800157427787781, 0.9788492918014526, 0.9484124779701233, 0.12843617796897888, 0.07141990959644318, 0.18670374155044556, 0.2545080780982971, 0.09503231942653656, 0.18205417692661285, 0.6908726692199707, 0.09783095121383667, 0.5350953340530396, 0.14022958278656006, 0.12143925577402115, 0.0879594162106514, 0.08127321302890778, 0.17510639131069183, 0.3043755888938904, 0.09854855388402939, 0.09056495875120163, 0.10517428815364838, 0.08758698403835297, 0.09322229027748108, 0.07396727800369263, 0.63312828540802, 0.2098180502653122, 0.41606104373931885, 0.27882006764411926, 0.9741755127906799, 0.6049143075942993, 0.6689043641090393, 0.07955894619226456, 0.29602816700935364, 0.07423408329486847, 0.11713407188653946, 0.05575641617178917, 0.05424432456493378, 0.1654462367296219, 0.17353588342666626, 0.8267849683761597, 0.8600987792015076, 0.14951550960540771, 0.49044445157051086, 0.05038096010684967, 0.08447005599737167, 0.8862070441246033, 0.6417353749275208, 0.7379566431045532, 0.11048150807619095, 0.09313155710697174, 0.14243867993354797, 0.43463146686553955, 0.8090839385986328, 0.03858598694205284, 0.2913549840450287, 0.18628181517124176, 0.10481924563646317, 0.17658044397830963, 0.18022066354751587, 0.18055908381938934, 0.17845426499843597, 0.07331985235214233, 0.7269771695137024, 0.9148424863815308, 0.9590418934822083, 0.494566947221756, 0.2143457978963852, 0.775501549243927, 0.2633310854434967, 0.2592056691646576, 0.0631430372595787, 0.12195400148630142, 0.09745602309703827, 0.1483079344034195, 0.17957425117492676, 0.051493532955646515, 0.1670282483100891, 0.11814945936203003, 0.08863727003335953, 0.12503018975257874, 0.10407385975122452, 0.04490107297897339, 0.07930220663547516, 0.716552197933197, 0.05742664635181427, 0.08351728320121765, 0.036240361630916595, 0.9427452087402344, 0.041925571858882904, 0.17719517648220062, 0.0861416831612587, 0.08277666568756104, 0.08892801403999329, 0.06806080788373947, 0.12937848269939423, 0.4469336271286011, 0.6150302886962891, 0.4376049339771271, 0.1653946191072464, 0.631240963935852, 0.059374257922172546, 0.0585298016667366, 0.07611081749200821, 0.9058855772018433, 0.1617150604724884, 0.49372610449790955, 0.37128767371177673, 0.3428453207015991, 0.1512831598520279, 0.0997336134314537, 0.07018248736858368, 0.9364387392997742, 0.1657174974679947, 0.21005573868751526, 0.5078838467597961, 0.9468238949775696, 0.838485836982727, 0.3213772773742676, 0.07916152477264404, 0.307893842458725, 0.15972603857517242, 0.11473771184682846, 0.06117427349090576, 0.06851819157600403, 0.053594741970300674, 0.314988374710083, 0.9664475321769714, 0.09841635823249817, 0.45198243856430054, 0.10782851278781891, 0.10190090537071228, 0.5254969596862793, 0.6513193249702454, 0.22798486053943634, 0.07036681473255157, 0.2609281539916992, 0.23522070050239563, 0.9130396842956543, 0.13974179327487946, 0.10337241739034653, 0.6921228766441345, 0.0789804607629776, 0.09570867568254471, 0.03877133131027222, 0.37357810139656067, 0.39853376150131226, 0.03783592954277992, 0.07791807502508163, 0.9587406516075134, 0.8409166932106018, 0.21261121332645416, 0.7140528559684753, 0.6840340495109558, 0.20072895288467407, 0.14504791796207428, 0.07148427516222, 0.1919429898262024, 0.25054600834846497, 0.5026727914810181, 0.7752289175987244, 0.7749819755554199, 0.8161753416061401, 0.08296950906515121, 0.48923206329345703, 0.7354929447174072, 0.6211127638816833, 0.17490436136722565, 0.2401803433895111, 0.9824449419975281, 0.1124374121427536, 0.09208007156848907, 0.21470709145069122, 0.9555569291114807, 0.5747814178466797, 0.9505884051322937, 0.28675636649131775, 0.40744349360466003, 0.1937406212091446, 0.0837949886918068, 0.28242239356040955, 0.13793478906154633, 0.2113739401102066, 0.36879727244377136, 0.2556811273097992, 0.18465466797351837, 0.5269849300384521, 0.16645924746990204, 0.925057053565979, 0.27872762084007263, 0.15202832221984863, 0.24153290688991547, 0.19786083698272705, 0.7051478624343872, 0.16888555884361267, 0.1343153417110443, 0.21826080977916718, 0.08068614453077316, 0.4018171429634094, 0.3003990948200226, 0.3582245111465454, 0.16528238356113434, 0.26206088066101074, 0.33057036995887756, 0.1520535796880722, 0.8932311534881592, 0.9147811532020569, 0.03244517743587494, 0.8746503591537476, 0.13627851009368896, 0.06440756469964981, 0.1872110366821289, 0.05078677088022232, 0.07334078103303909, 0.22016336023807526, 0.08887555450201035, 0.06775002926588058, 0.21548064053058624, 0.3074958026409149, 0.9523741602897644, 0.29618099331855774, 0.14872464537620544, 0.1345270425081253, 0.9276514053344727, 0.12316062301397324, 0.18795982003211975, 0.2676618993282318, 0.5684791803359985, 0.12268802523612976, 0.09223499894142151, 0.22074668109416962, 0.14872203767299652, 0.2209358662366867, 0.5927354693412781, 0.09969618916511536, 0.12717917561531067, 0.03214757889509201, 0.10459014028310776, 0.49130314588546753, 0.050756607204675674, 0.768352746963501, 0.2953248918056488, 0.10414561629295349, 0.0669463649392128, 0.0993889644742012, 0.9700499773025513, 0.7825756072998047, 0.13618089258670807, 0.4948529005050659, 0.20264597237110138, 0.12364871799945831, 0.9698301553726196, 0.09691792726516724, 0.9802820682525635, 0.2548801600933075, 0.8052836060523987, 0.12872722744941711, 0.047283854335546494, 0.08267372846603394, 0.7854335904121399, 0.4986006021499634, 0.9830736517906189, 0.8344449996948242, 0.08342614769935608, 0.8988367319107056, 0.07913582026958466, 0.16256432235240936, 0.1266719251871109, 0.8442909121513367, 0.24973008036613464, 0.05948591232299805, 0.3102774918079376, 0.06810437142848969, 0.14699530601501465, 0.13320814073085785, 0.6312004923820496, 0.194647878408432, 0.11751347035169601, 0.3081504702568054, 0.6470027565956116, 0.09630577266216278, 0.9520646333694458, 0.2158241719007492, 0.05490634962916374, 0.44675329327583313, 0.2413443922996521, 0.24924767017364502, 0.045865848660469055, 0.9104239344596863, 0.12545569241046906, 0.031547434628009796, 0.8212133646011353, 0.07558979094028473, 0.22171376645565033, 0.20605161786079407, 0.9233387112617493, 0.09673008322715759, 0.2879188358783722, 0.5152226090431213, 0.17120914161205292, 0.19257068634033203, 0.2687547206878662, 0.250861257314682, 0.19239100813865662, 0.9183904528617859, 0.9788143038749695, 0.330215185880661, 0.05514035373926163, 0.6602117419242859, 0.3431989848613739, 0.13414059579372406, 0.09696720540523529, 0.19279734790325165, 0.43912044167518616, 0.8001068830490112, 0.23368403315544128, 0.8585601449012756, 0.2606804370880127, 0.1601490080356598, 0.600275456905365, 0.07717045396566391, 0.46964219212532043, 0.23565727472305298, 0.11846422404050827, 0.08454393595457077, 0.08788254857063293, 0.08537538349628448, 0.6000378131866455, 0.1590980887413025, 0.4283824861049652, 0.14806614816188812, 0.15148580074310303, 0.06656070053577423, 0.3550402820110321, 0.29562434554100037, 0.643750786781311, 0.3735930621623993, 0.08967041969299316, 0.788843035697937, 0.1689559817314148, 0.1200011819601059, 0.2963518500328064, 0.1377408504486084, 0.10966134816408157, 0.06155659630894661, 0.21309247612953186, 0.403719425201416, 0.1779804229736328, 0.14923211932182312, 0.1137271299958229, 0.8441887497901917, 0.8197499513626099, 0.12985090911388397, 0.24891139566898346, 0.09039386361837387, 0.07987668365240097, 0.1078457161784172, 0.14149142801761627, 0.07231692224740982, 0.031037578359246254, 0.10268858820199966, 0.19702915847301483, 0.08087815344333649, 0.42461881041526794, 0.05455290898680687, 0.184599369764328, 0.19254441559314728, 0.2439166009426117, 0.17098499834537506, 0.12473742663860321, 0.7778302431106567, 0.5048677325248718, 0.3030906021595001, 0.10724375396966934, 0.11569128185510635, 0.21345606446266174, 0.16369250416755676, 0.08613627403974533, 0.11879021674394608, 0.04656687378883362, 0.11382021009922028, 0.2567674517631531, 0.37489455938339233, 0.8971191644668579, 0.07351329922676086, 0.07803112268447876, 0.10064209252595901, 0.1817176640033722, 0.3018684685230255, 0.3621908724308014, 0.2394048422574997, 0.13902504742145538, 0.08973277360200882, 0.23619554936885834, 0.21796710789203644, 0.9653882384300232, 0.18315498530864716, 0.165557861328125, 0.9581953883171082, 0.6859418153762817, 0.07872264832258224, 0.6887549161911011, 0.2847181558609009, 0.11659014970064163, 0.9076900482177734, 0.17119744420051575, 0.9882807731628418, 0.5897115468978882, 0.08581636101007462, 0.10432488471269608, 0.23046112060546875, 0.21303296089172363, 0.6885174512863159, 0.796196460723877, 0.08707630634307861, 0.14408986270427704, 0.07081439346075058, 0.7653533816337585, 0.32539162039756775, 0.10723225027322769, 0.08095850795507431, 0.2526434659957886, 0.04359298571944237, 0.8781285285949707, 0.8955957293510437, 0.591726541519165, 0.21010245382785797, 0.839326798915863, 0.742404580116272, 0.15514354407787323, 0.12015135586261749, 0.06516388058662415, 0.26187577843666077, 0.3104982376098633, 0.17724916338920593, 0.1427791714668274, 0.12471532076597214, 0.5237746238708496, 0.05129837989807129, 0.17091700434684753, 0.7411925196647644, 0.8958102464675903, 0.19282320141792297, 0.9187607169151306, 0.675754725933075, 0.39129146933555603, 0.04784828796982765, 0.13602852821350098, 0.2945876717567444, 0.2884039878845215, 0.11653536558151245, 0.8980653285980225, 0.5308529734611511, 0.17805641889572144, 0.9799734354019165, 0.24841296672821045, 0.9290702939033508, 0.047746527940034866, 0.14920219779014587, 0.07715201377868652, 0.039106376469135284, 0.4628360867500305, 0.9497281908988953, 0.22027049958705902, 0.9608980417251587, 0.18429605662822723, 0.07999620586633682, 0.9264395236968994, 0.9796192646026611, 0.07748504728078842, 0.20357416570186615, 0.66489577293396, 0.36701589822769165, 0.1598125547170639, 0.17254553735256195, 0.8886229991912842, 0.4247722625732422, 0.15383002161979675, 0.6981849074363708, 0.051825664937496185, 0.11013194173574448, 0.10139807313680649, 0.18754443526268005, 0.6110090613365173, 0.18043270707130432, 0.08207729458808899, 0.22660595178604126, 0.27521514892578125, 0.32459601759910583, 0.08917856961488724, 0.7840893864631653, 0.9665429592132568, 0.26484978199005127, 0.1835935264825821, 0.052394162863492966, 0.09087662398815155, 0.5875054001808167, 0.3420374393463135, 0.30743059515953064, 0.056336138397455215, 0.09195747971534729, 0.054876022040843964, 0.24143816530704498, 0.06654092669487, 0.2193390130996704, 0.05166291445493698, 0.9829546809196472, 0.12455251812934875, 0.18891796469688416, 0.06321584433317184, 0.7859940528869629, 0.9183246493339539, 0.3128603398799896, 0.1275957077741623, 0.4620141386985779, 0.06829267740249634, 0.20554997026920319, 0.08446206897497177, 0.11707594990730286, 0.32342302799224854, 0.12678608298301697, 0.06396607309579849, 0.5389683246612549, 0.34015992283821106, 0.9588242769241333, 0.9242371916770935, 0.06549020111560822, 0.1530752182006836, 0.07144693285226822, 0.2624313533306122, 0.19258153438568115, 0.07586891204118729, 0.18756245076656342, 0.28536686301231384, 0.10844562202692032, 0.07320274412631989, 0.07601654529571533, 0.8461300730705261, 0.9708080887794495, 0.6154967546463013, 0.05179581791162491, 0.1359146237373352, 0.7171795964241028, 0.15769104659557343, 0.9367156624794006, 0.09412319213151932, 0.6734151840209961, 0.3860139846801758, 0.087758868932724, 0.5907324552536011, 0.2937864065170288, 0.15449205040931702, 0.08577443659305573, 0.1491522192955017, 0.6110966205596924, 0.09232243150472641, 0.24276722967624664, 0.23465833067893982, 0.26636677980422974, 0.18661628663539886, 0.32987383008003235, 0.4215770661830902, 0.502205491065979, 0.35701143741607666, 0.2526431679725647, 0.3373367488384247, 0.2744552791118622, 0.10321915149688721, 0.08747629821300507, 0.40527644753456116, 0.1943681240081787, 0.16060860455036163, 0.5540446043014526, 0.9244911670684814, 0.28272730112075806, 0.9480722546577454, 0.6241531372070312, 0.6567570567131042, 0.11207874119281769, 0.08412858843803406, 0.07355218380689621, 0.6175254583358765, 0.2391636073589325, 0.042488161474466324, 0.2323169708251953, 0.1382605880498886, 0.21014899015426636, 0.7829718589782715, 0.11634168773889542, 0.7764168977737427, 0.06443027406930923, 0.10794547945261002, 0.11853376775979996, 0.07043052464723587, 0.35812926292419434, 0.050237301737070084, 0.3185052275657654, 0.8532701730728149, 0.6366671323776245, 0.7920483946800232, 0.6690872311592102, 0.05291431024670601, 0.12007756531238556, 0.21292327344417572, 0.23677968978881836, 0.7590728998184204, 0.13333863019943237, 0.246194526553154, 0.5984537601470947, 0.0933937281370163, 0.2762932777404785, 0.10966923087835312, 0.22227537631988525, 0.2871905565261841, 0.10051555931568146, 0.07353408634662628, 0.13491642475128174, 0.11155934631824493, 0.5466418862342834, 0.06622010469436646, 0.07411739975214005, 0.09424785524606705, 0.10393930971622467, 0.1565224677324295, 0.2418106347322464, 0.7775709629058838, 0.40649548172950745, 0.4533576965332031, 0.10926596820354462, 0.07005085051059723, 0.530134379863739, 0.14677704870700836, 0.08564208447933197, 0.14689810574054718, 0.21073707938194275, 0.1623435914516449, 0.09260085970163345, 0.17546214163303375, 0.35836660861968994, 0.15249958634376526, 0.051177144050598145, 0.8913748860359192, 0.06293102353811264, 0.0576910674571991, 0.849334716796875, 0.3240163326263428, 0.11389519274234772, 0.9294513463973999, 0.9293938875198364, 0.09687101095914841, 0.9863843321800232, 0.33740755915641785, 0.30851346254348755, 0.43407654762268066, 0.44809943437576294, 0.11263347417116165, 0.12904946506023407, 0.5201724767684937, 0.05251569300889969, 0.0878170058131218, 0.8913441896438599, 0.09314527362585068, 0.10919169336557388, 0.3196467161178589, 0.199660986661911, 0.07774363458156586, 0.19945548474788666, 0.09393901377916336, 0.15488284826278687, 0.27390238642692566, 0.07577473670244217, 0.5368971824645996, 0.1686023622751236, 0.3489040434360504, 0.16316989064216614, 0.06980141252279282, 0.05223757401108742, 0.36114591360092163, 0.7777547240257263, 0.9221698045730591, 0.14757677912712097, 0.19519302248954773, 0.15338924527168274, 0.1875467151403427, 0.3225766718387604, 0.9447291493415833, 0.6685267090797424, 0.552498996257782, 0.9561615586280823, 0.9773503541946411, 0.16973459720611572, 0.10653124749660492, 0.803097128868103, 0.07809862494468689, 0.32041069865226746, 0.1503627598285675, 0.11344892531633377, 0.4375506043434143, 0.6794583201408386, 0.28558990359306335, 0.11064781248569489, 0.513252317905426, 0.8965704441070557, 0.12108533829450607, 0.971566915512085, 0.26455557346343994, 0.8051974177360535, 0.08237600326538086, 0.4044174551963806, 0.9217811822891235, 0.3446210026741028, 0.9128595590591431, 0.15678457915782928, 0.1805083155632019, 0.2943445146083832, 0.5034189224243164, 0.42533525824546814, 0.08324509114027023, 0.0747002363204956, 0.08361013978719711, 0.05235705152153969, 0.06997483968734741, 0.9429413676261902, 0.20783817768096924, 0.7589816451072693, 0.12323525547981262, 0.15550588071346283, 0.09301594644784927, 0.9486421346664429, 0.3962320387363434, 0.12264282256364822, 0.07726404070854187, 0.09336651116609573, 0.160609170794487, 0.9422991275787354, 0.985565721988678, 0.0567467026412487, 0.6011083126068115, 0.0648765116930008, 0.09221076965332031, 0.04597393423318863, 0.2970256805419922, 0.1671910285949707, 0.12434826791286469, 0.2861924171447754, 0.08092186599969864, 0.38042521476745605, 0.448191374540329, 0.07106040418148041, 0.30181896686553955, 0.9473066926002502, 0.07279711961746216, 0.13347327709197998, 0.15046487748622894, 0.07919125258922577, 0.2757803797721863, 0.6979663372039795, 0.19611181318759918, 0.9837955236434937, 0.2670122981071472, 0.11153999716043472, 0.358095645904541, 0.943040668964386, 0.09230762720108032, 0.18719139695167542, 0.12976408004760742, 0.19944050908088684, 0.11537627130746841, 0.8548641800880432, 0.1767263263463974, 0.9677973389625549, 0.3470951318740845, 0.13808390498161316, 0.6979719996452332, 0.5162292718887329, 0.1004260703921318, 0.21514789760112762, 0.10949335247278214, 0.08184705674648285, 0.11175984144210815, 0.10586509853601456, 0.16431625187397003, 0.12904280424118042, 0.1650785207748413, 0.08131776750087738, 0.29328471422195435, 0.2659461796283722, 0.13975921273231506, 0.09492461383342743, 0.7434700131416321, 0.6432123184204102, 0.12979845702648163, 0.46640434861183167, 0.4909718632698059, 0.9499312043190002, 0.06805217266082764, 0.2776428759098053, 0.36509352922439575, 0.7362796664237976, 0.16169413924217224, 0.19563087821006775, 0.08558841794729233, 0.3283853530883789, 0.9564340710639954, 0.9445574283599854, 0.5744612812995911, 0.08076992630958557, 0.45152613520622253, 0.15129774808883667, 0.10282038897275925, 0.39099788665771484, 0.42964863777160645, 0.11239912360906601, 0.6843178868293762, 0.14634546637535095, 0.06182098388671875, 0.5454867482185364, 0.2238883674144745, 0.16826632618904114, 0.2824089229106903, 0.16852207481861115, 0.20545171201229095, 0.09168565273284912, 0.2206946313381195, 0.594833493232727, 0.15872575342655182, 0.1525629758834839, 0.1992751508951187, 0.25822049379348755, 0.36895740032196045, 0.08181025832891464, 0.12254437804222107, 0.09885181486606598, 0.045094214379787445, 0.2183673083782196, 0.17108887434005737, 0.22555769979953766, 0.10672566294670105, 0.9513438940048218, 0.2817685008049011, 0.23881298303604126, 0.091891348361969, 0.19713646173477173, 0.1977570652961731, 0.05833787843585014, 0.8446767926216125, 0.11711049824953079, 0.9517002701759338, 0.15936031937599182, 0.9276478290557861, 0.6213694214820862, 0.4735875129699707, 0.16676448285579681, 0.3867360055446625, 0.6681944727897644, 0.0966436117887497, 0.610787034034729, 0.15964366495609283, 0.019192134961485863, 0.11103920638561249, 0.23229411244392395, 0.8957950472831726, 0.23238486051559448, 0.983134388923645, 0.08997202664613724, 0.6673498749732971, 0.6706554889678955, 0.047529153525829315, 0.9636693596839905, 0.12066053599119186, 0.7835742831230164, 0.6300317049026489, 0.8540547490119934, 0.08690576255321503, 0.05977717787027359, 0.21321403980255127, 0.14851535856723785, 0.1480979174375534, 0.132515087723732, 0.07007993757724762, 0.15054132044315338, 0.1910850554704666, 0.06572507321834564, 0.08317968994379044, 0.10893407464027405, 0.044448915868997574, 0.9019978642463684, 0.18692132830619812, 0.19717763364315033, 0.9890318512916565, 0.5210999250411987, 0.32493406534194946, 0.10348474979400635, 0.05162873491644859, 0.38663530349731445, 0.33039313554763794, 0.17717322707176208, 0.8171253204345703, 0.35740596055984497, 0.26872071623802185, 0.059236790984869, 0.6238995790481567, 0.06724324077367783, 0.13812734186649323, 0.09733600169420242, 0.1639658659696579, 0.8652204275131226, 0.8033509850502014, 0.3443050980567932, 0.284006804227829, 0.11647536605596542, 0.3917887508869171, 0.4785676896572113, 0.6686187982559204, 0.06217925250530243, 0.7926099300384521, 0.5718132257461548, 0.162143275141716, 0.13328644633293152, 0.6719618439674377, 0.5589846968650818, 0.1312144547700882, 0.7386345267295837, 0.1800384372472763, 0.1817350685596466, 0.7475547194480896, 0.07949074357748032, 0.9511666893959045, 0.6369145512580872, 0.19102975726127625, 0.12320012599229813, 0.048007939010858536, 0.15083704888820648, 0.2961259186267853, 0.17477504909038544, 0.40621131658554077, 0.9469956755638123, 0.7513898015022278, 0.2819085717201233, 0.11930748075246811, 0.06527215987443924, 0.09259647130966187, 0.2886923551559448, 0.7670317888259888, 0.5209245085716248, 0.23894092440605164, 0.08311987668275833, 0.6993585824966431, 0.18063458800315857, 0.558582603931427, 0.9469401836395264, 0.1606569141149521, 0.5834043622016907, 0.10410500317811966, 0.963519811630249, 0.10436058044433594, 0.07818379253149033, 0.10154330730438232, 0.17111171782016754, 0.14266163110733032, 0.3903188705444336, 0.5502722263336182, 0.9457800984382629, 0.12269087880849838, 0.6469452381134033, 0.8769394159317017, 0.06686646491289139, 0.044006332755088806, 0.040862683206796646, 0.836661696434021, 0.20538635551929474, 0.26552391052246094, 0.8943516612052917, 0.05661946162581444, 0.22553189098834991, 0.5738529562950134, 0.10971654206514359, 0.14508545398712158, 0.9377329349517822, 0.44407325983047485, 0.312498539686203, 0.08937730640172958, 0.1370605081319809, 0.09470084309577942, 0.08290384709835052, 0.048696644604206085, 0.0574517659842968, 0.2755776643753052, 0.1833440363407135, 0.9426604509353638, 0.026306934654712677, 0.231962651014328, 0.1842331886291504, 0.07757700979709625, 0.4043630361557007, 0.06702978163957596, 0.1914224624633789, 0.852189302444458, 0.3068512976169586, 0.45324721932411194, 0.18849942088127136, 0.6775882840156555, 0.9642729759216309, 0.8957362174987793, 0.1799028366804123, 0.28873738646507263, 0.12297092378139496, 0.9804525375366211, 0.12827953696250916, 0.08973364531993866, 0.07659894973039627, 0.08235666155815125, 0.14425794780254364, 0.11945824325084686, 0.15098093450069427, 0.04415689781308174, 0.46298447251319885, 0.16230408847332, 0.5015791058540344, 0.08767630159854889, 0.2651580572128296, 0.3160039484500885, 0.7121833562850952, 0.03686195984482765, 0.16036315262317657, 0.11430336534976959, 0.2988091707229614, 0.08539067208766937, 0.5215737819671631, 0.10233078896999359, 0.4710802435874939, 0.5963220596313477, 0.6220448613166809, 0.11389519274234772, 0.42380645871162415, 0.28859731554985046, 0.06869130581617355, 0.40606603026390076, 0.2907087206840515, 0.054889220744371414, 0.15464884042739868, 0.24987822771072388, 0.5310467481613159, 0.07390998303890228, 0.05485845357179642, 0.2982400059700012, 0.38667231798171997, 0.9015889167785645, 0.9020241498947144, 0.06652096658945084, 0.2940851151943207, 0.6861669421195984, 0.0691172257065773, 0.08123338222503662, 0.5001720786094666, 0.0976504236459732, 0.36227598786354065, 0.7535483241081238, 0.07832407206296921, 0.6341375708580017, 0.11515473574399948, 0.2607227563858032, 0.3849226236343384, 0.0567784383893013, 0.09255456924438477, 0.12402170896530151, 0.07449795305728912, 0.1093546450138092, 0.0811280831694603, 0.46645814180374146, 0.7680109739303589, 0.9140061736106873, 0.10683285444974899, 0.07419872283935547, 0.13095709681510925, 0.11775924265384674, 0.3891129791736603, 0.8750856518745422, 0.3003990948200226, 0.11110340803861618, 0.21154344081878662, 0.4885031282901764, 0.2410966008901596, 0.35468533635139465, 0.3149169981479645, 0.6459420919418335, 0.35441020131111145, 0.11234778165817261, 0.903404176235199, 0.10597690939903259, 0.9547812938690186, 0.14896796643733978, 0.32210370898246765, 0.9394993185997009, 0.07300235331058502, 0.07695911079645157, 0.06592785567045212, 0.2097545564174652, 0.9835897088050842, 0.20190516114234924, 0.18255266547203064, 0.6254090666770935, 0.09794172644615173, 0.171024888753891, 0.2194986492395401, 0.9323257803916931, 0.03735153377056122, 0.9411020278930664, 0.08808210492134094, 0.2643715739250183, 0.087518610060215, 0.5420107841491699, 0.06760388612747192, 0.24216826260089874, 0.1324467658996582, 0.2705225646495819, 0.8827305436134338, 0.510746955871582, 0.17972514033317566, 0.5650440454483032, 0.07779952138662338, 0.13972002267837524, 0.38068583607673645, 0.13628025352954865, 0.9569109678268433, 0.09859700500965118, 0.09700102359056473, 0.4100346863269806, 0.2149299830198288, 0.3612458407878876, 0.11324337124824524, 0.08496454358100891, 0.10171361267566681, 0.13912993669509888, 0.057951413094997406, 0.10767395049333572, 0.30966344475746155, 0.5839477777481079, 0.4589395821094513, 0.23613932728767395, 0.08788735419511795, 0.19215485453605652, 0.14635330438613892, 0.32384106516838074, 0.0844988226890564, 0.9609538316726685, 0.7178798913955688, 0.10145089775323868, 0.11908102035522461, 0.1057557538151741, 0.22314542531967163, 0.17598144710063934, 0.876484215259552, 0.07510856539011002, 0.11352312564849854, 0.06175423413515091, 0.07515362650156021, 0.13556194305419922, 0.2931445837020874, 0.15413826704025269, 0.3244682252407074, 0.4219837486743927, 0.11849460005760193, 0.15655414760112762, 0.3749224543571472, 0.12293211370706558, 0.13123548030853271, 0.20371553301811218, 0.3294696509838104, 0.7896531224250793, 0.10652512311935425, 0.23861365020275116, 0.47654736042022705, 0.27083173394203186, 0.15637284517288208, 0.7335609197616577, 0.15006612241268158, 0.19664248824119568, 0.3869285583496094, 0.21779599785804749, 0.17551323771476746, 0.06579865515232086, 0.2880057394504547, 0.7657740712165833, 0.20965172350406647, 0.3840913772583008, 0.7650660276412964, 0.09649989753961563, 0.2216399610042572, 0.881395697593689, 0.3901105523109436, 0.9814505577087402, 0.9392208456993103, 0.14840355515480042, 0.9411259889602661, 0.1363309770822525, 0.7822138071060181, 0.18969149887561798, 0.11413291841745377, 0.14026761054992676, 0.6384426951408386, 0.3351019024848938, 0.2898508906364441, 0.3275626003742218, 0.05934888496994972, 0.9512394666671753, 0.2596895396709442, 0.42440125346183777, 0.7245733141899109, 0.9228957295417786, 0.06278721988201141, 0.15448124706745148, 0.48089250922203064, 0.8493187427520752, 0.27960267663002014, 0.21166197955608368, 0.18886613845825195, 0.8894505500793457, 0.3705277740955353, 0.062125541269779205, 0.9380066990852356, 0.9766620397567749, 0.1780996173620224, 0.28758227825164795, 0.029525848105549812, 0.04002058506011963, 0.09036942571401596, 0.1612522304058075, 0.6895437240600586, 0.038501955568790436, 0.8335262537002563, 0.6418347954750061, 0.12015197426080704, 0.10571727156639099, 0.958507239818573, 0.498269259929657, 0.09384144842624664, 0.5998310446739197, 0.35548630356788635, 0.9416825771331787, 0.18629981577396393, 0.41456472873687744, 0.14236372709274292, 0.09608188271522522, 0.966873824596405, 0.23286588490009308, 0.07848513126373291, 0.4096284806728363, 0.7763144373893738, 0.05144545063376427, 0.07355231046676636, 0.09756319224834442, 0.1897139847278595, 0.18832063674926758, 0.09764931350946426, 0.10309527814388275, 0.4363037943840027, 0.06319507211446762, 0.13125881552696228, 0.2209283709526062, 0.06168774887919426, 0.07703250646591187, 0.0871938169002533, 0.20833291113376617, 0.17889399826526642, 0.23416350781917572, 0.04799068346619606, 0.36407670378685, 0.15111762285232544, 0.8837265372276306, 0.09897439181804657, 0.1779794543981552, 0.3380104899406433, 0.801679253578186, 0.21411441266536713, 0.2742061913013458, 0.1032116711139679, 0.48661884665489197, 0.15398648381233215, 0.36037105321884155, 0.09641168266534805, 0.09168750047683716, 0.5636975765228271, 0.04867135360836983, 0.18427841365337372, 0.05479854717850685, 0.34246429800987244, 0.963455855846405, 0.4756441116333008, 0.6813192367553711, 0.9238035082817078, 0.3790276348590851, 0.11811262369155884, 0.14286482334136963, 0.08746898174285889, 0.3975815176963806, 0.4560167193412781, 0.09093910455703735, 0.08233179897069931, 0.15539884567260742, 0.4912009537220001, 0.17395204305648804, 0.9079968333244324, 0.9648128747940063, 0.21285666525363922, 0.11061691492795944, 0.19131866097450256, 0.12411035597324371, 0.12184812873601913, 0.19838730990886688, 0.07558604329824448, 0.0710153728723526, 0.8553929924964905, 0.058577097952365875, 0.5151642560958862, 0.0696953684091568, 0.45894110202789307, 0.7167773842811584, 0.09668730199337006, 0.36021357774734497, 0.3403932750225067, 0.1257101446390152, 0.5443452000617981, 0.1671692281961441, 0.08721383661031723, 0.33465105295181274, 0.7297830581665039, 0.2681410312652588, 0.09302907437086105, 0.8649809956550598, 0.927145779132843, 0.18987904489040375, 0.11591731756925583, 0.07456118613481522, 0.7097222805023193, 0.247886523604393, 0.21466755867004395, 0.1785794347524643, 0.06756602227687836, 0.3017693758010864, 0.0791589692234993, 0.2096925526857376, 0.2527956962585449, 0.12318189442157745, 0.44973912835121155, 0.10973472893238068, 0.34195417165756226, 0.8064280152320862, 0.26823925971984863, 0.08712518960237503, 0.17098499834537506, 0.1214371919631958, 0.1227288618683815, 0.3126598596572876, 0.6849198341369629, 0.11004691570997238, 0.1321135014295578, 0.2206421047449112, 0.25835898518562317, 0.12679478526115417, 0.9636969566345215, 0.05330410599708557, 0.7220670580863953, 0.5321799516677856, 0.033516738563776016, 0.13815896213054657, 0.15093202888965607, 0.48851945996284485, 0.8789963126182556, 0.353766530752182, 0.1767968088388443, 0.5707838535308838, 0.03999080881476402, 0.10982032865285873, 0.07706809788942337, 0.7177120447158813, 0.9249519109725952, 0.07230491191148758, 0.03518690913915634, 0.31697115302085876, 0.05811193957924843, 0.040377426892519, 0.9190258383750916, 0.046166833490133286, 0.13534224033355713, 0.07665333151817322, 0.1649504452943802, 0.08278001099824905, 0.4575619697570801, 0.7010827660560608, 0.28601059317588806, 0.583814799785614, 0.21364498138427734, 0.1236831545829773, 0.4449627101421356, 0.1182916909456253, 0.1055881455540657, 0.8201590180397034, 0.10560555011034012, 0.12567779421806335, 0.05687711387872696, 0.11263693869113922, 0.43268224596977234, 0.11224450170993805, 0.08230490982532501, 0.9702494740486145, 0.1392611265182495, 0.1401708424091339, 0.04702580347657204, 0.19072715938091278, 0.29615354537963867, 0.0946025475859642, 0.24220210313796997, 0.22570499777793884, 0.14098452031612396, 0.15787070989608765, 0.12110348790884018, 0.21118511259555817, 0.15115246176719666, 0.7889791131019592, 0.17192335426807404, 0.3081319332122803, 0.8980756998062134, 0.069008469581604, 0.13510127365589142, 0.21548086404800415, 0.18402782082557678, 0.24835631251335144, 0.9237959980964661, 0.05581636726856232, 0.5179732441902161, 0.0761013925075531, 0.7654795050621033, 0.08881311118602753, 0.07809500396251678, 0.23672699928283691, 0.06485652923583984, 0.30356526374816895, 0.8666242957115173, 0.17274139821529388, 0.8152614831924438, 0.11458146572113037, 0.25069648027420044, 0.1344612091779709, 0.08817612379789352, 0.4036405384540558, 0.09940212965011597, 0.1283467561006546, 0.5840916037559509, 0.08248759061098099, 0.09132523089647293, 0.8574508428573608, 0.48133599758148193, 0.19975072145462036, 0.16593939065933228, 0.06664390861988068, 0.1985131949186325, 0.17788490653038025, 0.8986635208129883, 0.6098062992095947, 0.12973859906196594, 0.12535075843334198, 0.08166617155075073, 0.9389963150024414, 0.06247256323695183, 0.7396242618560791, 0.3413044512271881, 0.1801583617925644, 0.1886858195066452, 0.16514547169208527, 0.4395083785057068, 0.34664666652679443, 0.0755707323551178, 0.1249876320362091, 0.3166062831878662, 0.24228379130363464, 0.07460581511259079, 0.09443804621696472, 0.15593987703323364, 0.24196496605873108, 0.0718240812420845, 0.06073527783155441, 0.8928605318069458, 0.8160406351089478, 0.11503292620182037, 0.48130887746810913, 0.9688439965248108, 0.15054291486740112, 0.3218567669391632, 0.12848825752735138, 0.2607831656932831, 0.11791583895683289, 0.8536376357078552, 0.3587256968021393, 0.8614484071731567, 0.896023690700531, 0.36099696159362793, 0.526792585849762, 0.23536966741085052, 0.5727823972702026, 0.903296947479248, 0.9912304878234863, 0.05756255239248276, 0.9331203103065491, 0.05475209653377533, 0.13304372131824493, 0.06535188853740692, 0.7094264030456543, 0.10502058267593384, 0.7496225237846375, 0.18847715854644775, 0.08110526204109192, 0.42645132541656494, 0.22981807589530945, 0.6102653741836548, 0.12707947194576263, 0.18294307589530945, 0.18376797437667847, 0.05784069374203682, 0.8267639875411987, 0.6193974018096924, 0.30851811170578003, 0.11461858451366425, 0.0739455446600914, 0.2675964832305908, 0.093082956969738, 0.5713908076286316, 0.08098486065864563, 0.2412261962890625, 0.8174866437911987, 0.16092248260974884, 0.3716021180152893, 0.09203637391328812, 0.6969923377037048, 0.12612323462963104, 0.09909435361623764, 0.19462692737579346, 0.09113840758800507, 0.24053891003131866, 0.07284874469041824, 0.3913927376270294, 0.9413271546363831, 0.5038808584213257, 0.38189250230789185, 0.24052222073078156, 0.11389519274234772, 0.7691903114318848, 0.1858823448419571, 0.47730425000190735, 0.46058231592178345, 0.7999635338783264, 0.34944385290145874, 0.18091119825839996, 0.8405859470367432, 0.3208877742290497, 0.22775200009346008, 0.16862984001636505, 0.8553681373596191, 0.8468983173370361, 0.9063388109207153, 0.09665132313966751, 0.15694065392017365, 0.1202801913022995, 0.06550966948270798, 0.15639548003673553, 0.22071009874343872, 0.887408971786499, 0.12405401468276978, 0.8579064011573792, 0.12939628958702087, 0.08981214463710785, 0.6155325770378113, 0.6228451728820801, 0.40688490867614746, 0.6762663722038269, 0.06676138192415237, 0.1352827399969101, 0.07980727404356003, 0.6620645523071289, 0.9047821164131165, 0.15617378056049347, 0.09574109315872192, 0.3819137215614319, 0.0861627608537674, 0.17608830332756042, 0.18702776730060577, 0.0655750259757042, 0.2872222065925598, 0.07272188365459442, 0.8561610579490662, 0.1594952493906021, 0.123471699655056, 0.2927440404891968, 0.9721447825431824, 0.09366259723901749, 0.45605960488319397, 0.10520097613334656, 0.3724516034126282, 0.6771080493927002, 0.2668822705745697, 0.37648871541023254, 0.9806984066963196, 0.9455418586730957, 0.11125171184539795, 0.8263475298881531, 0.11630553007125854, 0.11583469808101654, 0.24609136581420898, 0.20853331685066223, 0.11389519274234772, 0.1345740258693695, 0.7060222625732422, 0.06418361514806747, 0.078620046377182, 0.14752614498138428, 0.07148175686597824, 0.8766964077949524, 0.40766438841819763, 0.5202729105949402, 0.054562758654356, 0.19759827852249146, 0.07469331473112106, 0.5984185934066772, 0.9612230658531189, 0.8482640385627747, 0.08246984332799911, 0.1019681766629219, 0.02943583019077778, 0.9436925649642944, 0.11177244782447815, 0.3144489824771881, 0.12691903114318848, 0.06788672506809235, 0.24676014482975006, 0.14792446792125702, 0.707614004611969, 0.5262879729270935, 0.1477142572402954, 0.06812459230422974, 0.0809406116604805, 0.8913098573684692, 0.17482037842273712, 0.5232365727424622, 0.28272730112075806, 0.11983487755060196, 0.12910547852516174, 0.2675018906593323, 0.2030254304409027, 0.5044269561767578, 0.2820839583873749, 0.35469815135002136, 0.07321254163980484, 0.06761336326599121, 0.08091507107019424, 0.28395897150039673, 0.18749627470970154, 0.08843088895082474, 0.039090003818273544, 0.47983258962631226, 0.17323407530784607, 0.19957567751407623, 0.19392481446266174, 0.4256036877632141, 0.07092466205358505, 0.059788938611745834, 0.3229062259197235, 0.3874320089817047, 0.0831495150923729, 0.4868580996990204, 0.17788508534431458, 0.25452739000320435, 0.08714168518781662, 0.32445478439331055, 0.9221809506416321, 0.1079467311501503, 0.670503556728363, 0.22630898654460907, 0.9590359926223755, 0.9150729179382324, 0.2702784836292267, 0.9747132062911987, 0.3676822781562805, 0.07098717242479324, 0.14434051513671875, 0.04938451200723648, 0.29292577505111694, 0.17683424055576324, 0.17078247666358948, 0.6689834594726562, 0.9356508851051331, 0.262136310338974, 0.4549565315246582, 0.045680589973926544, 0.1263292133808136, 0.125796839594841, 0.10681788623332977, 0.8172675371170044, 0.9913591146469116, 0.6175054907798767, 0.1974938064813614, 0.1356355845928192, 0.5032087564468384, 0.09224141389131546, 0.25985437631607056, 0.43229979276657104, 0.09219285100698471, 0.14482270181179047, 0.1768110692501068, 0.1117258220911026, 0.39146727323532104, 0.3677479922771454, 0.26748543977737427, 0.39791837334632874, 0.14482992887496948, 0.13781794905662537, 0.09811486303806305, 0.24195103347301483, 0.1578647792339325, 0.07094504684209824, 0.5627757906913757, 0.2697184383869171, 0.530951976776123, 0.8481278419494629, 0.3711787760257721, 0.9465325474739075, 0.04536147788167, 0.2642718255519867, 0.16759446263313293, 0.2421809583902359, 0.15242953598499298, 0.34523192048072815, 0.13543294370174408, 0.15728367865085602, 0.9123343825340271, 0.9195032715797424, 0.3214098811149597, 0.2606056332588196, 0.07322534173727036, 0.09682325273752213, 0.13028234243392944, 0.20159080624580383, 0.12945671379566193, 0.6825295090675354, 0.09916746616363525, 0.28850069642066956, 0.37572717666625977, 0.9196489453315735, 0.34258490800857544, 0.12964960932731628, 0.12706591188907623, 0.05350837856531143, 0.19676551222801208, 0.9183719754219055, 0.1886187642812729, 0.17578376829624176, 0.09598408639431, 0.25624653697013855, 0.9111109972000122, 0.29704707860946655, 0.250477135181427, 0.1632857471704483, 0.12927617132663727, 0.4788040220737457, 0.19690535962581635, 0.11729949712753296, 0.14755398035049438, 0.3215978145599365, 0.07752561569213867, 0.07363901287317276, 0.4653523564338684, 0.05477512627840042, 0.6039454340934753, 0.2442960888147354, 0.1027488261461258, 0.10453066974878311, 0.06078040599822998, 0.10002634674310684, 0.16648989915847778, 0.23095029592514038, 0.11436087638139725, 0.15854386985301971, 0.512018620967865, 0.03779691457748413, 0.16312535107135773, 0.057563070207834244, 0.47407492995262146, 0.08858570456504822, 0.3038042187690735, 0.5398593544960022, 0.20551343262195587, 0.11928017437458038, 0.1458505094051361, 0.47991764545440674, 0.914011538028717, 0.13920722901821136, 0.2086784839630127, 0.22385339438915253, 0.062576062977314, 0.7166010737419128, 0.06271827965974808, 0.7957066297531128, 0.11879280209541321, 0.31122198700904846, 0.06752488762140274, 0.10391336679458618, 0.1086546927690506, 0.05893617868423462, 0.19402189552783966, 0.5970278382301331, 0.18157526850700378, 0.0647747814655304, 0.1295180767774582, 0.8623477816581726, 0.8881258368492126, 0.08478488028049469, 0.9610978364944458, 0.920864999294281, 0.09508811682462692, 0.03990667685866356, 0.036843229085206985, 0.7025889754295349, 0.12402317672967911, 0.022191796451807022, 0.5264543890953064, 0.29871541261672974, 0.10900359600782394, 0.09055360406637192, 0.1755450963973999, 0.930060088634491, 0.1436162292957306, 0.9795678853988647, 0.24644921720027924, 0.08467080444097519, 0.15965355932712555, 0.20816640555858612, 0.5215269327163696, 0.10421907156705856, 0.12432751059532166, 0.06755933910608292, 0.4487031102180481, 0.7806707620620728, 0.12137015163898468, 0.09685320407152176, 0.14495813846588135, 0.30437707901000977, 0.11908681690692902, 0.32249125838279724, 0.15660488605499268, 0.22031156718730927, 0.20665472745895386, 0.5447849035263062, 0.3232171833515167, 0.1565539836883545, 0.0748315379023552, 0.12114240974187851, 0.5506666302680969, 0.3957942724227905, 0.19889669120311737, 0.38709262013435364, 0.25758662819862366, 0.11144185066223145, 0.10960225760936737, 0.07054778933525085, 0.08898729830980301, 0.1005631610751152, 0.2760688364505768, 0.8514520525932312, 0.07122311741113663, 0.4688771069049835, 0.13765399158000946, 0.04613104462623596, 0.830772340297699, 0.06946728378534317, 0.12120592594146729, 0.3288940191268921, 0.37981247901916504, 0.9793612360954285, 0.2773348093032837, 0.29098933935165405, 0.21188130974769592, 0.1252673715353012, 0.2164943814277649, 0.7206268310546875, 0.0778442770242691, 0.1032237634062767, 0.09866207838058472, 0.9577362537384033, 0.3875260353088379, 0.12182766944169998, 0.0975334420800209, 0.08127851784229279, 0.960637092590332, 0.32987481355667114, 0.12015780806541443, 0.7070830464363098, 0.23763972520828247, 0.11389519274234772, 0.9361228346824646, 0.08633577823638916, 0.15253175795078278, 0.08496642857789993, 0.7495962977409363, 0.06556732952594757, 0.8786628246307373, 0.23676864802837372, 0.3268638849258423, 0.27960461378097534, 0.1505804806947708, 0.08696629852056503, 0.4486500918865204, 0.6374944448471069, 0.17649532854557037, 0.14197002351284027, 0.05490323528647423, 0.12304720282554626, 0.5881354808807373, 0.14143802225589752, 0.09078460186719894, 0.25019389390945435, 0.0797542929649353, 0.10125669836997986, 0.9758634567260742, 0.2510035037994385, 0.11641883105039597, 0.056216608732938766, 0.26788926124572754, 0.6312295794487, 0.5003271102905273, 0.8430590033531189, 0.5440486669540405, 0.07023771107196808, 0.09778367727994919, 0.6951020359992981, 0.7242388129234314, 0.0780285969376564, 0.4287012219429016, 0.7904220819473267, 0.7340614795684814, 0.6149730682373047, 0.10308027267456055, 0.7838225364685059, 0.05978025123476982, 0.0895824283361435, 0.1501227617263794, 0.6355797052383423, 0.4786064624786377, 0.43528586626052856, 0.12414509057998657, 0.12912175059318542, 0.41912561655044556, 0.08828351646661758, 0.34258490800857544, 0.2914920449256897, 0.26764756441116333, 0.05827099829912186, 0.6333451271057129, 0.9389318823814392, 0.0839414894580841, 0.9522958993911743, 0.2588937282562256, 0.07503882050514221, 0.6564429998397827, 0.9408277273178101, 0.06978529691696167, 0.44796907901763916, 0.7200645208358765, 0.9702826738357544, 0.09370478987693787, 0.120885469019413, 0.07499612122774124, 0.2258102297782898, 0.06716398149728775, 0.1328934133052826, 0.17285703122615814, 0.6346890926361084, 0.9018094539642334, 0.6884105205535889, 0.10038930922746658, 0.14252527058124542, 0.049094971269369125, 0.05681714788079262, 0.37736615538597107, 0.6774627566337585, 0.19990350306034088, 0.22321131825447083, 0.11546235531568527, 0.263541579246521, 0.3370639681816101, 0.36315155029296875, 0.1714349091053009, 0.34828439354896545, 0.1401909589767456, 0.19676943123340607, 0.4402843713760376, 0.1555650234222412, 0.07690852880477905, 0.1263292133808136, 0.08873579651117325, 0.4205312728881836, 0.0644785463809967, 0.19644497334957123, 0.09989172220230103, 0.06153080612421036, 0.0779450386762619, 0.06042693182826042, 0.9338507056236267, 0.2484479397535324, 0.42564359307289124, 0.25708311796188354, 0.08988428115844727, 0.36434707045555115, 0.08202321827411652, 0.8093084096908569, 0.1270812302827835, 0.15386244654655457, 0.056291867047548294, 0.41294655203819275, 0.3152744472026825, 0.09183190017938614, 0.7176510691642761, 0.16306214034557343, 0.6840579509735107, 0.8972830772399902, 0.27283963561058044, 0.08113554865121841, 0.5042028427124023, 0.12177921086549759, 0.16506047546863556, 0.25831323862075806, 0.10807453840970993, 0.4715932309627533, 0.15662796795368195, 0.05474574863910675, 0.0672822967171669, 0.11436865478754044, 0.16579189896583557, 0.3177502453327179, 0.07153323292732239, 0.16730843484401703, 0.9750223755836487, 0.1374279409646988, 0.08956180512905121, 0.4150128662586212, 0.10136482119560242, 0.5075303912162781, 0.15175680816173553, 0.26838046312332153, 0.8144757747650146, 0.11534202843904495, 0.1125158965587616, 0.2472349852323532, 0.1858518272638321, 0.08392089605331421, 0.06854654848575592, 0.9260721802711487, 0.35176408290863037, 0.32908594608306885, 0.1626185178756714, 0.44509512186050415, 0.3156121075153351, 0.1660900115966797, 0.09977223724126816, 0.24894015491008759, 0.06263241171836853, 0.6186529994010925, 0.6150287389755249, 0.4013102650642395, 0.46046578884124756, 0.22483016550540924, 0.0899839699268341, 0.49130192399024963, 0.27304553985595703, 0.06605712324380875, 0.23687291145324707, 0.2052937000989914, 0.49495357275009155, 0.9691327810287476, 0.12560635805130005, 0.048436377197504044, 0.5514435172080994, 0.17463764548301697, 0.5221654176712036, 0.30246272683143616, 0.8240121006965637, 0.091466523706913, 0.09396286308765411, 0.2034251093864441, 0.5216412544250488, 0.9841974377632141, 0.33945390582084656, 0.7307345271110535, 0.14081133902072906, 0.06050606817007065, 0.5569477081298828, 0.5914444327354431, 0.10529632866382599, 0.07351028919219971, 0.5142195224761963, 0.30982184410095215, 0.16925448179244995, 0.4887418746948242, 0.10741996020078659, 0.9580259323120117, 0.22297830879688263, 0.25676900148391724, 0.13057607412338257, 0.2241622507572174, 0.314450204372406, 0.9166938066482544, 0.3168056607246399, 0.9702497124671936, 0.9370684027671814, 0.12457118928432465, 0.8256714344024658, 0.15981775522232056, 0.1724245846271515, 0.22193516790866852, 0.3756871223449707, 0.3084626793861389, 0.9490350484848022, 0.6153106093406677, 0.8128120303153992, 0.6383176445960999, 0.9459267258644104, 0.15116338431835175, 0.103788360953331, 0.68370521068573, 0.07448217272758484, 0.04218897596001625, 0.11854108422994614, 0.37647467851638794, 0.6285308003425598, 0.09365899860858917, 0.293585866689682, 0.06406521052122116, 0.14625243842601776, 0.6444012522697449, 0.32672828435897827, 0.17807529866695404, 0.15789765119552612, 0.30630069971084595, 0.0890638679265976, 0.15856437385082245, 0.14612014591693878, 0.14004984498023987, 0.23313838243484497, 0.34030261635780334, 0.059099242091178894, 0.09363874047994614, 0.21406985819339752, 0.9265537261962891, 0.24506781995296478, 0.2241726964712143, 0.08502507954835892, 0.07452654838562012, 0.11218901723623276, 0.8834590315818787, 0.6115771532058716, 0.3571604788303375, 0.14673534035682678, 0.916901171207428, 0.12696100771427155, 0.07428935915231705, 0.223382830619812, 0.08302692323923111, 0.7198467254638672, 0.08828219026327133, 0.1925291270017624, 0.11955197155475616, 0.3772622048854828, 0.1603911817073822, 0.436788409948349, 0.04615636542439461, 0.06525877863168716, 0.16285660862922668, 0.1792488843202591, 0.05545249208807945, 0.22452077269554138, 0.8058143258094788, 0.06186331436038017, 0.8896499276161194, 0.6111587882041931, 0.11411209404468536, 0.3829708695411682, 0.12238633632659912, 0.9620605707168579, 0.8421151638031006, 0.22551900148391724, 0.11646933853626251, 0.9298515915870667, 0.4712403118610382, 0.08968090265989304, 0.18584772944450378, 0.8670600056648254, 0.1416587382555008, 0.051044221967458725, 0.1734839528799057, 0.04866810515522957, 0.059295106679201126, 0.08757780492305756, 0.09459113329648972, 0.9779305458068848, 0.0931888073682785, 0.15541736781597137, 0.6830766201019287, 0.5676568150520325, 0.32855600118637085, 0.2931056618690491, 0.14405326545238495, 0.11195765435695648, 0.1019425019621849, 0.9663102626800537, 0.1354418843984604, 0.1140633076429367, 0.22561103105545044, 0.9858381152153015, 0.3951437473297119, 0.08691541105508804, 0.8153015971183777, 0.09798022359609604, 0.5893447995185852, 0.6045860052108765, 0.08280333876609802, 0.07933425903320312, 0.06069318577647209, 0.8607906699180603, 0.9622581005096436, 0.6833401322364807, 0.5252254009246826, 0.045898690819740295, 0.0638502910733223, 0.8347969055175781, 0.08772188425064087, 0.08228326588869095, 0.06475651264190674, 0.06780946999788284, 0.6085445880889893, 0.06187035143375397, 0.8547792434692383, 0.14059987664222717, 0.1005520299077034, 0.20722252130508423, 0.08966926485300064, 0.7162262201309204, 0.7172648906707764, 0.35260388255119324, 0.46791958808898926, 0.16508173942565918, 0.08580081909894943, 0.16839072108268738, 0.598042905330658, 0.11263436079025269, 0.13668328523635864, 0.2763741910457611, 0.0669383704662323, 0.6544618010520935, 0.07729438692331314, 0.15250664949417114, 0.12041245400905609, 0.2439521849155426, 0.487275630235672, 0.6876844763755798, 0.15996477007865906, 0.7923499941825867, 0.026260750368237495, 0.16691455245018005, 0.48968449234962463, 0.989111602306366, 0.24051547050476074, 0.08880434185266495, 0.1965879648923874, 0.7454525828361511, 0.9744712114334106, 0.22298869490623474, 0.5261415839195251, 0.12995712459087372, 0.44216883182525635, 0.3260604739189148, 0.9548497200012207, 0.4604022800922394, 0.7365285754203796, 0.24488689005374908, 0.9549757242202759, 0.5668855309486389, 0.11935946345329285, 0.9741036891937256, 0.11842361837625504, 0.6547331809997559, 0.12876302003860474, 0.12319245934486389, 0.31830644607543945, 0.7807751893997192, 0.24788112938404083, 0.5041927695274353, 0.11408098042011261, 0.13555936515331268, 0.3472621738910675, 0.28333398699760437, 0.41202232241630554, 0.06294219940900803, 0.16359569132328033, 0.9571851491928101, 0.1224200576543808, 0.09913843870162964, 0.1402326077222824, 0.49802833795547485, 0.05495351925492287, 0.06688527017831802, 0.12040941417217255, 0.3304935395717621, 0.22421199083328247, 0.31811100244522095, 0.41799017786979675, 0.1192769929766655, 0.23197442293167114, 0.08174854516983032, 0.0881020575761795, 0.2029162496328354, 0.15629929304122925, 0.2235409915447235, 0.10807805508375168, 0.9570298790931702, 0.18552018702030182, 0.27298760414123535, 0.8316774368286133, 0.27428799867630005, 0.15699291229248047, 0.8371334671974182, 0.05452167987823486, 0.18573161959648132, 0.20897342264652252, 0.2524605095386505, 0.14945970475673676, 0.13839176297187805, 0.13344423472881317, 0.2511657178401947, 0.2561384439468384, 0.5734396576881409, 0.8598073124885559, 0.24592480063438416, 0.2353769838809967, 0.5830227136611938, 0.09403952956199646, 0.1182931587100029, 0.19500817358493805, 0.11446017026901245, 0.7692804336547852, 0.3694494366645813, 0.12100446224212646, 0.08963050693273544, 0.22763891518115997, 0.3268941342830658, 0.08744046837091446, 0.14455094933509827, 0.09384267032146454, 0.2433301955461502, 0.06505332142114639, 0.39909282326698303, 0.41743990778923035, 0.25563183426856995, 0.23991453647613525, 0.7765292525291443, 0.28236421942710876, 0.23177042603492737, 0.3894902765750885, 0.929645299911499, 0.9726884365081787, 0.06180441752076149, 0.23554100096225739, 0.14815400540828705, 0.07042315602302551, 0.06805017590522766, 0.881811797618866, 0.1906733214855194, 0.5311218500137329, 0.05604312941431999, 0.8262328505516052, 0.8501906394958496, 0.8213232755661011, 0.18464109301567078, 0.12796834111213684, 0.8030271530151367, 0.6084606051445007, 0.10091778635978699, 0.35483023524284363, 0.0828869417309761, 0.17937937378883362, 0.0250107254832983, 0.3173862099647522, 0.2331991195678711, 0.16749289631843567, 0.17901070415973663, 0.3877716362476349, 0.3785427510738373, 0.17975866794586182, 0.10538826882839203, 0.12820212543010712, 0.16915632784366608, 0.6928367018699646, 0.05525187402963638, 0.527625322341919, 0.2268402874469757, 0.21210627257823944, 0.877755343914032, 0.09532991051673889, 0.2914787530899048, 0.04372508078813553, 0.1049877405166626, 0.08458028733730316, 0.3285389542579651, 0.5019160509109497, 0.2225964069366455, 0.11274515092372894, 0.2096375972032547, 0.16757658123970032, 0.05134736746549606, 0.058501482009887695, 0.2161106914281845, 0.3541371524333954, 0.9835959672927856, 0.6529173851013184, 0.14961309731006622, 0.7092916369438171, 0.0217078048735857, 0.7764533162117004, 0.2366979867219925, 0.6264050006866455, 0.43674206733703613, 0.21284815669059753, 0.2463536560535431, 0.5557488203048706, 0.6929382681846619, 0.06261524558067322, 0.43287891149520874, 0.9600370526313782, 0.8927721977233887, 0.5263739228248596, 0.0965842604637146, 0.2238408327102661, 0.8987703323364258, 0.44522494077682495, 0.9367051720619202, 0.16774195432662964, 0.05614299699664116, 0.12852522730827332, 0.11122522503137589, 0.1331464797258377, 0.11048037558794022, 0.5778986215591431, 0.26347658038139343, 0.9586734175682068, 0.6275333166122437, 0.10516161471605301, 0.5955991148948669, 0.948801577091217, 0.9508998990058899, 0.17564623057842255, 0.08912455290555954, 0.12983082234859467, 0.18307219445705414, 0.3400367200374603, 0.1250094473361969, 0.16906104981899261, 0.7366700172424316, 0.7966747879981995, 0.06648556888103485, 0.06040727347135544, 0.14433129131793976, 0.08059776574373245, 0.10528338700532913, 0.05459383875131607, 0.04894237220287323, 0.6052552461624146, 0.34511590003967285, 0.1068057119846344, 0.5208898186683655, 0.07850507646799088, 0.2644416391849518, 0.12894661724567413, 0.6364933252334595, 0.3964446187019348, 0.029154837131500244, 0.21917876601219177, 0.2716737985610962, 0.2415829300880432, 0.11559479683637619, 0.3815443813800812, 0.7034658789634705, 0.4883963167667389, 0.0536683052778244, 0.46281540393829346, 0.06619250029325485, 0.054755955934524536, 0.1359410583972931, 0.10373921692371368, 0.27157747745513916, 0.15483853220939636, 0.5612711310386658, 0.8808414340019226, 0.14413422346115112, 0.06286625564098358, 0.11772818863391876, 0.7350025773048401, 0.12477608025074005, 0.6153697967529297, 0.22054177522659302, 0.09921235591173172, 0.5964905619621277, 0.6036830544471741, 0.10551460087299347]\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "id": "Tv0HExQIRAjr"
   },
   "outputs": [],
   "source": [
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "id": "XsCGfagARAjt"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_CNN.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rW9cKb57GPYd"
   },
   "source": [
    "#### Freezing weight embeddings and training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e3Jyo-lmGOpp",
    "outputId": "778bc6e1-012b-4f29-93cf-dde2ddfdb73e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.363 | Train Acc: 84.13%\n",
      "\t Val. Loss: 0.506 |  Val. Acc: 76.64%\n",
      "Epoch: 02 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.331 | Train Acc: 86.00%\n",
      "\t Val. Loss: 0.510 |  Val. Acc: 76.77%\n",
      "Epoch: 03 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.325 | Train Acc: 86.09%\n",
      "\t Val. Loss: 0.520 |  Val. Acc: 76.80%\n",
      "Epoch: 04 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.314 | Train Acc: 86.74%\n",
      "\t Val. Loss: 0.519 |  Val. Acc: 76.62%\n",
      "Epoch: 05 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.285 | Train Acc: 88.26%\n",
      "\t Val. Loss: 0.524 |  Val. Acc: 76.42%\n",
      "Epoch: 06 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.270 | Train Acc: 89.09%\n",
      "\t Val. Loss: 0.536 |  Val. Acc: 75.66%\n",
      "Epoch: 07 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.255 | Train Acc: 89.06%\n",
      "\t Val. Loss: 0.539 |  Val. Acc: 75.91%\n",
      "Epoch: 08 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.242 | Train Acc: 90.39%\n",
      "\t Val. Loss: 0.552 |  Val. Acc: 76.67%\n",
      "Epoch: 09 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.231 | Train Acc: 90.64%\n",
      "\t Val. Loss: 0.559 |  Val. Acc: 76.57%\n",
      "Epoch: 10 | Epoch Time: 0m 0s\n",
      "\tTrain Loss: 0.224 | Train Acc: 91.24%\n",
      "\t Val. Loss: 0.591 |  Val. Acc: 75.91%\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "FREEZING = 10\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "#freeze embeddings\n",
    "model_CNN.embedding.weight.requires_grad = unfrozen = False\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train_func3(model_CNN, train_iter, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate3(model_CNN, valid_iter, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model_CNN.state_dict(), 'tut5-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')\n",
    "\n",
    "    if (epoch + 1) >= FREEZING:\n",
    "        #unfreeze embeddings\n",
    "        model_CNN.embedding.weight.requires_grad = unfrozen = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2BaSbzPzIeG8",
    "outputId": "b560f8ad-d471-44e2-9b59-f4957676e42e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.488 | Test Acc: 78.12%\n"
     ]
    }
   ],
   "source": [
    "model_CNN.load_state_dict(torch.load('tut5-model.pt'))\n",
    "\n",
    "test_loss, test_acc = evaluate3(model_CNN, test_iter, criterion)\n",
    "\n",
    "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N0eC5WHYEl4P"
   },
   "source": [
    "#### Making final predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "id": "OyS2AhfpU0oA"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en')\n",
    "model_CNN.load_state_dict(torch.load('tut5-model.pt'))\n",
    "\n",
    "def predict_sentiment(model, sentence, min_len = 5):\n",
    "    model.eval()\n",
    "    tokenized = [tok.text for tok in nlp.tokenizer(sentence.lower())]\n",
    "    if len(tokenized) < min_len:\n",
    "        tokenized += ['<pad>'] * (min_len - len(tokenized))\n",
    "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "    tensor = torch.LongTensor(indexed).to(device)\n",
    "    tensor = tensor.unsqueeze(0)\n",
    "    prediction = torch.sigmoid(model(tensor))\n",
    "    return prediction.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "id": "7oLnf15vtkau"
   },
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "for text in test['text']:\n",
    "  test_predictions.append(predict_sentiment(model_CNN,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zd4Iza2Ot-xW",
    "outputId": "d09566af-a99f-4867-b819-24d6122da423"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5283995270729065, 0.9746401906013489, 0.06333830207586288, 0.5737374424934387, 0.7506172060966492, 0.07446950674057007, 0.3572539985179901, 0.24838243424892426, 0.13165640830993652, 0.06532054394483566, 0.4001598358154297, 0.08954495191574097, 0.9209583401679993, 0.06060301885008812, 0.1508021056652069, 0.5595267415046692, 0.2201589196920395, 0.9784345626831055, 0.9113001227378845, 0.09412188082933426, 0.09148187935352325, 0.11780577152967453, 0.07467789202928543, 0.29299986362457275, 0.06749706715345383, 0.8941499590873718, 0.07728300243616104, 0.045582789927721024, 0.04353645071387291, 0.968582034111023, 0.33230239152908325, 0.9319546818733215, 0.08120282739400864, 0.041865747421979904, 0.30544084310531616, 0.2326241433620453, 0.5081981420516968, 0.14489521086215973, 0.07469717413187027, 0.493653804063797, 0.27219727635383606, 0.0871315598487854, 0.08838817477226257, 0.6908309459686279, 0.1719997376203537, 0.348044216632843, 0.28583860397338867, 0.5825009346008301, 0.03354937583208084, 0.12487149238586426, 0.04217652976512909, 0.2838905453681946, 0.49150025844573975, 0.8528624176979065, 0.7896785140037537, 0.2224009931087494, 0.30819520354270935, 0.05121603608131409, 0.9226936101913452, 0.09777006506919861, 0.5523804426193237, 0.6226841807365417, 0.3060002028942108, 0.1518155187368393, 0.02970387414097786, 0.5143962502479553, 0.5266764760017395, 0.1979595124721527, 0.09814877808094025, 0.07739878445863724, 0.9370361566543579, 0.04041213169693947, 0.14288963377475739, 0.19328472018241882, 0.06879349052906036, 0.5230953693389893, 0.24630984663963318, 0.1758975088596344, 0.04518784210085869, 0.12258277833461761, 0.08454182744026184, 0.06271081417798996, 0.13234268128871918, 0.729047954082489, 0.05234045907855034, 0.030990498140454292, 0.05425536632537842, 0.9750843644142151, 0.10298420488834381, 0.10959372669458389, 0.4351370334625244, 0.9673205614089966, 0.8602197170257568, 0.05413094907999039, 0.5862324237823486, 0.05704363062977791, 0.27134251594543457, 0.07016584277153015, 0.5557262301445007, 0.09134044498205185, 0.12881700694561005, 0.04357970878481865, 0.038992926478385925, 0.14133965969085693, 0.09975411742925644, 0.3462441563606262, 0.9648008942604065, 0.14220760762691498, 0.13497930765151978, 0.1375630795955658, 0.15422631800174713, 0.14374293386936188, 0.28718507289886475, 0.794689416885376, 0.5002568364143372, 0.21071182191371918, 0.1320226490497589, 0.47093209624290466, 0.09505482763051987, 0.11124344170093536, 0.07032384723424911, 0.6183171272277832, 0.10613848268985748, 0.04655475541949272, 0.09335727244615555, 0.23457440733909607, 0.19799654185771942, 0.243245467543602, 0.6948063373565674, 0.037543587386608124, 0.33031007647514343, 0.054849665611982346, 0.031709421426057816, 0.19258420169353485, 0.0538528636097908, 0.07003095746040344, 0.03509519249200821, 0.4379362165927887, 0.04684194549918175, 0.1746438592672348, 0.0646214634180069, 0.8769596219062805, 0.15748202800750732, 0.44724810123443604, 0.43270689249038696, 0.04152233153581619, 0.056664057075977325, 0.11200862377882004, 0.03936057537794113, 0.03179129958152771, 0.17332039773464203, 0.09719500690698624, 0.0390578918159008, 0.9409309029579163, 0.09626727551221848, 0.052283842116594315, 0.05646878480911255, 0.061813484877347946, 0.04321561008691788, 0.09184883534908295, 0.061567917466163635, 0.16030752658843994, 0.08911444246768951, 0.031441040337085724, 0.19107834994792938, 0.07325679808855057, 0.12572775781154633, 0.34230437874794006, 0.04259498789906502, 0.08756911754608154, 0.17844919860363007, 0.11212451010942459, 0.07887613773345947, 0.051996100693941116, 0.17301836609840393, 0.5877490043640137, 0.24350818991661072, 0.24307049810886383, 0.0556158572435379, 0.8777605891227722, 0.06747744232416153, 0.6420087218284607, 0.15818871557712555, 0.5144767165184021, 0.7155135273933411, 0.08172468841075897, 0.9620921015739441, 0.2502489984035492, 0.17206905782222748, 0.9715386629104614, 0.15325187146663666, 0.691300630569458, 0.7459860444068909, 0.8489269614219666, 0.04634174704551697, 0.9784345626831055, 0.7566384077072144, 0.09255225211381912, 0.10609796643257141, 0.023950902745127678, 0.23308280110359192, 0.0351453460752964, 0.05219531059265137, 0.3287205696105957, 0.16491547226905823, 0.9557548761367798, 0.13516314327716827, 0.06626836955547333, 0.06197947636246681, 0.12306259572505951, 0.157164067029953, 0.8964090943336487, 0.03985761106014252, 0.933441162109375, 0.04900897666811943, 0.036020781844854355, 0.07611905038356781, 0.11604540795087814, 0.19242694973945618, 0.3770228326320648, 0.19643454253673553, 0.04822903499007225, 0.03575057536363602, 0.9844409227371216, 0.11537180095911026, 0.3856448233127594, 0.20573845505714417, 0.6246834397315979, 0.6394979357719421, 0.04944189265370369, 0.4171333312988281, 0.0817597359418869, 0.07359200716018677, 0.06901002675294876, 0.06453919410705566, 0.05092814937233925, 0.5561259388923645, 0.25567522644996643, 0.04358658194541931, 0.06049175187945366, 0.12759390473365784, 0.2674844563007355, 0.059345293790102005, 0.13380596041679382, 0.09600237011909485, 0.9897289276123047, 0.2737294137477875, 0.08504659682512283, 0.058446601033210754, 0.17953501641750336, 0.9628995060920715, 0.059500981122255325, 0.0740349143743515, 0.39409139752388, 0.3467739224433899, 0.06483884900808334, 0.9843165874481201, 0.12137740105390549, 0.07932475209236145, 0.07104812562465668, 0.49125412106513977, 0.04489544779062271, 0.2198052555322647, 0.0841611996293068, 0.8567478060722351, 0.11616001278162003, 0.5867524743080139, 0.4886153042316437, 0.03216928243637085, 0.10487955063581467, 0.19632737338542938, 0.06216062977910042, 0.08022063970565796, 0.1655275821685791, 0.9814908504486084, 0.10898385941982269, 0.11603127419948578, 0.06522174924612045, 0.17301514744758606, 0.27107828855514526, 0.487842857837677, 0.08042873442173004, 0.19057828187942505, 0.06435388326644897, 0.8300941586494446, 0.9065921902656555, 0.1953132003545761, 0.12335436791181564, 0.6035860776901245, 0.09227550774812698, 0.08222515136003494, 0.02785376086831093, 0.464497834444046, 0.128358393907547, 0.051113322377204895, 0.08897056430578232, 0.15365223586559296, 0.09478528797626495, 0.04579712450504303, 0.1316983848810196, 0.16694530844688416, 0.0979972630739212, 0.462342232465744, 0.030401477590203285, 0.15371155738830566, 0.0365445539355278, 0.9571489095687866, 0.6654154658317566, 0.05522347614169121, 0.6922446489334106, 0.03979552537202835, 0.20869815349578857, 0.04497158154845238, 0.04449222609400749, 0.29340627789497375, 0.1924528032541275, 0.5322331190109253, 0.39247003197669983, 0.8746033906936646, 0.0620046928524971, 0.188172847032547, 0.1898190975189209, 0.0772823840379715, 0.1272984892129898, 0.16068807244300842, 0.9485483765602112, 0.6629387736320496, 0.6444836258888245, 0.6064940690994263, 0.26419806480407715, 0.1034703329205513, 0.13440167903900146, 0.037968408316373825, 0.040710918605327606, 0.9475704431533813, 0.06994978338479996, 0.42539918422698975, 0.531183123588562, 0.11306902021169662, 0.2781727612018585, 0.04168560355901718, 0.9828055500984192, 0.08451155573129654, 0.8976098299026489, 0.36583051085472107, 0.6269064545631409, 0.8539292216300964, 0.06699016690254211, 0.05604125186800957, 0.02700001746416092, 0.054322756826877594, 0.0775843933224678, 0.5167977213859558, 0.23172003030776978, 0.12043298035860062, 0.10686862468719482, 0.04668718948960304, 0.09898574650287628, 0.6780699491500854, 0.09496035426855087, 0.7620041370391846, 0.06383775174617767, 0.9088608026504517, 0.9810077548027039, 0.04907461255788803, 0.08282671123743057, 0.10130349546670914, 0.8933200836181641, 0.06606793403625488, 0.366983026266098, 0.44125327467918396, 0.07009952515363693, 0.09152577817440033, 0.10399773716926575, 0.04590022563934326, 0.6604007482528687, 0.15593773126602173, 0.2108064889907837, 0.954901397228241, 0.587402880191803, 0.09073896706104279, 0.963371217250824, 0.20517663657665253, 0.33591198921203613, 0.23536156117916107, 0.4850136637687683, 0.05852283537387848, 0.4490406811237335, 0.06257378309965134, 0.9489229321479797, 0.1285376101732254, 0.041122276335954666, 0.2645687460899353, 0.24744361639022827, 0.0866713747382164, 0.06380260735750198, 0.522635817527771, 0.14357976615428925, 0.9649218320846558, 0.8783577084541321, 0.07916680723428726, 0.1330830305814743, 0.05840173363685608, 0.4868066906929016, 0.04806702584028244, 0.06758912652730942, 0.2629656493663788, 0.11216755956411362, 0.14179064333438873, 0.0623827688395977, 0.08792123943567276, 0.035551633685827255, 0.08134603500366211, 0.051960211247205734, 0.146327942609787, 0.0765894204378128, 0.05435307323932648, 0.9078081846237183, 0.35037413239479065, 0.4547208249568939, 0.7698031663894653, 0.8407888412475586, 0.5821385979652405, 0.07234953343868256, 0.8955633044242859, 0.25070154666900635, 0.0559384860098362, 0.2969748079776764, 0.15582525730133057, 0.04037792608141899, 0.037388697266578674, 0.368089497089386, 0.4595484137535095, 0.8443635106086731, 0.9781726598739624, 0.2956799566745758, 0.6092240214347839, 0.3844872713088989, 0.1303458958864212, 0.04382290318608284, 0.16517214477062225, 0.1428500860929489, 0.1005912721157074, 0.05672028660774231, 0.09740825742483139, 0.10985660552978516, 0.046901945024728775, 0.12908510863780975, 0.9760028719902039, 0.030902082100510597, 0.0340142585337162, 0.08575835078954697, 0.2804804742336273, 0.9700424671173096, 0.03865664824843407, 0.0351472944021225, 0.10684560984373093, 0.44686800241470337, 0.0612323172390461, 0.9351593255996704, 0.05301253870129585, 0.055620819330215454, 0.09403111785650253, 0.0301367174834013, 0.29669493436813354, 0.6126183867454529, 0.7113238573074341, 0.5259412527084351, 0.09760574996471405, 0.14822828769683838, 0.05396437272429466, 0.07037360221147537, 0.15073907375335693, 0.22356382012367249, 0.09563127905130386, 0.026816409081220627, 0.690365195274353, 0.06660516560077667, 0.14035901427268982, 0.06842580437660217, 0.04912304878234863, 0.059141241014003754, 0.1050419807434082, 0.920482873916626, 0.031582679599523544, 0.4986433982849121, 0.07463423162698746, 0.9846785068511963, 0.024934234097599983, 0.08718833327293396, 0.3827361762523651, 0.027464738115668297, 0.16966670751571655, 0.0720328763127327, 0.10297349095344543, 0.8936717510223389, 0.13682901859283447, 0.04630179703235626, 0.0738096609711647, 0.0662875846028328, 0.22975240647792816, 0.02953583188354969, 0.07073239237070084, 0.13519471883773804, 0.20707474648952484, 0.08576783537864685, 0.10477393120527267, 0.14139652252197266, 0.5409386157989502, 0.023422149941325188, 0.16795586049556732, 0.11363819241523743, 0.40481582283973694, 0.8595040440559387, 0.06992150098085403, 0.0638345330953598, 0.8826932311058044, 0.057609330862760544, 0.0821695476770401, 0.05828510597348213, 0.451893150806427, 0.9723185300827026, 0.6214339137077332, 0.04007076844573021, 0.2158990502357483, 0.7290074229240417, 0.11041530966758728, 0.08729319274425507, 0.07477681338787079, 0.14624032378196716, 0.08877507597208023, 0.060637906193733215, 0.11386087536811829, 0.06361629068851471, 0.15197636187076569, 0.08571112900972366, 0.37857067584991455, 0.043748896569013596, 0.12147628515958786, 0.048430681228637695, 0.097530297935009, 0.49529293179512024, 0.965612530708313, 0.029265791177749634, 0.5677751898765564, 0.43799325823783875, 0.12983623147010803, 0.07145824283361435, 0.29500168561935425, 0.034430716186761856, 0.08872584998607635, 0.3371942937374115, 0.10078568011522293, 0.09373952448368073, 0.21270519495010376, 0.25077638030052185, 0.13279174268245697, 0.06753794103860855, 0.060882676392793655, 0.13971593976020813, 0.03057788871228695, 0.16378605365753174, 0.9723787307739258, 0.04929134249687195, 0.19876091182231903, 0.6441710591316223, 0.12143835425376892, 0.19271960854530334, 0.10370813310146332, 0.04438260570168495, 0.8160569071769714, 0.1076580137014389, 0.1387663185596466, 0.11608075350522995, 0.9532841444015503, 0.666015625, 0.8446811437606812, 0.9739504456520081, 0.16129076480865479, 0.2675381600856781, 0.05665191635489464, 0.23370997607707977, 0.062376681715250015, 0.1844126433134079, 0.15267010033130646, 0.9257696270942688, 0.49291980266571045, 0.03750922158360481, 0.7671125531196594, 0.09108006954193115, 0.04512004554271698, 0.04561612010002136, 0.14733077585697174, 0.9696206450462341, 0.8978881239891052, 0.2989808917045593, 0.053900737315416336, 0.0855632945895195, 0.0878874883055687, 0.08277111500501633, 0.07607968896627426, 0.1662561446428299, 0.6528930068016052, 0.9294884204864502, 0.09535283595323563, 0.08621914684772491, 0.3949121832847595, 0.32605502009391785, 0.17709925770759583, 0.17935645580291748, 0.17779795825481415, 0.6400324106216431, 0.3035740554332733, 0.8990793228149414, 0.9606850743293762, 0.26427340507507324, 0.1030762568116188, 0.09011504054069519, 0.04999002069234848, 0.055441755801439285, 0.047715380787849426, 0.13675379753112793, 0.041838452219963074, 0.07201283425092697, 0.37584900856018066, 0.31400007009506226, 0.15362860262393951, 0.1754147708415985, 0.037845153361558914, 0.790437638759613, 0.07431574165821075, 0.1534356325864792, 0.04533838853240013, 0.056416042149066925, 0.07837439328432083, 0.14124321937561035, 0.042920034378767014, 0.5031062364578247, 0.2127743810415268, 0.06445799767971039, 0.0587194450199604, 0.9770738482475281, 0.04003206640481949, 0.07299544662237167, 0.05087437480688095, 0.07344722747802734, 0.05711209401488304, 0.04552948847413063, 0.6630821228027344, 0.04331858456134796, 0.10039042681455612, 0.8194433450698853, 0.044503163546323776, 0.3979637622833252, 0.7941358685493469, 0.046326763927936554, 0.9669817090034485, 0.05088547244668007, 0.03825545310974121, 0.5065898299217224, 0.25808894634246826, 0.3092593848705292, 0.0872846469283104, 0.0842241570353508, 0.8262415528297424, 0.03263093903660774, 0.339765340089798, 0.2502225935459137, 0.3712300956249237, 0.042243730276823044, 0.5180103778839111, 0.06894984096288681, 0.13347743451595306, 0.5312263369560242, 0.04262655973434448, 0.044117845594882965, 0.15304848551750183, 0.9765275716781616, 0.36644819378852844, 0.09561226516962051, 0.033526912331581116, 0.1509668529033661, 0.10749806463718414, 0.5860291719436646, 0.15518026053905487, 0.7055016756057739, 0.10585909336805344, 0.29405197501182556, 0.9096640348434448, 0.035345081239938736, 0.21076862514019012, 0.816918671131134, 0.7993066310882568, 0.7275079488754272, 0.2039700150489807, 0.725479006767273, 0.06091341748833656, 0.23763670027256012, 0.18077029287815094, 0.9549517035484314, 0.6676313281059265, 0.1475970596075058, 0.14102908968925476, 0.05503571033477783, 0.07540623843669891, 0.18178941309452057, 0.12947475910186768, 0.23076234757900238, 0.076292984187603, 0.06200159713625908, 0.3413812816143036, 0.05900433659553528, 0.23299835622310638, 0.6976761817932129, 0.21279160678386688, 0.20582233369350433, 0.9615805745124817, 0.28822267055511475, 0.06525665521621704, 0.283250093460083, 0.24662581086158752, 0.03671548515558243, 0.21614031493663788, 0.05310923606157303, 0.650306224822998, 0.11241096258163452, 0.06898750364780426, 0.8029413223266602, 0.06612254679203033, 0.4235445559024811, 0.7133358120918274, 0.2945040166378021, 0.2913638651371002, 0.5429951548576355, 0.06981364637613297, 0.06821420043706894, 0.13401037454605103, 0.7242246270179749, 0.07179348170757294, 0.015045900829136372, 0.38247495889663696, 0.7814582586288452, 0.1187494546175003, 0.08408263325691223, 0.061221204698085785, 0.08727851510047913, 0.23074868321418762, 0.20400390028953552, 0.0480416864156723, 0.200820192694664, 0.06896669417619705, 0.0798979103565216, 0.3953891694545746, 0.05405287817120552, 0.8395661115646362, 0.02981850691139698, 0.8907662034034729, 0.03382463380694389, 0.8150620460510254, 0.08875764161348343, 0.0761006698012352, 0.1498102992773056, 0.2555449903011322, 0.429240345954895, 0.2490597367286682, 0.21424011886119843, 0.05201288312673569, 0.2746753990650177, 0.06619513779878616, 0.052207693457603455, 0.45837804675102234, 0.10115619003772736, 0.37031757831573486, 0.11085326224565506, 0.5905148983001709, 0.10814500600099564, 0.048616647720336914, 0.1531636267900467, 0.09237842261791229, 0.5783272385597229, 0.07007724791765213, 0.7733994722366333, 0.21760627627372742, 0.04957197234034538, 0.5560637712478638, 0.9899779558181763, 0.9430862665176392, 0.05958186462521553, 0.0767190232872963, 0.9870833158493042, 0.09624342620372772, 0.21167124807834625, 0.6779748201370239, 0.24954082071781158, 0.1806618720293045, 0.36186516284942627, 0.28587421774864197, 0.07470536231994629, 0.0760003924369812, 0.4581137001514435, 0.29432976245880127, 0.29203617572784424, 0.08316823840141296, 0.9857677817344666, 0.5598484873771667, 0.20183566212654114, 0.12985661625862122, 0.9830411672592163, 0.5535261034965515, 0.12119387835264206, 0.14833539724349976, 0.1661406010389328, 0.05466907471418381, 0.07568194717168808, 0.0742841437458992, 0.9071246385574341, 0.475492924451828, 0.29957523941993713, 0.06760786473751068, 0.06797337532043457, 0.8205429911613464, 0.16692258417606354, 0.7140287160873413, 0.9608870148658752, 0.13853773474693298, 0.10995712876319885, 0.12118034809827805, 0.1638188660144806, 0.08236439526081085, 0.04486739635467529, 0.2127196490764618, 0.07498066127300262, 0.10798202455043793, 0.5352353453636169, 0.27260836958885193, 0.10714823752641678, 0.08879538625478745, 0.9201048612594604, 0.022314012050628662, 0.21635130047798157, 0.15622061491012573, 0.06641516089439392, 0.1037730798125267, 0.07419828325510025, 0.03132255747914314, 0.15587124228477478, 0.3178447186946869, 0.043095432221889496, 0.09280885010957718, 0.983648419380188, 0.029526783153414726, 0.07595899701118469, 0.028524257242679596, 0.0951201394200325, 0.45882830023765564, 0.9043982028961182, 0.19480429589748383, 0.341981440782547, 0.1726653277873993, 0.09625492244958878, 0.09900585561990738, 0.20912256836891174, 0.09969641268253326, 0.1951146423816681, 0.05762847140431404, 0.13320307433605194, 0.05682000890374184, 0.9545150995254517, 0.11351897567510605, 0.059987228363752365, 0.13572481274604797, 0.6850622892379761, 0.25771650671958923, 0.12938685715198517, 0.028456853702664375, 0.25830158591270447, 0.16815871000289917, 0.7082467675209045, 0.05882987752556801, 0.9701412916183472, 0.08909393101930618, 0.2044602632522583, 0.13817982375621796, 0.43058690428733826, 0.9050909280776978, 0.09430558979511261, 0.038758113980293274, 0.5260100960731506, 0.029648546129465103, 0.14051787555217743, 0.35674118995666504, 0.044103074818849564, 0.17350666224956512, 0.3221420347690582, 0.10490197688341141, 0.31152379512786865, 0.068557508289814, 0.9608164429664612, 0.038966212421655655, 0.9821296334266663, 0.5332281589508057, 0.17547956109046936, 0.41600534319877625, 0.10643620043992996, 0.3959140479564667, 0.028386659920215607, 0.06445453315973282, 0.3456178605556488, 0.09638557583093643, 0.09258735924959183, 0.07632745057344437, 0.36595141887664795, 0.9823733568191528, 0.4930926263332367, 0.4018027186393738, 0.9670960903167725, 0.7613374590873718, 0.972075343132019, 0.11140792071819305, 0.1019456684589386, 0.14325101673603058, 0.7011667490005493, 0.056218091398477554, 0.7136189341545105, 0.10314710438251495, 0.06022492051124573, 0.04268024116754532, 0.048666492104530334, 0.7467827796936035, 0.0712350532412529, 0.20228858292102814, 0.6321057081222534, 0.9730805158615112, 0.8954095840454102, 0.06339753419160843, 0.028397057205438614, 0.029389820992946625, 0.35895872116088867, 0.5064469575881958, 0.35311684012413025, 0.07887423783540726, 0.2511022686958313, 0.20820669829845428, 0.03038201667368412, 0.15004074573516846, 0.03235584497451782, 0.4601342976093292, 0.12696686387062073, 0.8376486301422119, 0.10064427554607391, 0.13466688990592957, 0.07974418997764587, 0.05968984216451645, 0.20635384321212769, 0.23425047099590302, 0.29483506083488464, 0.050675664097070694, 0.8055770993232727, 0.21124745905399323, 0.8946678638458252, 0.5163282752037048, 0.23157835006713867, 0.19815538823604584, 0.05839882418513298, 0.0941471979022026, 0.3162735104560852, 0.6472300887107849, 0.6029407382011414, 0.9438889026641846, 0.9599809050559998, 0.028825871646404266, 0.26243993639945984, 0.05855276808142662, 0.06692007184028625, 0.5663073062896729, 0.6541134119033813, 0.2426019012928009, 0.25286227464675903, 0.06341632455587387, 0.9827868938446045, 0.45095282793045044, 0.7786230444908142, 0.3491166830062866, 0.8882509469985962, 0.14637310802936554, 0.44440290331840515, 0.06593167036771774, 0.9130579829216003, 0.06333531439304352, 0.0870758444070816, 0.5313229560852051, 0.040264129638671875, 0.10256034135818481, 0.12850813567638397, 0.09632608294487, 0.44197118282318115, 0.10669390857219696, 0.18982090055942535, 0.027994321659207344, 0.03815269097685814, 0.2428428828716278, 0.059759266674518585, 0.08693340420722961, 0.2314780205488205, 0.2147406041622162, 0.9675801992416382, 0.15105511248111725, 0.8677228093147278, 0.5322845578193665, 0.30683547258377075, 0.8842760920524597, 0.067970409989357, 0.8536834716796875, 0.3062628507614136, 0.9361310005187988, 0.8738619685173035, 0.09546460956335068, 0.03254030644893646, 0.48790261149406433, 0.01888038031756878, 0.056856054812669754, 0.438244491815567, 0.05772370472550392, 0.05716361105442047, 0.045947059988975525, 0.054920073598623276, 0.16634416580200195, 0.22288955748081207, 0.12796084582805634, 0.07452919334173203, 0.51025390625, 0.05167664214968681, 0.04919133707880974, 0.270465612411499, 0.9111953973770142, 0.06318880617618561, 0.3976845443248749, 0.473011314868927, 0.30724063515663147, 0.031562238931655884, 0.04810012876987457, 0.2377958446741104, 0.0881996750831604, 0.10615839064121246, 0.6471643447875977, 0.49207308888435364, 0.48145511746406555, 0.0582285039126873, 0.07419061660766602, 0.24541911482810974, 0.3394692838191986, 0.9632425904273987, 0.995849609375, 0.03093271516263485, 0.9609301686286926, 0.1079404428601265, 0.134011372923851, 0.35112351179122925, 0.21191959083080292, 0.0952269658446312, 0.20511628687381744, 0.7309114336967468, 0.04385564476251602, 0.050539515912532806, 0.047174639999866486, 0.11583656817674637, 0.459099143743515, 0.9299716353416443, 0.18936023116111755, 0.5493256449699402, 0.24662581086158752, 0.5273857712745667, 0.7527452111244202, 0.9655538201332092, 0.27330073714256287, 0.9461491703987122, 0.020668180659413338, 0.07944276183843613, 0.24238485097885132, 0.0870475322008133, 0.28006711602211, 0.3287791311740875, 0.8114549517631531, 0.0784035250544548, 0.038524143397808075, 0.07317691296339035, 0.07093725353479385, 0.9599730968475342, 0.09266967326402664, 0.4299194812774658, 0.9032561779022217, 0.04148833081126213, 0.7903850078582764, 0.17205080389976501, 0.9735270142555237, 0.8967034816741943, 0.13955806195735931, 0.06740132719278336, 0.07418642938137054, 0.8924558758735657, 0.12652814388275146, 0.7287291288375854, 0.05144988372921944, 0.15923729538917542, 0.056343428790569305, 0.20108187198638916, 0.10384152829647064, 0.6769842505455017, 0.15635962784290314, 0.037597063928842545, 0.9182345271110535, 0.20598110556602478, 0.8854387998580933, 0.12344175577163696, 0.20777544379234314, 0.06957220286130905, 0.06602411717176437, 0.19344566762447357, 0.9297349452972412, 0.19456349313259125, 0.09670204669237137, 0.11419535428285599, 0.2791749835014343, 0.43683257699012756, 0.13790054619312286, 0.5419905185699463, 0.3273480534553528, 0.03558388724923134, 0.24341101944446564, 0.10382932424545288, 0.48075634241104126, 0.10860870778560638, 0.4992281198501587, 0.31316089630126953, 0.18715800344944, 0.06730065494775772, 0.1838332712650299, 0.17196179926395416, 0.1715470403432846, 0.1524410992860794, 0.5991864800453186, 0.3308965563774109, 0.08051881194114685, 0.03294022008776665, 0.22365355491638184, 0.2543518543243408, 0.8032333850860596, 0.032586682587862015, 0.03870570287108421, 0.07024632394313812, 0.0846705213189125, 0.1359717696905136, 0.26721376180648804, 0.0348697192966938, 0.05256408452987671, 0.19765326380729675, 0.5076262354850769, 0.060605864971876144, 0.09594190865755081, 0.06499399244785309, 0.0664701834321022, 0.23512797057628632, 0.5636712312698364, 0.3725873827934265, 0.0732634961605072, 0.9919673800468445, 0.09625396877527237, 0.05663488060235977, 0.6167488098144531, 0.02129938453435898, 0.1083793193101883, 0.30076098442077637, 0.12329744547605515, 0.4922439157962799, 0.09153816103935242, 0.9621657133102417, 0.3781547248363495, 0.31030064821243286, 0.890662431716919, 0.05655129253864288, 0.2860362231731415, 0.05209798365831375, 0.0819692388176918, 0.03731982037425041, 0.7980614900588989, 0.6764668822288513, 0.19193129241466522, 0.3061111569404602, 0.45441311597824097, 0.05188000947237015, 0.07129122316837311, 0.7093441486358643, 0.5704578161239624, 0.15099216997623444, 0.13137607276439667, 0.14952616393566132, 0.6973659992218018, 0.0833582654595375, 0.2649092376232147, 0.08246199786663055, 0.0697978213429451, 0.19158042967319489, 0.2871725261211395, 0.07079446315765381, 0.5516820549964905, 0.9388291835784912, 0.6380100846290588, 0.0941457524895668, 0.03556736186146736, 0.23899565637111664, 0.36181798577308655, 0.10295085608959198, 0.42975500226020813, 0.030374648049473763, 0.09630727767944336, 0.32482588291168213, 0.08120089769363403, 0.33783140778541565, 0.09327708929777145, 0.41372326016426086, 0.12048839032649994, 0.35677772760391235, 0.07583347707986832, 0.09612040966749191, 0.0659717470407486, 0.464059978723526, 0.07635535299777985, 0.5584039688110352, 0.9805516004562378, 0.08059114217758179, 0.0863194689154625, 0.09296209365129471, 0.2215522974729538, 0.05196116864681244, 0.19435937702655792, 0.7827160954475403, 0.1107257753610611, 0.11226952075958252, 0.842420756816864, 0.2131509780883789, 0.04775228351354599, 0.950549840927124, 0.13363532721996307, 0.0563887283205986, 0.09822004288434982, 0.06198059767484665, 0.033618487417697906, 0.6680852770805359, 0.08029325306415558, 0.9333744049072266, 0.953041672706604, 0.05227971822023392, 0.09355297684669495, 0.0772312805056572, 0.03321579843759537, 0.7816603779792786, 0.04754560440778732, 0.15594105422496796, 0.5503624677658081, 0.14700442552566528, 0.9205711483955383, 0.9626109004020691, 0.09710711985826492, 0.6425076127052307, 0.055340833961963654, 0.15494179725646973, 0.062220096588134766, 0.3243395984172821, 0.1518886238336563, 0.038864124566316605, 0.10676425695419312, 0.05930750444531441, 0.1041802391409874, 0.24662581086158752, 0.9744536876678467, 0.5107623338699341, 0.04787394031882286, 0.06089358031749725, 0.050783514976501465, 0.04050040990114212, 0.2675820291042328, 0.47181403636932373, 0.07758461683988571, 0.07945913821458817, 0.7623608708381653, 0.06403239071369171, 0.04141014814376831, 0.0562329962849617, 0.6852284669876099, 0.07360589504241943, 0.06240321695804596, 0.7234296202659607, 0.23583675920963287, 0.046559348702430725, 0.04323972761631012, 0.7324776649475098, 0.26295846700668335, 0.524867832660675, 0.060293618589639664, 0.02429952658712864, 0.1005798801779747, 0.0867890790104866, 0.3629913926124573, 0.06960783153772354, 0.13702599704265594, 0.51333087682724, 0.08975031971931458, 0.056178055703639984, 0.05502362176775932, 0.5330923795700073, 0.8972413539886475, 0.6684843301773071, 0.04664191976189613, 0.09433455020189285, 0.8052921295166016, 0.29993781447410583, 0.4323432445526123, 0.05141361430287361, 0.07393242418766022, 0.7198531031608582, 0.1254895031452179, 0.6306426525115967, 0.028401657938957214, 0.06157251447439194, 0.08653433620929718, 0.19119693338871002, 0.47977036237716675, 0.22658371925354004, 0.04296133667230606, 0.9128755927085876, 0.13974584639072418, 0.256130188703537, 0.10384152829647064, 0.8761158585548401, 0.04897904396057129, 0.06323722004890442, 0.06422005593776703, 0.7498414516448975, 0.07863409072160721, 0.026264609768986702, 0.9025968909263611, 0.3913898169994354, 0.0833233967423439, 0.3791489899158478, 0.154887393116951, 0.7507944107055664, 0.2573157250881195, 0.5605199337005615, 0.15113869309425354, 0.2603875994682312, 0.1496620923280716, 0.11755666136741638, 0.17415055632591248, 0.044007547199726105, 0.02936442196369171, 0.31148484349250793, 0.24258846044540405, 0.4275707006454468, 0.06959311664104462, 0.12757106125354767, 0.0578523613512516, 0.2832731306552887, 0.592771589756012, 0.07235809415578842, 0.5401051640510559, 0.1351473331451416, 0.042067527770996094, 0.18932025134563446, 0.08497438579797745, 0.6417931914329529, 0.10310542583465576, 0.11356324702501297, 0.1106957271695137, 0.8537816405296326, 0.5940071940422058, 0.021865205839276314, 0.980198323726654, 0.980943500995636, 0.033818114548921585, 0.07523823529481888, 0.7060815095901489, 0.07806656509637833, 0.5667688250541687, 0.7521932721138, 0.6082313656806946, 0.6400395631790161, 0.9822937846183777, 0.9582016468048096, 0.08156540989875793, 0.05241472274065018, 0.1423400342464447, 0.2069346159696579, 0.05237391218543053, 0.14284083247184753, 0.725304901599884, 0.06663179397583008, 0.5796440243721008, 0.11707248538732529, 0.07560493797063828, 0.055430881679058075, 0.05899448320269585, 0.10899358987808228, 0.2213495969772339, 0.07439234107732773, 0.05476069822907448, 0.07530137151479721, 0.06766333431005478, 0.04106259346008301, 0.041399598121643066, 0.6367658972740173, 0.15825524926185608, 0.3244645595550537, 0.1668514758348465, 0.9799385070800781, 0.5323038101196289, 0.606707751750946, 0.053966037929058075, 0.33146998286247253, 0.07548510283231735, 0.18373988568782806, 0.04094516858458519, 0.05734153464436531, 0.11470428854227066, 0.11540649086236954, 0.8190608024597168, 0.865770697593689, 0.19226613640785217, 0.37717923521995544, 0.03535822406411171, 0.06918218731880188, 0.8588892221450806, 0.7444237470626831, 0.7643340229988098, 0.11740907281637192, 0.07955508679151535, 0.1283007711172104, 0.34983301162719727, 0.763703465461731, 0.018652133643627167, 0.1860879361629486, 0.16426105797290802, 0.053849827498197556, 0.1202453076839447, 0.12475995719432831, 0.12341566383838654, 0.11562004685401917, 0.05018264800310135, 0.7932403683662415, 0.8923056125640869, 0.9750403165817261, 0.4054252803325653, 0.29139405488967896, 0.7725765705108643, 0.19257806241512299, 0.17738771438598633, 0.03653440624475479, 0.08259490877389908, 0.061860766261816025, 0.10373584926128387, 0.1272563934326172, 0.033912885934114456, 0.1357189565896988, 0.09697001427412033, 0.08493950963020325, 0.1637340635061264, 0.07775580883026123, 0.022018615156412125, 0.05050709471106529, 0.6516488790512085, 0.03195411339402199, 0.03897282853722572, 0.0317288413643837, 0.9597263336181641, 0.02552284486591816, 0.12443843483924866, 0.053123071789741516, 0.07202117145061493, 0.12309456616640091, 0.05054934322834015, 0.09212173521518707, 0.3797343373298645, 0.5593173503875732, 0.4309922754764557, 0.10188666731119156, 0.6500717401504517, 0.0419563427567482, 0.041744064539670944, 0.04269026592373848, 0.9414833188056946, 0.13529518246650696, 0.4799492061138153, 0.26595431566238403, 0.29357796907424927, 0.11079871654510498, 0.08049498498439789, 0.07941283285617828, 0.9370602369308472, 0.13646990060806274, 0.185816690325737, 0.4936853051185608, 0.96702641248703, 0.8435686826705933, 0.21129347383975983, 0.05143866688013077, 0.41374823451042175, 0.12952116131782532, 0.0681324303150177, 0.054334525018930435, 0.07427886128425598, 0.02192346751689911, 0.24913492798805237, 0.965369701385498, 0.05706705525517464, 0.3421296179294586, 0.07200372219085693, 0.050289131700992584, 0.4568607211112976, 0.5888593196868896, 0.15502704679965973, 0.06361187249422073, 0.1784600168466568, 0.2243245542049408, 0.9534339904785156, 0.08186441659927368, 0.06877003610134125, 0.6497842669487, 0.06608626246452332, 0.06723976880311966, 0.024405276402831078, 0.29865336418151855, 0.3043348789215088, 0.018841242417693138, 0.07258538156747818, 0.9621622562408447, 0.915956974029541, 0.16133186221122742, 0.72368323802948, 0.7571162581443787, 0.12470868974924088, 0.08119671791791916, 0.03646104410290718, 0.12875749170780182, 0.20103460550308228, 0.3738488554954529, 0.7617670893669128, 0.8103286623954773, 0.8276987671852112, 0.05764661356806755, 0.4147568345069885, 0.7234064340591431, 0.5895684957504272, 0.15334178507328033, 0.18739426136016846, 0.9939543604850769, 0.06269015371799469, 0.06269387155771255, 0.2138296514749527, 0.9687687158584595, 0.720284640789032, 0.9607847929000854, 0.36471980810165405, 0.27679169178009033, 0.1903182566165924, 0.05569439008831978, 0.2911268472671509, 0.10065384209156036, 0.23040254414081573, 0.3089708983898163, 0.26808232069015503, 0.16054502129554749, 0.37351194024086, 0.1111217588186264, 0.9383726119995117, 0.19581376016139984, 0.09116406738758087, 0.17820274829864502, 0.2426697015762329, 0.7361255288124084, 0.08483967930078506, 0.09138772636651993, 0.20165523886680603, 0.04402618482708931, 0.37151244282722473, 0.21642908453941345, 0.2643505930900574, 0.11586787551641464, 0.3164437711238861, 0.23893678188323975, 0.09498821198940277, 0.9257636070251465, 0.9160504341125488, 0.022314516827464104, 0.8864972591400146, 0.09675133973360062, 0.041497159749269485, 0.11459406465291977, 0.03178519755601883, 0.04125094413757324, 0.1795673966407776, 0.0591161772608757, 0.04181184992194176, 0.12962327897548676, 0.2361423671245575, 0.9597177505493164, 0.25230276584625244, 0.11506899446249008, 0.13532771170139313, 0.928056001663208, 0.07608510553836823, 0.12741102278232574, 0.16764728724956512, 0.4951278865337372, 0.0718609020113945, 0.14942589402198792, 0.13962741196155548, 0.10004828870296478, 0.16248582303524017, 0.5973210334777832, 0.06383008509874344, 0.08114230632781982, 0.02415565401315689, 0.07437607645988464, 0.5462091565132141, 0.04192689433693886, 0.8265711665153503, 0.19883348047733307, 0.09512662142515182, 0.039956022053956985, 0.05257461592555046, 0.9714820981025696, 0.8030422329902649, 0.0875762403011322, 0.38404059410095215, 0.17830316722393036, 0.07960496097803116, 0.9711110591888428, 0.09919784963130951, 0.9867925643920898, 0.19041402637958527, 0.8746064901351929, 0.059961143881082535, 0.025848794728517532, 0.04923437535762787, 0.73991459608078, 0.4059387147426605, 0.9886323809623718, 0.782482922077179, 0.0470743402838707, 0.8591024279594421, 0.038960766047239304, 0.10475470125675201, 0.08821366727352142, 0.8727381229400635, 0.18775220215320587, 0.02918359450995922, 0.3550347685813904, 0.049935147166252136, 0.11213420331478119, 0.11002141982316971, 0.515125572681427, 0.13707053661346436, 0.09110045433044434, 0.34878867864608765, 0.6928651928901672, 0.05625511705875397, 0.9594932794570923, 0.158177450299263, 0.05571923404932022, 0.37371891736984253, 0.2957121431827545, 0.3704945147037506, 0.03758874163031578, 0.903240442276001, 0.1539607048034668, 0.015406117774546146, 0.8418155908584595, 0.0511145144701004, 0.12489399313926697, 0.16406214237213135, 0.9381730556488037, 0.045627161860466, 0.23560382425785065, 0.4087850749492645, 0.10259148478507996, 0.17850032448768616, 0.18930386006832123, 0.2049892395734787, 0.17985963821411133, 0.9009494781494141, 0.9831171035766602, 0.35454466938972473, 0.03576935455203056, 0.6036607027053833, 0.36898720264434814, 0.08581084758043289, 0.05426908656954765, 0.15564493834972382, 0.41405603289604187, 0.7823675870895386, 0.26356470584869385, 0.9079524278640747, 0.2867916524410248, 0.11505966633558273, 0.5632675886154175, 0.041488226503133774, 0.606650710105896, 0.15128560364246368, 0.05900121107697487, 0.0488445945084095, 0.049926768988370895, 0.06228353828191757, 0.5560649633407593, 0.11269441246986389, 0.36286604404449463, 0.08474575728178024, 0.10768696665763855, 0.037259649485349655, 0.32448241114616394, 0.22623834013938904, 0.6640291213989258, 0.2923767864704132, 0.05042693018913269, 0.7673706412315369, 0.1370876282453537, 0.07058721035718918, 0.20737169682979584, 0.0819234624505043, 0.09620668739080429, 0.03818671405315399, 0.14397314190864563, 0.40222227573394775, 0.13654719293117523, 0.09131857752799988, 0.07166016101837158, 0.8541768789291382, 0.8352939486503601, 0.08821132779121399, 0.1887221485376358, 0.04699137434363365, 0.05514891445636749, 0.09090424329042435, 0.10587531328201294, 0.03896431252360344, 0.02493777871131897, 0.07505914568901062, 0.12856319546699524, 0.04717988148331642, 0.3462361991405487, 0.04430387541651726, 0.18413963913917542, 0.16306109726428986, 0.17871922254562378, 0.14744290709495544, 0.10370822250843048, 0.7636786103248596, 0.4529668986797333, 0.2931036949157715, 0.07197874784469604, 0.07865190505981445, 0.14902639389038086, 0.12372023612260818, 0.08121427148580551, 0.07750996947288513, 0.029786935076117516, 0.06676755100488663, 0.20546697080135345, 0.4921937584877014, 0.9122022986412048, 0.03730493411421776, 0.04604509845376015, 0.080173559486866, 0.1199997141957283, 0.284132719039917, 0.3886450529098511, 0.1888183355331421, 0.0893637016415596, 0.06532453745603561, 0.1717759668827057, 0.15200857818126678, 0.9745015501976013, 0.1181432455778122, 0.13176579773426056, 0.969011127948761, 0.6284558773040771, 0.05113240331411362, 0.6767094135284424, 0.3078535795211792, 0.0623697005212307, 0.9093223810195923, 0.15617229044437408, 0.9916564226150513, 0.6360051035881042, 0.061041854321956635, 0.051726773381233215, 0.17498396337032318, 0.1395711451768875, 0.7896519303321838, 0.8476120829582214, 0.05910557880997658, 0.12600931525230408, 0.03846074640750885, 0.7262833714485168, 0.3062494993209839, 0.0737161636352539, 0.0519692525267601, 0.2080080807209015, 0.023205040022730827, 0.8920086622238159, 0.916820228099823, 0.5213702917098999, 0.18546707928180695, 0.8585469126701355, 0.8675389289855957, 0.13199162483215332, 0.08468306809663773, 0.042329687625169754, 0.16940079629421234, 0.20343053340911865, 0.11528725922107697, 0.10085723549127579, 0.06526375561952591, 0.4871314465999603, 0.024048391729593277, 0.11592534184455872, 0.6500220894813538, 0.904714047908783, 0.15850479900836945, 0.9343465566635132, 0.6144224405288696, 0.2870079576969147, 0.04679065942764282, 0.1018986850976944, 0.2579266130924225, 0.3813862204551697, 0.074070505797863, 0.9076714515686035, 0.5054316520690918, 0.15047620236873627, 0.9866661429405212, 0.3389607071876526, 0.9172568917274475, 0.022793490439653397, 0.11715377867221832, 0.049148980528116226, 0.02899225801229477, 0.3900865614414215, 0.9625759720802307, 0.36840856075286865, 0.9682422876358032, 0.13698989152908325, 0.0617167092859745, 0.9442830085754395, 0.9882248640060425, 0.07856772840023041, 0.10293099284172058, 0.6298238635063171, 0.39097490906715393, 0.21954217553138733, 0.20180091261863708, 0.93532395362854, 0.34468209743499756, 0.0927782878279686, 0.7002995610237122, 0.04406512901186943, 0.06532814353704453, 0.07636310905218124, 0.11069977283477783, 0.5759255290031433, 0.14439423382282257, 0.04744993895292282, 0.15823601186275482, 0.23012645542621613, 0.2777497172355652, 0.07873909175395966, 0.8254727125167847, 0.9851953983306885, 0.17066147923469543, 0.11987142264842987, 0.04070405662059784, 0.03651288524270058, 0.5668776631355286, 0.46459323167800903, 0.3801632225513458, 0.02455216832458973, 0.05456273630261421, 0.04103663191199303, 0.13797450065612793, 0.04434816911816597, 0.20499025285243988, 0.03251374512910843, 0.9888932108879089, 0.0918646901845932, 0.1923649162054062, 0.037192098796367645, 0.7825035452842712, 0.9438285827636719, 0.2344539314508438, 0.0767170786857605, 0.40807440876960754, 0.04833262786269188, 0.13710637390613556, 0.04959995672106743, 0.08892038464546204, 0.22293485701084137, 0.07064060121774673, 0.037392377853393555, 0.5859274864196777, 0.2742654085159302, 0.9751225709915161, 0.9415236711502075, 0.04012910649180412, 0.09626423567533493, 0.0380677655339241, 0.17293505370616913, 0.14908087253570557, 0.053607068955898285, 0.22059626877307892, 0.23089852929115295, 0.05761457979679108, 0.04388471692800522, 0.05396922677755356, 0.9392738938331604, 0.9770596027374268, 0.7719951868057251, 0.028795866295695305, 0.08144952356815338, 0.6867103576660156, 0.09843137860298157, 0.9512006640434265, 0.07034096121788025, 0.6630223393440247, 0.2900444567203522, 0.0777096077799797, 0.6367762088775635, 0.28453245759010315, 0.08790338784456253, 0.07429773360490799, 0.08940771222114563, 0.6075319647789001, 0.053756438195705414, 0.17985597252845764, 0.16929011046886444, 0.17893409729003906, 0.13303369283676147, 0.2844877541065216, 0.36245325207710266, 0.4690890610218048, 0.2569328844547272, 0.15706557035446167, 0.22158150374889374, 0.24238590896129608, 0.07202981412410736, 0.04088679701089859, 0.26948606967926025, 0.2529248595237732, 0.14747625589370728, 0.6040624380111694, 0.9456450343132019, 0.24662581086158752, 0.9594210982322693, 0.5806865096092224, 0.6529536247253418, 0.07935942709445953, 0.05579238757491112, 0.04164532572031021, 0.51904296875, 0.18558070063591003, 0.03548938408493996, 0.17007330060005188, 0.0762648805975914, 0.20451025664806366, 0.8030595183372498, 0.07368744909763336, 0.8099337220191956, 0.03749101236462593, 0.0576995313167572, 0.0738845244050026, 0.06912607699632645, 0.3147889971733093, 0.027836117893457413, 0.2894080579280853, 0.8512765765190125, 0.6183390617370605, 0.7834791541099548, 0.6467849612236023, 0.04237765818834305, 0.08056194335222244, 0.15892162919044495, 0.16695912182331085, 0.834293782711029, 0.14629758894443512, 0.162807434797287, 0.5099532008171082, 0.10251398384571075, 0.17530441284179688, 0.14104777574539185, 0.13377170264720917, 0.3401871621608734, 0.05622169375419617, 0.036828555166721344, 0.09465798735618591, 0.05913741886615753, 0.5817802548408508, 0.05359967052936554, 0.06938911229372025, 0.08162378519773483, 0.06020443141460419, 0.11216351389884949, 0.15635843575000763, 0.7439009547233582, 0.3291567265987396, 0.4296192526817322, 0.06426508724689484, 0.046286605298519135, 0.47517961263656616, 0.09752221405506134, 0.07316805422306061, 0.166286438703537, 0.13320480287075043, 0.0907849669456482, 0.06685908883810043, 0.10389705002307892, 0.20605070888996124, 0.17378591001033783, 0.02958795614540577, 0.894027590751648, 0.03378096595406532, 0.029333090409636497, 0.8503597974777222, 0.29943954944610596, 0.0744447410106659, 0.9540286660194397, 0.9581773281097412, 0.052175045013427734, 0.9885047078132629, 0.22634606063365936, 0.38749420642852783, 0.4690583646297455, 0.35674113035202026, 0.0800246074795723, 0.1841590255498886, 0.4249599874019623, 0.04499918967485428, 0.06138282269239426, 0.9105731844902039, 0.07986931502819061, 0.07183333486318588, 0.407065749168396, 0.14463792741298676, 0.0512724407017231, 0.14413313567638397, 0.11356993764638901, 0.1551409661769867, 0.20921383798122406, 0.06513618677854538, 0.49880120158195496, 0.1226201057434082, 0.21806064248085022, 0.10215619951486588, 0.035198260098695755, 0.039132408797740936, 0.26179152727127075, 0.7510901093482971, 0.9040328860282898, 0.1316567063331604, 0.15364815294742584, 0.1057981327176094, 0.1533377766609192, 0.31007829308509827, 0.9330196380615234, 0.6499707102775574, 0.5139836668968201, 0.9563801288604736, 0.9807692766189575, 0.10371743142604828, 0.05808815360069275, 0.823603630065918, 0.046864625066518784, 0.2577071487903595, 0.0878099799156189, 0.07381238788366318, 0.3732815682888031, 0.7301502823829651, 0.19793526828289032, 0.09737130254507065, 0.446010023355484, 0.9311685562133789, 0.07518095523118973, 0.9766626358032227, 0.17933395504951477, 0.8094682097434998, 0.04427995905280113, 0.3080734610557556, 0.9399276375770569, 0.31354454159736633, 0.9248982667922974, 0.11226935684680939, 0.15482747554779053, 0.24368701875209808, 0.5601375699043274, 0.43364062905311584, 0.06593596190214157, 0.039623141288757324, 0.051402200013399124, 0.04271354898810387, 0.03717145696282387, 0.9573132991790771, 0.13420513272285461, 0.7456867694854736, 0.09196966886520386, 0.08689231425523758, 0.05237659439444542, 0.9698749780654907, 0.4254007339477539, 0.08797639608383179, 0.04889097064733505, 0.05168451741337776, 0.10387197881937027, 0.9690207242965698, 0.9915487170219421, 0.031180232763290405, 0.765156626701355, 0.04222486540675163, 0.059609707444906235, 0.025121008977293968, 0.23213107883930206, 0.11526650935411453, 0.11571640521287918, 0.22973701357841492, 0.053026508539915085, 0.542414128780365, 0.3663843870162964, 0.09788811206817627, 0.2818452715873718, 0.9555745720863342, 0.044257376343011856, 0.09093119949102402, 0.10257291793823242, 0.05777157098054886, 0.3674566149711609, 0.5984347462654114, 0.16330412030220032, 0.990912914276123, 0.20707930624485016, 0.0591391958296299, 0.28366532921791077, 0.9596938490867615, 0.057800132781267166, 0.1317310631275177, 0.12090452015399933, 0.13128642737865448, 0.092210553586483, 0.8986682295799255, 0.10991368442773819, 0.9717462062835693, 0.33212870359420776, 0.08639315515756607, 0.6355796456336975, 0.44598639011383057, 0.05413363501429558, 0.1614447683095932, 0.05902981013059616, 0.053435105830430984, 0.07687579095363617, 0.060093533247709274, 0.13238264620304108, 0.07587086409330368, 0.14132916927337646, 0.04123866185545921, 0.21547342836856842, 0.2519434690475464, 0.19269800186157227, 0.06152105703949928, 0.793663740158081, 0.7030882835388184, 0.098869189620018, 0.47073858976364136, 0.528293251991272, 0.9537520408630371, 0.04582266882061958, 0.19688698649406433, 0.33112356066703796, 0.6686893701553345, 0.12706893682479858, 0.13048513233661652, 0.05286406725645065, 0.26263493299484253, 0.9795457124710083, 0.9477463364601135, 0.605029284954071, 0.050089262425899506, 0.35050809383392334, 0.12052003294229507, 0.0788024291396141, 0.3486049473285675, 0.3343912959098816, 0.06402409821748734, 0.6372862458229065, 0.09420235455036163, 0.04452849552035332, 0.5482435822486877, 0.21052606403827667, 0.11012575030326843, 0.1490751951932907, 0.10963617265224457, 0.1418991982936859, 0.09613337367773056, 0.16126157343387604, 0.689210832118988, 0.11247332394123077, 0.09018208086490631, 0.13627463579177856, 0.20479199290275574, 0.29063767194747925, 0.08948445320129395, 0.075147844851017, 0.06786498427391052, 0.05472378432750702, 0.16868573427200317, 0.10136707872152328, 0.19361333549022675, 0.07890642434358597, 0.9655508399009705, 0.22380363941192627, 0.18420565128326416, 0.060949958860874176, 0.12026423960924149, 0.20441949367523193, 0.05049171671271324, 0.8667547106742859, 0.05956517904996872, 0.9652302861213684, 0.08445119112730026, 0.9369283318519592, 0.6185133457183838, 0.3934508264064789, 0.10104648768901825, 0.2892281711101532, 0.6221744418144226, 0.07023055106401443, 0.6665739417076111, 0.14416949450969696, 0.00946853868663311, 0.0722208097577095, 0.18644297122955322, 0.8798675537109375, 0.1676390916109085, 0.9878522157669067, 0.04706624895334244, 0.6264070272445679, 0.7665228247642517, 0.03603503853082657, 0.9695203304290771, 0.08642749488353729, 0.7580413818359375, 0.6517866849899292, 0.8707777857780457, 0.048715174198150635, 0.03910451382398605, 0.15070246160030365, 0.09901314228773117, 0.10008598864078522, 0.094924196600914, 0.04282630980014801, 0.09842640906572342, 0.11920563131570816, 0.03691429644823074, 0.058451808989048004, 0.10838314145803452, 0.03116811066865921, 0.9390257596969604, 0.11478262394666672, 0.12172851711511612, 0.99437016248703, 0.4608749449253082, 0.3273054361343384, 0.06424915045499802, 0.030555563047528267, 0.34498119354248047, 0.32793012261390686, 0.15778447687625885, 0.7849267721176147, 0.3051570653915405, 0.17576025426387787, 0.03601367771625519, 0.6301984190940857, 0.03955373167991638, 0.11503539979457855, 0.06337228417396545, 0.09557366371154785, 0.8653783202171326, 0.7884434461593628, 0.43985414505004883, 0.1948203593492508, 0.068894162774086, 0.26480504870414734, 0.5289149284362793, 0.7401466965675354, 0.0631769523024559, 0.7801069021224976, 0.5602539777755737, 0.09907868504524231, 0.08620890229940414, 0.6058025360107422, 0.49641504883766174, 0.09101959317922592, 0.6892132759094238, 0.09895221143960953, 0.14098763465881348, 0.8560481667518616, 0.05768552049994469, 0.9491286873817444, 0.758306622505188, 0.14744718372821808, 0.0858306884765625, 0.050633180886507034, 0.10998375713825226, 0.23403875529766083, 0.11713386327028275, 0.39427420496940613, 0.948996901512146, 0.7191869020462036, 0.2191317230463028, 0.0757145956158638, 0.04000142961740494, 0.04527267813682556, 0.2100195437669754, 0.7337630391120911, 0.4588406980037689, 0.16747620701789856, 0.056913867592811584, 0.652073323726654, 0.13992193341255188, 0.5862961411476135, 0.9528575539588928, 0.12276213616132736, 0.5263181328773499, 0.06617456674575806, 0.9747153520584106, 0.05110524222254753, 0.052787359803915024, 0.07355301827192307, 0.12273158878087997, 0.10888049751520157, 0.3067612051963806, 0.4973742663860321, 0.9637923836708069, 0.06944089382886887, 0.6508646607398987, 0.9004426002502441, 0.048364099115133286, 0.0352129265666008, 0.024974646046757698, 0.8644721508026123, 0.14046095311641693, 0.19827933609485626, 0.8700034618377686, 0.03912319615483284, 0.135051429271698, 0.5873199105262756, 0.07433187961578369, 0.10384152829647064, 0.9295438528060913, 0.31739571690559387, 0.3389052152633667, 0.058971259742975235, 0.08049371838569641, 0.053846325725317, 0.06633921712636948, 0.03805001080036163, 0.030136745423078537, 0.2960921823978424, 0.12227943539619446, 0.9601975083351135, 0.012445826083421707, 0.16557633876800537, 0.12551206350326538, 0.05827616527676582, 0.3009014427661896, 0.04515458270907402, 0.1370917558670044, 0.8642030358314514, 0.22269421815872192, 0.4246954023838043, 0.13767988979816437, 0.6630542278289795, 0.9840444326400757, 0.8956008553504944, 0.1992001086473465, 0.1935397833585739, 0.07911276072263718, 0.9843223690986633, 0.10410614311695099, 0.048664387315511703, 0.046430397778749466, 0.04602368548512459, 0.14979036152362823, 0.10669039189815521, 0.1781919151544571, 0.040025871247053146, 0.415305495262146, 0.10009610652923584, 0.4675571918487549, 0.07340838760137558, 0.19979657232761383, 0.21171745657920837, 0.7336103916168213, 0.025710538029670715, 0.10193021595478058, 0.10992234945297241, 0.25433239340782166, 0.0445655956864357, 0.39845535159111023, 0.07295193523168564, 0.3918050527572632, 0.5789978504180908, 0.599107563495636, 0.0744447410106659, 0.30789580941200256, 0.4117223024368286, 0.06742718070745468, 0.5556522607803345, 0.1436190903186798, 0.05333545431494713, 0.10244394838809967, 0.22544893622398376, 0.46341466903686523, 0.06404213607311249, 0.0326211117208004, 0.2940433919429779, 0.32449328899383545, 0.8917548656463623, 0.9098780751228333, 0.04239585995674133, 0.18687385320663452, 0.6981933116912842, 0.04880852252244949, 0.07252505421638489, 0.5331293344497681, 0.08407260477542877, 0.2983591556549072, 0.7711456418037415, 0.051616791635751724, 0.5401501655578613, 0.12020032852888107, 0.2060411125421524, 0.3176257610321045, 0.03105604276061058, 0.06100878119468689, 0.20524431765079498, 0.049482669681310654, 0.07307595014572144, 0.055202219635248184, 0.4487701952457428, 0.7378507256507874, 0.9107173085212708, 0.0586947537958622, 0.0543258972465992, 0.07984161376953125, 0.0954805389046669, 0.30551832914352417, 0.890683650970459, 0.20085032284259796, 0.06954993307590485, 0.13743798434734344, 0.38472023606300354, 0.1831924021244049, 0.2749232351779938, 0.19928103685379028, 0.610060453414917, 0.22860656678676605, 0.06883382052183151, 0.9200387597084045, 0.0628606453537941, 0.9659450650215149, 0.08778179436922073, 0.2720453441143036, 0.9446814656257629, 0.0381304994225502, 0.03995104506611824, 0.03468317538499832, 0.14856116473674774, 0.989944577217102, 0.15366210043430328, 0.12114442884922028, 0.6700170040130615, 0.06354647874832153, 0.10315647721290588, 0.17675507068634033, 0.9399092197418213, 0.024159016087651253, 0.9331544637680054, 0.07258936017751694, 0.17450255155563354, 0.0527152493596077, 0.46561750769615173, 0.03356829285621643, 0.29593539237976074, 0.09082138538360596, 0.261368989944458, 0.9135738015174866, 0.44395142793655396, 0.11826344579458237, 0.46068912744522095, 0.04073246195912361, 0.09765306860208511, 0.3011375665664673, 0.0803452581167221, 0.9840201139450073, 0.055514395236968994, 0.07119123637676239, 0.43389174342155457, 0.22391101717948914, 0.23784862458705902, 0.13246212899684906, 0.04891448840498924, 0.11854175478219986, 0.08324620872735977, 0.04027467966079712, 0.07641837000846863, 0.2584541141986847, 0.615241289138794, 0.3990389406681061, 0.1384144276380539, 0.05512044578790665, 0.11848568171262741, 0.10394028574228287, 0.23579838871955872, 0.05674479901790619, 0.9653262495994568, 0.7385297417640686, 0.0474228672683239, 0.0751332938671112, 0.0635194331407547, 0.14586272835731506, 0.11450711637735367, 0.8960776925086975, 0.05070118606090546, 0.07281282544136047, 0.030852796509861946, 0.04753973335027695, 0.09848704189062119, 0.2899243235588074, 0.15411508083343506, 0.35102179646492004, 0.3326943814754486, 0.0829041600227356, 0.0921449363231659, 0.2811332643032074, 0.07470517605543137, 0.09840045124292374, 0.2653101980686188, 0.3081602156162262, 0.8274361491203308, 0.06380508095026016, 0.22918343544006348, 0.44463658332824707, 0.2004638910293579, 0.12064534425735474, 0.771675705909729, 0.08645875006914139, 0.12706995010375977, 0.31344833970069885, 0.12383943796157837, 0.12279569357633591, 0.08683294802904129, 0.26663345098495483, 0.745952844619751, 0.15905044972896576, 0.2877832055091858, 0.7790672779083252, 0.07248322665691376, 0.1626611053943634, 0.8873416185379028, 0.5491878986358643, 0.9861197471618652, 0.9541599750518799, 0.09683893620967865, 0.9527117609977722, 0.10743752121925354, 0.7998152375221252, 0.10727308690547943, 0.06251156330108643, 0.09893901646137238, 0.5997951626777649, 0.2573363184928894, 0.21776562929153442, 0.24588973820209503, 0.055221475660800934, 0.9556170105934143, 0.23512881994247437, 0.40015432238578796, 0.7189905047416687, 0.9128632545471191, 0.04609077796339989, 0.09526867419481277, 0.5107857584953308, 0.8409245014190674, 0.25260818004608154, 0.1730956882238388, 0.1529025435447693, 0.9278024435043335, 0.2930850088596344, 0.06916961073875427, 0.9432068467140198, 0.9836562871932983, 0.10843079537153244, 0.20607908070087433, 0.016737600788474083, 0.02982235699892044, 0.05162931606173515, 0.13585492968559265, 0.6671186685562134, 0.03123238869011402, 0.8808814883232117, 0.816624104976654, 0.061807211488485336, 0.07292607426643372, 0.9757574200630188, 0.5525306463241577, 0.0703202486038208, 0.7298001050949097, 0.3325923979282379, 0.9478015899658203, 0.1155843734741211, 0.3814815878868103, 0.09331121295690536, 0.05684257298707962, 0.966619610786438, 0.16791217029094696, 0.04536551609635353, 0.4058966040611267, 0.7179596424102783, 0.04687907546758652, 0.10235847532749176, 0.07316134124994278, 0.12817391753196716, 0.12213848531246185, 0.05384392663836479, 0.05874444544315338, 0.4317617416381836, 0.03600919991731644, 0.0802493542432785, 0.1878189593553543, 0.035605672746896744, 0.04676560312509537, 0.04338065907359123, 0.11225859820842743, 0.17318569123744965, 0.16886159777641296, 0.020045366138219833, 0.3870277404785156, 0.09838860481977463, 0.8942311406135559, 0.055831748992204666, 0.1655857115983963, 0.3242399990558624, 0.8250361680984497, 0.14957763254642487, 0.22343343496322632, 0.0714060440659523, 0.4035625755786896, 0.11464232206344604, 0.27451273798942566, 0.06417743116617203, 0.09257443249225616, 0.496073454618454, 0.0340820848941803, 0.18855814635753632, 0.02777104265987873, 0.2911575734615326, 0.9722276926040649, 0.4268544316291809, 0.6145843863487244, 0.9347183108329773, 0.31637826561927795, 0.07760024815797806, 0.11452794820070267, 0.110841304063797, 0.37545111775398254, 0.48644986748695374, 0.06705112755298615, 0.04331228882074356, 0.14435681700706482, 0.422252357006073, 0.12053384631872177, 0.9009608626365662, 0.9727486371994019, 0.15496602654457092, 0.06984707713127136, 0.17561738193035126, 0.10376282036304474, 0.07760622352361679, 0.13001033663749695, 0.04979945719242096, 0.06850314885377884, 0.8559013605117798, 0.03786429390311241, 0.47876542806625366, 0.05673026293516159, 0.5270227193832397, 0.6631754040718079, 0.05701659992337227, 0.3017992675304413, 0.2576166093349457, 0.07191705703735352, 0.420423686504364, 0.11029749363660812, 0.04702606052160263, 0.470866858959198, 0.6968536972999573, 0.29297181963920593, 0.06019092723727226, 0.9169259667396545, 0.9153720736503601, 0.16549190878868103, 0.11724033951759338, 0.049037866294384, 0.6312806606292725, 0.24070920050144196, 0.1534471958875656, 0.11679115891456604, 0.04239863157272339, 0.2335444986820221, 0.06680402904748917, 0.2412327378988266, 0.18334843218326569, 0.11007165163755417, 0.3731050193309784, 0.07154858112335205, 0.2603789269924164, 0.8421671390533447, 0.22421035170555115, 0.05252373218536377, 0.14744290709495544, 0.07689660042524338, 0.08404932916164398, 0.2271692305803299, 0.6229223608970642, 0.07890411466360092, 0.14948660135269165, 0.1559389978647232, 0.22057406604290009, 0.11295413970947266, 0.966745138168335, 0.026735778898000717, 0.6801565289497375, 0.5941339135169983, 0.018602818250656128, 0.10233206301927567, 0.07782935351133347, 0.410956472158432, 0.9005904197692871, 0.28799378871917725, 0.12824396789073944, 0.49541229009628296, 0.03282434120774269, 0.07159680873155594, 0.05199306830763817, 0.7849764227867126, 0.9328539371490479, 0.05481212958693504, 0.039606962352991104, 0.24225249886512756, 0.03619029000401497, 0.020143093541264534, 0.9457346796989441, 0.030748853459954262, 0.10018410533666611, 0.06136973202228546, 0.11159912496805191, 0.03616337105631828, 0.4969165325164795, 0.6768131852149963, 0.20876963436603546, 0.5437937378883362, 0.1399328112602234, 0.09368453919887543, 0.5236162543296814, 0.08494781702756882, 0.065461665391922, 0.8170616030693054, 0.05632137879729271, 0.08081883192062378, 0.04424002394080162, 0.0706813856959343, 0.373892605304718, 0.06271252036094666, 0.05948381870985031, 0.9734251499176025, 0.08466441184282303, 0.10407230257987976, 0.026349149644374847, 0.2932371199131012, 0.23567795753479004, 0.05234429985284805, 0.17239165306091309, 0.2157277911901474, 0.08389227092266083, 0.10130435228347778, 0.10103568434715271, 0.13564139604568481, 0.13338463008403778, 0.8214613199234009, 0.11707499623298645, 0.20511922240257263, 0.9014268517494202, 0.04739683121442795, 0.09753788262605667, 0.24722158908843994, 0.1916724592447281, 0.16334925591945648, 0.9095564484596252, 0.04833344742655754, 0.3862406611442566, 0.04389207065105438, 0.7364561557769775, 0.06619277596473694, 0.05022156238555908, 0.19393762946128845, 0.04574374109506607, 0.20531867444515228, 0.8365387320518494, 0.11119788140058517, 0.7952014803886414, 0.08498325943946838, 0.16226407885551453, 0.0949883908033371, 0.057119373232126236, 0.2936813235282898, 0.06788421422243118, 0.07339124381542206, 0.6575211882591248, 0.039922889322042465, 0.051568448543548584, 0.8439638018608093, 0.37782081961631775, 0.1764332354068756, 0.11611990630626678, 0.05116729438304901, 0.14228172600269318, 0.11984606087207794, 0.9110828042030334, 0.5767066478729248, 0.0867454931139946, 0.12252750992774963, 0.09467247128486633, 0.9375947713851929, 0.07103084772825241, 0.747829258441925, 0.2828640043735504, 0.1439170092344284, 0.1264977604150772, 0.09755844622850418, 0.3603917956352234, 0.2989867031574249, 0.04338233172893524, 0.0839213952422142, 0.2404101938009262, 0.19263112545013428, 0.037041112780570984, 0.04776270315051079, 0.1021740585565567, 0.17505040764808655, 0.053280752152204514, 0.03626399487257004, 0.9450889825820923, 0.8382009863853455, 0.07746163755655289, 0.4448387026786804, 0.9744163155555725, 0.08288503438234329, 0.2281017303466797, 0.09087646752595901, 0.19649054110050201, 0.062234438955783844, 0.8217176198959351, 0.2812323570251465, 0.8738042116165161, 0.9159116744995117, 0.27992022037506104, 0.45515176653862, 0.2641819715499878, 0.47345635294914246, 0.9178951382637024, 0.994903564453125, 0.03691716119647026, 0.938512921333313, 0.034780483692884445, 0.1032935082912445, 0.0307986568659544, 0.7534245848655701, 0.09220366179943085, 0.8317802548408508, 0.2478887289762497, 0.08649878203868866, 0.3350585401058197, 0.14387263357639313, 0.5401800870895386, 0.07712601870298386, 0.12235914170742035, 0.1484510898590088, 0.03040454164147377, 0.8415764570236206, 0.578988254070282, 0.3034578263759613, 0.06245210021734238, 0.0426006093621254, 0.1824885755777359, 0.058183491230010986, 0.5196905732154846, 0.042835116386413574, 0.15390628576278687, 0.7948635816574097, 0.12114521116018295, 0.327921599149704, 0.06991156190633774, 0.6758450269699097, 0.07829207926988602, 0.05513530969619751, 0.1719389110803604, 0.061773207038640976, 0.16535396873950958, 0.05774090066552162, 0.3524007499217987, 0.9396615624427795, 0.4683287441730499, 0.2786465883255005, 0.24161396920681, 0.0744447410106659, 0.8211055397987366, 0.13286246359348297, 0.441214382648468, 0.4236149489879608, 0.8909019231796265, 0.2890726923942566, 0.11323964595794678, 0.8778892755508423, 0.35052743554115295, 0.1952391117811203, 0.12510114908218384, 0.8832867741584778, 0.8665243983268738, 0.9205188155174255, 0.04967439919710159, 0.11322643607854843, 0.08271720260381699, 0.0408712774515152, 0.1001422256231308, 0.1735798418521881, 0.8922978639602661, 0.08126405626535416, 0.913274884223938, 0.08300938457250595, 0.07847769558429718, 0.5754063129425049, 0.5428189039230347, 0.45277824997901917, 0.6573150157928467, 0.03840538486838341, 0.13882334530353546, 0.04187532141804695, 0.6854074001312256, 0.9585226774215698, 0.10206465423107147, 0.05536885932087898, 0.3428691029548645, 0.09648416936397552, 0.12165722250938416, 0.11275624483823776, 0.06345894932746887, 0.2495231181383133, 0.03749198466539383, 0.8361648321151733, 0.11150820553302765, 0.09066455811262131, 0.1997496336698532, 0.9822140336036682, 0.05777456983923912, 0.3610745966434479, 0.07486093789339066, 0.4991427958011627, 0.6157189011573792, 0.15025416016578674, 0.28322654962539673, 0.9851928949356079, 0.975068986415863, 0.0675085112452507, 0.8166497945785522, 0.14066848158836365, 0.08705740422010422, 0.1441817581653595, 0.31969860196113586, 0.0744447410106659, 0.08913401514291763, 0.7425578832626343, 0.04205639287829399, 0.04159485548734665, 0.09234107285737991, 0.06073886156082153, 0.8921159505844116, 0.3804018497467041, 0.47792306542396545, 0.03932441398501396, 0.15915873646736145, 0.048943985253572464, 0.5073205232620239, 0.9732100963592529, 0.91249018907547, 0.06683400273323059, 0.05894312262535095, 0.023347029462456703, 0.9463776350021362, 0.0713709220290184, 0.2634429931640625, 0.07535801082849503, 0.04533914104104042, 0.18056851625442505, 0.1020992174744606, 0.7719376683235168, 0.5318924188613892, 0.09445467591285706, 0.05975532904267311, 0.10374200344085693, 0.8959195613861084, 0.11397884786128998, 0.6085946559906006, 0.24662581086158752, 0.06444203853607178, 0.10128716379404068, 0.17488163709640503, 0.12352609634399414, 0.5640124082565308, 0.24907562136650085, 0.27313268184661865, 0.053125735372304916, 0.0648403987288475, 0.05399053543806076, 0.21721512079238892, 0.13182303309440613, 0.05265471339225769, 0.03310002386569977, 0.46850791573524475, 0.10861814767122269, 0.14965422451496124, 0.16992849111557007, 0.34981364011764526, 0.04652576893568039, 0.02754933573305607, 0.27801260352134705, 0.33587196469306946, 0.04144570231437683, 0.4222654104232788, 0.2615354657173157, 0.1756848841905594, 0.06935027241706848, 0.2585158944129944, 0.9232906699180603, 0.05936529114842415, 0.7382928133010864, 0.19240538775920868, 0.9740860462188721, 0.9247536063194275, 0.26728156208992004, 0.9791821241378784, 0.3636032044887543, 0.07990514487028122, 0.10312722623348236, 0.029792889952659607, 0.22145117819309235, 0.19069406390190125, 0.1269000917673111, 0.7158907055854797, 0.9382312893867493, 0.19869957864284515, 0.486825555562973, 0.025079086422920227, 0.10141534358263016, 0.12369320541620255, 0.16237680613994598, 0.8147988319396973, 0.9948312640190125, 0.602827787399292, 0.12824618816375732, 0.07191089540719986, 0.6058719158172607, 0.060865119099617004, 0.18670664727687836, 0.3918219804763794, 0.055861834436655045, 0.11379709839820862, 0.19124676287174225, 0.12797200679779053, 0.2931518256664276, 0.26609858870506287, 0.19496411085128784, 0.3579627573490143, 0.11132921278476715, 0.09213216602802277, 0.06580604612827301, 0.19531312584877014, 0.09272190928459167, 0.04910573735833168, 0.6582655310630798, 0.2010345607995987, 0.5000655055046082, 0.8507805466651917, 0.25016894936561584, 0.9593847393989563, 0.02751954086124897, 0.19963377714157104, 0.1441602110862732, 0.20779456198215485, 0.09689750522375107, 0.3780716359615326, 0.08930885046720505, 0.13320010900497437, 0.9241095185279846, 0.9142913818359375, 0.26987043023109436, 0.19115202128887177, 0.07617250084877014, 0.07733626663684845, 0.09466775506734848, 0.2304643988609314, 0.11230605095624924, 0.7406853437423706, 0.05418964475393295, 0.1953224241733551, 0.2921901047229767, 0.9484536647796631, 0.31030064821243286, 0.09255359321832657, 0.08033046126365662, 0.03932797163724899, 0.1336546093225479, 0.948906660079956, 0.13203378021717072, 0.2023543417453766, 0.05325127765536308, 0.23507235944271088, 0.9378021955490112, 0.21849925816059113, 0.20240584015846252, 0.1276150345802307, 0.0697622075676918, 0.557866632938385, 0.22202791273593903, 0.11341022700071335, 0.10856476426124573, 0.3583732545375824, 0.04775316268205643, 0.04700438678264618, 0.3884599506855011, 0.029409058392047882, 0.5181792974472046, 0.26242387294769287, 0.05905787646770477, 0.07614453136920929, 0.07155194133520126, 0.06908214837312698, 0.09663919359445572, 0.1485345959663391, 0.08341317623853683, 0.10888005048036575, 0.5944806337356567, 0.023422149941325188, 0.13816872239112854, 0.028621377423405647, 0.5631725192070007, 0.05214725062251091, 0.321590393781662, 0.6175042986869812, 0.17032526433467865, 0.06493601948022842, 0.08384459465742111, 0.3900131583213806, 0.9111077189445496, 0.10198874771595001, 0.15347307920455933, 0.19823773205280304, 0.03135775402188301, 0.7164235711097717, 0.04066145792603493, 0.7495927214622498, 0.08660386502742767, 0.21633633971214294, 0.048161912709474564, 0.05023818835616112, 0.07149767875671387, 0.044774867594242096, 0.12054681777954102, 0.5283664464950562, 0.10934314876794815, 0.03379712626338005, 0.10146532952785492, 0.8557204008102417, 0.899928092956543, 0.056986842304468155, 0.9620633125305176, 0.9568275809288025, 0.08406440168619156, 0.031683940440416336, 0.020069299265742302, 0.737519383430481, 0.07422778010368347, 0.011189743876457214, 0.5370917320251465, 0.28830984234809875, 0.09028436988592148, 0.05599035695195198, 0.1998976618051529, 0.9288224577903748, 0.0941772311925888, 0.9845802783966064, 0.2232290655374527, 0.04199924319982529, 0.14765748381614685, 0.17883656919002533, 0.48759275674819946, 0.0661117359995842, 0.08439130336046219, 0.061234962195158005, 0.4337019622325897, 0.8269115686416626, 0.11857453733682632, 0.05474105104804039, 0.17973129451274872, 0.23434944450855255, 0.07735458761453629, 0.36900538206100464, 0.10609018057584763, 0.27312248945236206, 0.15312069654464722, 0.5528751015663147, 0.25762030482292175, 0.10690717399120331, 0.04974464699625969, 0.07854605466127396, 0.4200875759124756, 0.46355023980140686, 0.16175276041030884, 0.27048981189727783, 0.2021835595369339, 0.06210603937506676, 0.05427592247724533, 0.03705928102135658, 0.04581763595342636, 0.0620870478451252, 0.2437480241060257, 0.9023728966712952, 0.08529320359230042, 0.3617877662181854, 0.08433344960212708, 0.048454899340867996, 0.8443030714988708, 0.046440109610557556, 0.07688365876674652, 0.24811771512031555, 0.39036574959754944, 0.9882293939590454, 0.20167410373687744, 0.19979152083396912, 0.24711915850639343, 0.07721330970525742, 0.20299777388572693, 0.6738153100013733, 0.05849464237689972, 0.06714262813329697, 0.0530267134308815, 0.9617486000061035, 0.3590952754020691, 0.08295117318630219, 0.11625458300113678, 0.05177498981356621, 0.9813001751899719, 0.23298147320747375, 0.08137822151184082, 0.8112447261810303, 0.16457520425319672, 0.0744447410106659, 0.9509751796722412, 0.057200849056243896, 0.09451474994421005, 0.05988278239965439, 0.7955741882324219, 0.03501797839999199, 0.8855152130126953, 0.17104654014110565, 0.40175873041152954, 0.31292790174484253, 0.13961298763751984, 0.06968192756175995, 0.5767943263053894, 0.6185526251792908, 0.12068767100572586, 0.12834806740283966, 0.03416862338781357, 0.0910363420844078, 0.6935681700706482, 0.09659545123577118, 0.04850105196237564, 0.17198064923286438, 0.05382361635565758, 0.055549073964357376, 0.9849199056625366, 0.15923620760440826, 0.07135361433029175, 0.03982385993003845, 0.22094634175300598, 0.6389203071594238, 0.5176181793212891, 0.8831197619438171, 0.5847579836845398, 0.052289996296167374, 0.06347841024398804, 0.6612817049026489, 0.6883779168128967, 0.040221892297267914, 0.3421642482280731, 0.7707610130310059, 0.7287171483039856, 0.6300380229949951, 0.05373469740152359, 0.8197124004364014, 0.04245249554514885, 0.0728469118475914, 0.09565403312444687, 0.6309770941734314, 0.38638925552368164, 0.38606154918670654, 0.09829636663198471, 0.09393128007650375, 0.36149391531944275, 0.05237032473087311, 0.31030064821243286, 0.20338714122772217, 0.18701303005218506, 0.0416075624525547, 0.5920715928077698, 0.9433969259262085, 0.06879349052906036, 0.969244122505188, 0.206198588013649, 0.05694739520549774, 0.6220189332962036, 0.9515577554702759, 0.03652101382613182, 0.3470677435398102, 0.7480478882789612, 0.9782941341400146, 0.07224009186029434, 0.15708261728286743, 0.07815532386302948, 0.167259082198143, 0.04477432370185852, 0.07358911633491516, 0.131414532661438, 0.6856143474578857, 0.8920145034790039, 0.6555318832397461, 0.058161184191703796, 0.09130057692527771, 0.0321354903280735, 0.04013543203473091, 0.25846123695373535, 0.769220232963562, 0.12019813805818558, 0.17045974731445312, 0.08575835078954697, 0.270298033952713, 0.27155086398124695, 0.2875690460205078, 0.13895875215530396, 0.26956596970558167, 0.10211091488599777, 0.12323775887489319, 0.3374023139476776, 0.10582835227251053, 0.041462406516075134, 0.10141534358263016, 0.06109056994318962, 0.3082461953163147, 0.050953660160303116, 0.152229905128479, 0.06164107099175453, 0.037805378437042236, 0.036307305097579956, 0.038031868636608124, 0.9420655965805054, 0.18892209231853485, 0.38726839423179626, 0.21974974870681763, 0.0479944609105587, 0.3400941789150238, 0.05111262947320938, 0.8176715970039368, 0.08121754229068756, 0.09465616196393967, 0.019807377830147743, 0.3162230849266052, 0.3222101330757141, 0.0997600108385086, 0.6873724460601807, 0.1342826634645462, 0.7200301289558411, 0.9405331611633301, 0.21072325110435486, 0.04799536615610123, 0.4739032983779907, 0.08044522255659103, 0.2351456880569458, 0.18639788031578064, 0.04537894204258919, 0.40078672766685486, 0.10323749482631683, 0.03480587154626846, 0.0444662943482399, 0.05789037421345711, 0.11151792109012604, 0.21461248397827148, 0.07652373611927032, 0.10124459862709045, 0.9783841371536255, 0.08580664545297623, 0.07748080044984818, 0.3931777775287628, 0.06514579802751541, 0.5119682550430298, 0.12245606631040573, 0.23887743055820465, 0.794312596321106, 0.12966740131378174, 0.06137099489569664, 0.17535284161567688, 0.16752730309963226, 0.060534924268722534, 0.046499840915203094, 0.941007673740387, 0.24366341531276703, 0.36607447266578674, 0.19523078203201294, 0.4076194763183594, 0.2407275289297104, 0.21002168953418732, 0.048352304846048355, 0.20943869650363922, 0.04630052298307419, 0.5996211171150208, 0.5917494297027588, 0.4703867435455322, 0.42657867074012756, 0.15014201402664185, 0.056618861854076385, 0.5993009805679321, 0.2001136690378189, 0.04169435799121857, 0.33190280199050903, 0.16310514509677887, 0.5284040570259094, 0.970302402973175, 0.07588482648134232, 0.03682229667901993, 0.5335168242454529, 0.101017065346241, 0.45286011695861816, 0.2439146637916565, 0.8064027428627014, 0.05245046317577362, 0.04749651625752449, 0.17362096905708313, 0.49593326449394226, 0.9900392889976501, 0.27350568771362305, 0.7817274928092957, 0.107616126537323, 0.040284477174282074, 0.44871804118156433, 0.5475926399230957, 0.04977182671427727, 0.03797757625579834, 0.5184129476547241, 0.25686079263687134, 0.07834138721227646, 0.47535163164138794, 0.05783088132739067, 0.9843393564224243, 0.22631582617759705, 0.17691965401172638, 0.08731524646282196, 0.16716893017292023, 0.26777827739715576, 0.9118595123291016, 0.27893757820129395, 0.9700028300285339, 0.9393268823623657, 0.07164958864450455, 0.8450764417648315, 0.108766570687294, 0.11713095754384995, 0.15962333977222443, 0.36245644092559814, 0.2121025025844574, 0.9635205268859863, 0.550061047077179, 0.8262670636177063, 0.6155279278755188, 0.9713867902755737, 0.10344856977462769, 0.07628488540649414, 0.5781657695770264, 0.04279904067516327, 0.03228773549199104, 0.06717154383659363, 0.2560194134712219, 0.637783408164978, 0.06508208066225052, 0.24660548567771912, 0.0375332236289978, 0.11746104061603546, 0.5631325840950012, 0.23357607424259186, 0.13872218132019043, 0.11045016348361969, 0.20796199142932892, 0.04953359439969063, 0.13372722268104553, 0.08423260599374771, 0.178811714053154, 0.20736008882522583, 0.30928194522857666, 0.03139171376824379, 0.050910960882902145, 0.1453040987253189, 0.9327182769775391, 0.1743854284286499, 0.15934564173221588, 0.05460295453667641, 0.038810305297374725, 0.07899577915668488, 0.9024620652198792, 0.5831870436668396, 0.2831213176250458, 0.13080938160419464, 0.9622483849525452, 0.11720182001590729, 0.03919295221567154, 0.12992119789123535, 0.07232329249382019, 0.7025476694107056, 0.07329028099775314, 0.13692794740200043, 0.1060502752661705, 0.30504316091537476, 0.0954536572098732, 0.3580433428287506, 0.038072746247053146, 0.06681416928768158, 0.13202673196792603, 0.09674307703971863, 0.035297051072120667, 0.20744311809539795, 0.7824214696884155, 0.03195597231388092, 0.8534450531005859, 0.5189301371574402, 0.08621753007173538, 0.38738730549812317, 0.07648912072181702, 0.9682270884513855, 0.8631542921066284, 0.1428283005952835, 0.06179029494524002, 0.9535682797431946, 0.3807945251464844, 0.07063714414834976, 0.14228011667728424, 0.875126838684082, 0.15210673213005066, 0.03086048923432827, 0.12893342971801758, 0.044605642557144165, 0.06174908205866814, 0.05477758124470711, 0.04845605790615082, 0.9857560992240906, 0.06609637290239334, 0.10878082364797592, 0.6499602794647217, 0.5753971338272095, 0.27881696820259094, 0.2197880893945694, 0.10273807495832443, 0.07750548422336578, 0.06116946414113045, 0.9741590023040771, 0.0946962833404541, 0.07412867993116379, 0.13913986086845398, 0.9872913956642151, 0.3369789123535156, 0.05213433876633644, 0.834876537322998, 0.05387997627258301, 0.5569654703140259, 0.5377725958824158, 0.05779625102877617, 0.04861469194293022, 0.05919140577316284, 0.8715956807136536, 0.9796950817108154, 0.675837516784668, 0.5477170944213867, 0.02809888869524002, 0.06670267134904861, 0.8127609491348267, 0.0681142807006836, 0.06386271864175797, 0.04387449473142624, 0.042267490178346634, 0.5810906887054443, 0.03541140630841255, 0.8548556566238403, 0.15327613055706024, 0.09402937442064285, 0.1520335078239441, 0.06601710617542267, 0.7856397032737732, 0.6688164472579956, 0.26675650477409363, 0.4873572885990143, 0.10853377729654312, 0.054185789078474045, 0.10367464274168015, 0.753621518611908, 0.09790123999118805, 0.08880212157964706, 0.2342151403427124, 0.05233592912554741, 0.6470931768417358, 0.05275905132293701, 0.09139340370893478, 0.08152564615011215, 0.1831776350736618, 0.5687433481216431, 0.6885800361633301, 0.10276249051094055, 0.787911593914032, 0.021902501583099365, 0.12584146857261658, 0.43824902176856995, 0.9909281134605408, 0.18980427086353302, 0.0481167808175087, 0.175532728433609, 0.7454484105110168, 0.9806391596794128, 0.19683073461055756, 0.4057380259037018, 0.08134650439023972, 0.4348549544811249, 0.21902409195899963, 0.9516606330871582, 0.3935234844684601, 0.7644855380058289, 0.24704311788082123, 0.9615593552589417, 0.5739120244979858, 0.06888817250728607, 0.9779036641120911, 0.07719206809997559, 0.6322497725486755, 0.11556516587734222, 0.08521930128335953, 0.2968646287918091, 0.7554900050163269, 0.1736968457698822, 0.5891827344894409, 0.08373747766017914, 0.08307516574859619, 0.2342820167541504, 0.18110764026641846, 0.37530404329299927, 0.040267664939165115, 0.10883256793022156, 0.9577802419662476, 0.0696917250752449, 0.07760318368673325, 0.1499466896057129, 0.4772408902645111, 0.03780875727534294, 0.054497282952070236, 0.05912800133228302, 0.2073739916086197, 0.13513827323913574, 0.23131926357746124, 0.5527987480163574, 0.06982047110795975, 0.1803763061761856, 0.05533609911799431, 0.04561295360326767, 0.1354748159646988, 0.09269122779369354, 0.19079488515853882, 0.06650492548942566, 0.9516465067863464, 0.12288779765367508, 0.3895484507083893, 0.8334642052650452, 0.2191956788301468, 0.1074611023068428, 0.8738240003585815, 0.04371499642729759, 0.1367398053407669, 0.1191520169377327, 0.2135942429304123, 0.14817261695861816, 0.191498264670372, 0.07900063693523407, 0.21136793494224548, 0.28144896030426025, 0.48826083540916443, 0.8975864052772522, 0.17338010668754578, 0.1825762838125229, 0.6357417106628418, 0.06341695040464401, 0.0871317908167839, 0.1540593057870865, 0.06425760686397552, 0.7518187165260315, 0.38942569494247437, 0.08319474756717682, 0.06722751259803772, 0.2703125476837158, 0.22523237764835358, 0.07546985894441605, 0.2365054488182068, 0.06776305288076401, 0.15299129486083984, 0.047081511467695236, 0.38666465878486633, 0.35061773657798767, 0.20945672690868378, 0.1334022581577301, 0.7868856191635132, 0.23493200540542603, 0.15851663053035736, 0.2953953742980957, 0.9356525540351868, 0.9833048582077026, 0.03309047967195511, 0.14910243451595306, 0.08161643892526627, 0.06327521055936813, 0.040969368070364, 0.936248242855072, 0.131205752491951, 0.5312214493751526, 0.040731530636548996, 0.886345624923706, 0.8397538661956787, 0.8087747097015381, 0.18212033808231354, 0.0942075103521347, 0.8185230493545532, 0.5862215161323547, 0.05648215860128403, 0.2392539530992508, 0.10562115907669067, 0.10091893374919891, 0.013151041232049465, 0.20191261172294617, 0.11859602481126785, 0.102296844124794, 0.11224597692489624, 0.405915766954422, 0.2555524408817291, 0.1783766895532608, 0.0629241093993187, 0.07599596679210663, 0.16188576817512512, 0.6253765225410461, 0.04279349371790886, 0.5015314221382141, 0.14639464020729065, 0.15233448147773743, 0.910653829574585, 0.06485314667224884, 0.1857362687587738, 0.03176769241690636, 0.062090709805488586, 0.05467524752020836, 0.30190518498420715, 0.5231429934501648, 0.1606912761926651, 0.055664729326963425, 0.26135727763175964, 0.10118091106414795, 0.03125494718551636, 0.03558903560042381, 0.19323678314685822, 0.33975476026535034, 0.9856858253479004, 0.7112163305282593, 0.12054725736379623, 0.6681336164474487, 0.010434712283313274, 0.7361425757408142, 0.17912450432777405, 0.5595161318778992, 0.42256784439086914, 0.15179911255836487, 0.1750841587781906, 0.45725584030151367, 0.641645610332489, 0.04908169060945511, 0.3163507878780365, 0.9758492708206177, 0.8871538043022156, 0.5456476211547852, 0.05997345969080925, 0.16509827971458435, 0.8843635320663452, 0.35470783710479736, 0.9356999397277832, 0.20001541078090668, 0.03189338743686676, 0.07707405090332031, 0.06794669479131699, 0.08938495069742203, 0.06067264825105667, 0.4938758611679077, 0.20683130621910095, 0.9609534740447998, 0.5192267894744873, 0.08302745223045349, 0.6525537371635437, 0.9586233496665955, 0.9538809061050415, 0.13390043377876282, 0.05393169820308685, 0.08374200016260147, 0.1043458804488182, 0.4713399112224579, 0.11594641208648682, 0.1330481618642807, 0.6700965762138367, 0.8875517249107361, 0.08288468420505524, 0.029121674597263336, 0.09039902687072754, 0.04623158648610115, 0.09466041624546051, 0.04211503267288208, 0.028475342318415642, 0.6363219618797302, 0.23464824259281158, 0.058640092611312866, 0.40334904193878174, 0.05436737835407257, 0.2011713832616806, 0.07639938592910767, 0.5459699630737305, 0.33139196038246155, 0.024220464751124382, 0.16158996522426605, 0.17766022682189941, 0.2912156879901886, 0.05937555059790611, 0.3173096179962158, 0.7962514758110046, 0.4762292206287384, 0.032977402210235596, 0.38914719223976135, 0.0507875494658947, 0.03452618420124054, 0.09237714856863022, 0.06647846847772598, 0.23102819919586182, 0.09645647555589676, 0.419061541557312, 0.8724690675735474, 0.13268235325813293, 0.03451498970389366, 0.06902801245450974, 0.7154555320739746, 0.08290120959281921, 0.6811238527297974, 0.15565992891788483, 0.05194144323468208, 0.5341581702232361, 0.5828579664230347, 0.12496006488800049]\n"
     ]
    }
   ],
   "source": [
    "print(test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "id": "NCWXRXAKuGlA"
   },
   "outputs": [],
   "source": [
    "test_predictions_rounded = [round(pred) for pred in test_predictions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "id": "FRduhnP5uKv1"
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"test.csv\", encoding='ISO-8859-1')\n",
    "test['Target'] = test_predictions_rounded\n",
    "test.to_csv('Predictions_CNN_FW.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zrKPf9z7EpHJ"
   },
   "source": [
    "## We observe that the initial CNN model gave us the best results on test dataset with 78.42% accuracy and hence we will choose that for making final predictions, which will be implemented in the final file."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
